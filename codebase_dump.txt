==================================
TEMPO Analyzer Codebase Dump
Generated: Fri Jan 16 11:58:08 PST 2026
==================================


========================================
FILE: src/tempo_app/core/batch_parser.py
========================================
"""Parser for batch site import files (Excel/CSV).

Parses Excel (.xlsx, .xls) or CSV files containing site information
for batch TEMPO data downloads.
"""

from dataclasses import dataclass, field
from datetime import date
from pathlib import Path
from typing import Optional
import pandas as pd

from .geo_utils import bbox_from_center, validate_coordinates


@dataclass
class ParsedSite:
    """A site parsed from an import file."""
    row_number: int              # Excel row number (for error reporting)
    site_name: str
    latitude: float
    longitude: float

    custom_date_start: Optional[str] = None  # ISO format date string
    custom_date_end: Optional[str] = None  # ISO format date string
    custom_max_cloud: Optional[float] = None  # Max cloud fraction
    custom_max_sza: Optional[float] = None  # Max solar zenith angle
    error: Optional[str] = None  # Validation error for this row


@dataclass
class ParseResult:
    """Result of parsing an import file."""
    sites: list[ParsedSite] = field(default_factory=list)
    errors: list[str] = field(default_factory=list)  # File-level errors
    warnings: list[str] = field(default_factory=list)  # Non-fatal issues
    file_path: Optional[str] = None

    @property
    def is_valid(self) -> bool:
        """Check if parsing succeeded with no fatal errors."""
        return len(self.errors) == 0

    @property
    def valid_sites(self) -> list[ParsedSite]:
        """Get only sites without errors."""
        return [s for s in self.sites if s.error is None]

    @property
    def invalid_sites(self) -> list[ParsedSite]:
        """Get sites with errors."""
        return [s for s in self.sites if s.error is not None]

    @property
    def site_count(self) -> int:
        """Total number of parsed sites (including invalid)."""
        return len(self.sites)

    @property
    def valid_count(self) -> int:
        """Number of valid sites."""
        return len(self.valid_sites)


# Column name aliases for flexible Excel formats
COLUMN_ALIASES = {
    "name": ["name", "site_name", "site", "location", "id", "site_id"],
    "latitude": ["latitude", "lat", "y", "lat_dd"],
    "longitude": ["longitude", "lon", "long", "x", "lng", "lon_dd"],
}


def _find_column(df: pd.DataFrame, aliases: list[str]) -> Optional[str]:
    """Find a column by checking multiple possible names."""
    for alias in aliases:
        if alias in df.columns:
            return alias
    return None


def _parse_date_value(value) -> Optional[str]:
    """Parse a date value from Excel into ISO format string."""
    if pd.isna(value):
        return None
    if isinstance(value, date):
        return value.isoformat()
    if isinstance(value, str):
        # Try to parse common date formats
        try:
            parsed = pd.to_datetime(value)
            return parsed.date().isoformat()
        except Exception:
            return value  # Return as-is, validation happens later
    return str(value)


def parse_import_file(file_path: Path) -> ParseResult:
    """Parse an Excel or CSV file for batch site import.

    Expected columns:
        Required: name (or site_name), latitude (or lat), longitude (or lon)
        Optional: date_start, date_end, max_cloud, max_sza

    Args:
        file_path: Path to .xlsx, .xls, or .csv file

    Returns:
        ParseResult with parsed sites and any errors/warnings
    """
    result = ParseResult(file_path=str(file_path))

    # Check file exists
    if not file_path.exists():
        result.errors.append(f"File not found: {file_path}")
        return result

    # Load file based on extension
    try:
        suffix = file_path.suffix.lower()
        if suffix in (".xlsx", ".xls"):
            df = pd.read_excel(file_path, engine="openpyxl" if suffix == ".xlsx" else None)
        elif suffix == ".csv":
            df = pd.read_csv(file_path)
        else:
            result.errors.append(f"Unsupported file format: {suffix}. Use .xlsx, .xls, or .csv")
            return result
    except Exception as e:
        result.errors.append(f"Failed to read file: {e}")
        return result

    # Check for empty file
    if df.empty:
        result.errors.append("File is empty or has no data rows")
        return result

    # Normalize column names (lowercase, strip whitespace)
    df.columns = [str(c).lower().strip() for c in df.columns]

    # Find required columns
    name_col = _find_column(df, COLUMN_ALIASES["name"])
    lat_col = _find_column(df, COLUMN_ALIASES["latitude"])
    lon_col = _find_column(df, COLUMN_ALIASES["longitude"])

    missing_cols = []
    if not name_col:
        missing_cols.append("name (or site_name, site, location)")
    if not lat_col:
        missing_cols.append("latitude (or lat, y)")
    if not lon_col:
        missing_cols.append("longitude (or lon, long, x)")

    if missing_cols:
        result.errors.append(f"Missing required columns: {', '.join(missing_cols)}")
        result.errors.append(f"Found columns: {', '.join(df.columns.tolist())}")
        return result

    # Find optional columns
    # Radius column removed as per user request
    date_start_col = _find_column(df, ["date_start", "start_date"])
    date_end_col = _find_column(df, ["date_end", "end_date"])
    max_cloud_col = _find_column(df, ["max_cloud", "cloud_fraction", "cloud"])
    max_sza_col = _find_column(df, ["max_sza", "sza", "solar_zenith"])

    # Parse each row
    for idx, row in df.iterrows():
        row_num = idx + 2  # Excel is 1-indexed, plus header row

        site = ParsedSite(
            row_number=row_num,
            site_name="",
            latitude=0.0,
            longitude=0.0,
        )

        # Parse site name
        if pd.notna(row[name_col]):
            site.site_name = str(row[name_col]).strip()
        if not site.site_name:
            site.error = "Missing site name"
            result.sites.append(site)
            continue

        # Parse latitude
        try:
            site.latitude = float(row[lat_col])
        except (ValueError, TypeError):
            site.error = f"Invalid latitude: {row[lat_col]}"
            result.sites.append(site)
            continue

        # Parse longitude
        try:
            site.longitude = float(row[lon_col])
        except (ValueError, TypeError):
            site.error = f"Invalid longitude: {row[lon_col]}"
            result.sites.append(site)
            continue

        # Validate coordinates
        valid, err_msg = validate_coordinates(site.latitude, site.longitude)
        if not valid:
            site.error = err_msg
            result.sites.append(site)
            continue

        # Parse optional fields
        if date_start_col and pd.notna(row[date_start_col]):
            site.custom_date_start = _parse_date_value(row[date_start_col])

        if date_end_col and pd.notna(row[date_end_col]):
            site.custom_date_end = _parse_date_value(row[date_end_col])

        if max_cloud_col and pd.notna(row[max_cloud_col]):
            try:
                site.custom_max_cloud = float(row[max_cloud_col])
            except (ValueError, TypeError):
                result.warnings.append(f"Row {row_num}: Invalid max_cloud, using default")

        if max_sza_col and pd.notna(row[max_sza_col]):
            try:
                site.custom_max_sza = float(row[max_sza_col])
            except (ValueError, TypeError):
                result.warnings.append(f"Row {row_num}: Invalid max_sza, using default")

        result.sites.append(site)

    # Final validation
    if not result.sites:
        result.errors.append("No sites found in file")

    return result


def create_sample_excel(file_path: Path, num_sites: int = 5) -> None:
    """Create a sample Excel file showing the expected format.

    Useful for users to understand the required structure.

    Args:
        file_path: Where to save the sample file
        num_sites: Number of sample sites to include
    """
    sample_data = {
        "name": [f"Site_{i+1}" for i in range(num_sites)],
        "latitude": [40.0 + i * 0.5 for i in range(num_sites)],
        "longitude": [-111.0 - i * 0.5 for i in range(num_sites)],
        "date_start": ["2024-01-01"] * num_sites,
        "date_end": ["2024-01-31"] * num_sites,
        "max_cloud": [0.3] * num_sites,
        "max_sza": [70.0] * num_sites,
    }

    df = pd.DataFrame(sample_data)
    df.to_excel(file_path, index=False, engine="openpyxl")


========================================
FILE: src/tempo_app/core/batch_scheduler.py
========================================
"""Batch job scheduler for processing multiple sites in parallel.

Manages the execution of batch import jobs, handling parallel site processing,
resume support, and progress tracking.
"""

import asyncio
from datetime import datetime, timedelta
from pathlib import Path
from typing import Callable, Optional
import logging
import uuid

from ..storage.database import Database
from ..storage.models import (
    BatchJob, BatchSite, BatchJobStatus, BatchSiteStatus,
    Dataset, DatasetStatus, Granule, BoundingBox
)
from .downloader import RSIGDownloader
from .processor import DataProcessor
from .geo_utils import bbox_from_center

logger = logging.getLogger(__name__)


def _sanitize_filename(name: str) -> str:
    """Sanitize a string for use as a filename."""
    return "".join(c if c.isalnum() or c in "._- " else "_" for c in name).strip()


class BatchScheduler:
    """Manages batch site processing with parallel execution and resume support.

    Example:
        scheduler = BatchScheduler(db, data_dir)
        scheduler.on_progress = lambda job, site, msg: print(msg)
        await scheduler.start_job(job_id)
    """

    def __init__(
        self,
        db: Database,
        data_dir: Path,
        max_concurrent_sites: int = 5,
        api_key: str = "",
        on_progress: Optional[Callable[[BatchJob, BatchSite, str], None]] = None,
        on_site_complete: Optional[Callable[[BatchSite], None]] = None,
        on_job_complete: Optional[Callable[[BatchJob], None]] = None,
    ):
        """Initialize the batch scheduler.

        Args:
            db: Database instance for persistence
            data_dir: Base directory for datasets
            max_concurrent_sites: Maximum sites to process in parallel
            api_key: RSIG API key for downloads (empty = anonymous)
            on_progress: Callback for progress updates
            on_site_complete: Callback when a site finishes
            on_job_complete: Callback when the entire job finishes
        """
        self.db = db
        self.data_dir = data_dir
        self.max_concurrent_sites = max_concurrent_sites
        self.api_key = api_key
        self.on_progress = on_progress
        self.on_site_complete = on_site_complete
        self.on_job_complete = on_job_complete

        self._running = False
        self._paused = False
        self._cancel_requested = False
        self._current_job: Optional[BatchJob] = None

    @property
    def is_running(self) -> bool:
        """Check if a job is currently running."""
        return self._running

    @property
    def current_job(self) -> Optional[BatchJob]:
        """Get the currently running job."""
        return self._current_job

    async def start_job(self, job_id: str) -> None:
        """Start or resume a batch job.

        Args:
            job_id: UUID of the batch job to start

        Raises:
            ValueError: If job not found or already running
        """
        job = self.db.get_batch_job(job_id)
        if not job:
            raise ValueError(f"Batch job {job_id} not found")

        if job.status == BatchJobStatus.RUNNING:
            raise ValueError("Job is already running")

        self._current_job = job
        self._running = True
        self._paused = False
        self._cancel_requested = False

        # Update job status
        job.status = BatchJobStatus.RUNNING
        self.db.update_batch_job(job)

        try:
            await self._process_job(job)
        except Exception as e:
            logger.error(f"Batch job failed: {e}")
            job.status = BatchJobStatus.ERROR
            job.error_message = str(e)
            self.db.update_batch_job(job)
        finally:
            self._running = False
            self._current_job = None

    async def pause_job(self) -> None:
        """Pause the currently running job."""
        self._paused = True

    async def cancel_job(self) -> None:
        """Cancel the currently running job."""
        self._cancel_requested = True

    async def _process_job(self, job: BatchJob) -> None:
        """Process all pending sites in a batch job."""
        # Reset any interrupted sites from previous runs
        reset_count = self.db.reset_interrupted_batch_sites(job.id)
        if reset_count > 0:
            logger.info(f"Reset {reset_count} interrupted sites")

        # Get pending sites
        pending_sites = self.db.get_pending_batch_sites(job.id)

        if not pending_sites:
            job.status = BatchJobStatus.COMPLETED
            self.db.update_batch_job(job)
            if self.on_job_complete:
                self.on_job_complete(job)
            return

        logger.info(f"Processing {len(pending_sites)} pending sites for job {job.name}")

        # Create semaphore for concurrency control
        semaphore = asyncio.Semaphore(job.batch_size or self.max_concurrent_sites)

        async def process_site_with_semaphore(site: BatchSite) -> None:
            async with semaphore:
                if self._cancel_requested or self._paused:
                    return
                await self._process_site(job, site)

        # Process sites in parallel batches
        tasks = [process_site_with_semaphore(site) for site in pending_sites]
        await asyncio.gather(*tasks, return_exceptions=True)

        # Update final job status
        job = self.db.get_batch_job(job.id)  # Refresh from DB

        if self._cancel_requested:
            job.status = BatchJobStatus.ERROR
            job.error_message = "Cancelled by user"
        elif self._paused:
            job.status = BatchJobStatus.PAUSED
        elif job.completed_sites + job.failed_sites >= job.total_sites:
            job.status = BatchJobStatus.COMPLETED

        job.last_processed_at = datetime.now()
        self.db.update_batch_job(job)

        if self.on_job_complete:
            self.on_job_complete(job)

    async def _process_site(self, job: BatchJob, site: BatchSite) -> None:
        """Process a single site within the batch job."""
        logger.info(f"Processing site: {site.site_name}")

        # Update status to downloading
        site.status = BatchSiteStatus.DOWNLOADING
        site.started_at = datetime.now()
        self.db.update_batch_site(site)

        if self.on_progress:
            self.on_progress(job, site, f"Starting {site.site_name}")

        try:
            # Determine settings (use per-site overrides or job defaults)
            date_start = site.custom_date_start or job.date_start
            date_end = site.custom_date_end or job.date_end
            max_cloud = site.custom_max_cloud if site.custom_max_cloud is not None else job.max_cloud
            max_sza = site.custom_max_sza if site.custom_max_sza is not None else job.max_sza

            # Create dataset for this site
            safe_name = _sanitize_filename(f"{job.name}_{site.site_name}")
            dataset = Dataset(
                id=str(uuid.uuid4()),
                name=safe_name,
                created_at=datetime.now(),
                bbox=site.bbox,
                date_start=date_start,
                date_end=date_end,
                day_filter=job.day_filter,
                hour_filter=job.hour_filter,
                max_cloud=max_cloud,
                max_sza=max_sza,
                status=DatasetStatus.DOWNLOADING,
            )
            dataset = self.db.create_dataset(dataset)
            site.dataset_id = dataset.id
            self.db.update_batch_site(site)

            # Create dataset directory
            dataset_dir = self.data_dir / "datasets" / safe_name
            dataset_dir.mkdir(parents=True, exist_ok=True)

            # Generate granule list
            granules = self._generate_granules(
                dataset, date_start, date_end, job.day_filter, job.hour_filter
            )
            self.db.create_granules_batch(granules)
            dataset.granule_count = len(granules)
            self.db.update_dataset(dataset)

            # Generate date and hour lists for downloader
            dates_list = sorted(set(g.date.isoformat() for g in granules))
            hours_list = sorted(set(g.hour for g in granules))

            if self.on_progress:
                self.on_progress(job, site, f"Downloading {site.site_name} ({len(granules)} granules)")

            # Download
            downloader = RSIGDownloader(dataset_dir, max_concurrent=4, api_key=self.api_key)
            files = await downloader.download_granules(
                dates=dates_list,
                hours=hours_list,
                bbox=site.bbox.to_list(),
                dataset_name=safe_name,
                max_cloud=max_cloud,
                max_sza=max_sza,
                status=None  # Could add status callback here
            )

            # Update to processing status
            site.status = BatchSiteStatus.PROCESSING
            self.db.update_batch_site(site)

            if self.on_progress:
                self.on_progress(job, site, f"Processing {site.site_name}")

            # Get all downloaded files
            all_files = list(dataset_dir.glob("tempo_*.nc"))

            if all_files:
                # Process the data
                ds_avg = await asyncio.to_thread(DataProcessor.process_dataset, all_files)
                if ds_avg:
                    output_path = dataset_dir / f"{safe_name}_processed.nc"
                    await asyncio.to_thread(DataProcessor.save_processed, ds_avg, output_path)
                    dataset.file_path = str(output_path)
                    dataset.file_size_mb = output_path.stat().st_size / (1024 * 1024)
                    dataset.status = DatasetStatus.COMPLETE
                else:
                    dataset.status = DatasetStatus.ERROR
            else:
                dataset.status = DatasetStatus.ERROR
                logger.warning(f"No files downloaded for site {site.site_name}")

            dataset.granules_downloaded = len(all_files)
            self.db.update_dataset(dataset)

            # Mark site complete
            site.status = BatchSiteStatus.COMPLETED
            site.completed_at = datetime.now()
            self.db.update_batch_site(site)

            # Update job counts
            job = self.db.get_batch_job(job.id)  # Refresh
            job.completed_sites += 1
            self.db.update_batch_job(job)

            if self.on_site_complete:
                self.on_site_complete(site)

            logger.info(f"Site {site.site_name} completed successfully")

        except Exception as e:
            logger.error(f"Site {site.site_name} failed: {e}")
            site.status = BatchSiteStatus.ERROR
            site.error_message = str(e)
            site.completed_at = datetime.now()
            self.db.update_batch_site(site)

            # Update job counts
            job = self.db.get_batch_job(job.id)  # Refresh
            job.failed_sites += 1
            self.db.update_batch_job(job)

    def _generate_granules(
        self,
        dataset: Dataset,
        date_start,
        date_end,
        day_filter: list[int],
        hour_filter: list[int]
    ) -> list[Granule]:
        """Generate list of granules for a dataset."""
        granules = []
        current = date_start

        while current <= date_end:
            if current.weekday() in day_filter:
                for hour in hour_filter:
                    granules.append(Granule(
                        dataset_id=dataset.id,
                        date=current,
                        hour=hour,
                        bbox_west=dataset.bbox.west,
                        bbox_south=dataset.bbox.south,
                        bbox_east=dataset.bbox.east,
                        bbox_north=dataset.bbox.north,
                        max_cloud=dataset.max_cloud,
                        max_sza=dataset.max_sza,
                    ))
            current += timedelta(days=1)

        return granules


def recover_interrupted_jobs(db: Database) -> int:
    """Recover jobs that were interrupted by app restart.

    Marks RUNNING jobs as PAUSED and resets in-progress sites.

    Args:
        db: Database instance

    Returns:
        Number of jobs recovered
    """
    recovered = 0
    jobs = db.get_all_batch_jobs()

    for job in jobs:
        if job.status == BatchJobStatus.RUNNING:
            job.status = BatchJobStatus.PAUSED
            job.error_message = "Interrupted by app restart"
            db.update_batch_job(job)

            # Reset in-progress sites
            db.reset_interrupted_batch_sites(job.id)

            recovered += 1
            logger.info(f"Recovered interrupted job: {job.name}")

    return recovered


========================================
FILE: src/tempo_app/core/column_exporter.py
========================================
"""Column-Centric Export Job Orchestrator.

This module provides the ExportJob class that runs all column pipelines
and merges the results into a single output.
"""

import pandas as pd
import numpy as np
import xarray as xr
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import logging

from .nodes import (
    ColumnDefinition,
    ExportContext,
    ColumnRunner,
    NodeConfig,
)

logger = logging.getLogger(__name__)


@dataclass
class ExportJobConfig:
    """Configuration for an export job."""
    
    dataset_id: str
    dataset_path: Path
    sites: Dict[str, tuple]  # {code: (lat, lon)}
    columns: List[ColumnDefinition]
    
    # Optional settings
    output_path: Optional[Path] = None
    output_name: str = "export"
    utc_offset: float = -6.0
    
    def to_dict(self) -> dict:
        return {
            "dataset_id": self.dataset_id,
            "dataset_path": str(self.dataset_path),
            "sites": self.sites,
            "columns": [c.to_dict() for c in self.columns],
            "output_name": self.output_name,
            "utc_offset": self.utc_offset,
        }


class ExportJob:
    """Orchestrator that runs all column pipelines and produces final output."""
    
    def __init__(self, config: ExportJobConfig):
        self.config = config
        self._context: Optional[ExportContext] = None
    
    def execute(self) -> pd.DataFrame:
        """Execute all column pipelines and merge results."""
        logger.info(f"Starting export job with {len(self.config.columns)} columns")
        
        # Load dataset
        ds = xr.open_dataset(self.config.dataset_path)
        
        # Create context
        self._context = ExportContext(
            dataset=ds,
            sites=self.config.sites,
            utc_offset=self.config.utc_offset,
        )
        
        # Run each column
        column_results: Dict[str, pd.Series] = {}
        index_df = None
        
        for col_def in self.config.columns:
            logger.info(f"Processing column: {col_def.name}")
            
            try:
                runner = ColumnRunner(col_def)
                result_df = runner.pipeline.run(pd.DataFrame(), self._context)
                
                # Store the result
                if not result_df.empty:
                    # Determine index columns based on what's present
                    index_cols = []
                    for col in ['Site', 'Date', 'Month', 'Hour', 'UTC_Time', 'Local_Time']:
                        if col in result_df.columns:
                            index_cols.append(col)
                    
                    # Extract Value column
                    if 'Value' in result_df.columns:
                        column_results[col_def.name] = result_df.set_index(index_cols)['Value']
                        
                        # Store index for alignment
                        if index_df is None:
                            index_df = result_df[index_cols].copy()
                    
            except Exception as e:
                logger.error(f"Column '{col_def.name}' failed: {e}")
                import traceback
                traceback.print_exc()
        
        ds.close()
        
        # Merge all columns
        if column_results:
            # Create DataFrame from all series
            merged = pd.DataFrame(column_results)
            merged = merged.reset_index()
            return merged
        else:
            logger.warning("No columns produced results")
            return pd.DataFrame()
    
    def export_to_excel(self, output_path: Path = None) -> Path:
        """Execute and save to Excel."""
        result_df = self.execute()
        
        if output_path is None:
            output_path = self.config.output_path or Path.cwd()
        
        output_path.mkdir(parents=True, exist_ok=True)
        file_path = output_path / f"{self.config.output_name}.xlsx"
        
        with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:
            result_df.to_excel(writer, sheet_name='Data', index=False)
            
            # Metadata sheet
            meta = pd.DataFrame({
                'Parameter': ['Dataset', 'Columns', 'Sites'],
                'Value': [
                    self.config.dataset_id,
                    ', '.join(c.name for c in self.config.columns),
                    ', '.join(self.config.sites.keys()),
                ]
            })
            meta.to_excel(writer, sheet_name='Info', index=False)
        
        logger.info(f"Exported to: {file_path}")
        return file_path


def create_default_column(name: str, variable: str = "NO2_TropVCD") -> ColumnDefinition:
    """Create a column with default pipeline: Source -> Nearest -> Hourly."""
    return ColumnDefinition(
        name=name,
        node_configs=[
            NodeConfig("select_variable", {"variable": variable}),
            NodeConfig("nearest_pixel", {}),
            NodeConfig("hourly", {}),
        ]
    )


def create_daily_mean_column(name: str, variable: str = "NO2_TropVCD", 
                             radius_km: float = 10.0) -> ColumnDefinition:
    """Create a column with daily mean aggregation."""
    return ColumnDefinition(
        name=name,
        node_configs=[
            NodeConfig("select_variable", {"variable": variable}),
            NodeConfig("radius_avg", {"radius_km": radius_km}),
            NodeConfig("daily_mean", {"min_hours": 1}),
        ]
    )


def create_filled_column(name: str, variable: str = "NO2_TropVCD") -> ColumnDefinition:
    """Create a column with gap filling."""
    return ColumnDefinition(
        name=name,
        node_configs=[
            NodeConfig("select_variable", {"variable": variable}),
            NodeConfig("nearest_pixel", {}),
            NodeConfig("hourly", {}),
            NodeConfig("gap_fill", {}),
        ]
    )


========================================
FILE: src/tempo_app/core/config.py
========================================
import json
from pathlib import Path
from typing import Dict, Any, Optional

class ConfigManager:
    """Manages application configuration."""
    
    DEFAULT_CONFIG = {
        "data_dir": None,  # None means use default logic
        "font_scale": 1.0,
        "theme_mode": "light", # Reserved for future
        "download_workers": 8,  # Number of parallel download workers
        "rsig_api_key": "",  # NASA RSIG API key (optional but recommended)
    }
    
    def __init__(self, app_name: str = "tempo_analyzer"):
        self.config_dir = Path.home() / f".{app_name}"
        self.config_file = self.config_dir / "config.json"
        self._config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """Load config from file or return defaults."""
        if not self.config_file.exists():
            return self.DEFAULT_CONFIG.copy()
            
        try:
            with open(self.config_file, 'r') as f:
                saved_config = json.load(f)
                # Merge with defaults to ensure all keys exist
                config = self.DEFAULT_CONFIG.copy()
                config.update(saved_config)
                return config
        except Exception as e:
            print(f"Error loading config: {e}")
            return self.DEFAULT_CONFIG.copy()
            
    def save_config(self):
        """Save current config to file."""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        try:
            with open(self.config_file, 'w') as f:
                json.dump(self._config, f, indent=4)
        except Exception as e:
            print(f"Error saving config: {e}")
            
    def get(self, key: str, default: Any = None) -> Any:
        return self._config.get(key, default)
        
    def set(self, key: str, value: Any):
        self._config[key] = value
        self.save_config()

    @property
    def data_dir(self) -> Optional[str]:
        return self._config.get("data_dir")
        
    @property
    def font_scale(self) -> float:
        return self._config.get("font_scale", 1.0)
    
    @property
    def download_workers(self) -> int:
        return self._config.get("download_workers", 4)

    @property
    def rsig_api_key(self) -> str:
        """Get the configured RSIG API key."""
        return self._config.get("rsig_api_key", "")


========================================
FILE: src/tempo_app/core/constants.py
========================================
"""Global constants and configuration for TEMPO Analyzer."""

# Default bounding box (Southern California)
DEFAULT_BBOX = [-119.68, 32.23, -116.38, 35.73]

# List of sites for marking on the map
# Format: 'Code': (Latitude, Longitude)
SITES = {
    # Utah
    'BV': (40.903, -111.884), 'HW': (40.736, -111.872),
    'RB': (40.767, -111.828), 'ER': (40.601, -112.356),
    # Colorado
    'LC': (39.779, -105.005),
    # Arizona
    'PX': (33.504, -112.096),
    # Texas
    'HA': (29.9, -95.33), 'HB': (29.67, -95.5),
    # California
    'PR': (34.01, -118.069), 'BN': (33.921, -116.858),
    'PS': (33.853, -116.541), 'SB': (34.107, -117.274)
}

# FIPS codes for downloading Census road data
STATE_FIPS = {
    '06': 'California', 
    '48': 'Texas', 
    '49': 'Utah', 
    '04': 'Arizona',
    '08': 'Colorado', 
    '36': 'New York', 
    '12': 'Florida'
}

# Region presets for the UI
REGION_PRESETS = {
    'Southern California': ([-119.68, 32.23, -116.38, 35.73], '06'),
    'Utah (Salt Lake)': ([-112.8, 40.0, -111.5, 41.5], '49'),
    'Texas (Houston)': ([-96.5, 29.0, -94.5, 30.5], '48'),
    'Arizona (Phoenix)': ([-113.3, 32.8, -111.0, 34.2], '04'),
    'Colorado (Denver)': ([-105.5, 39.3, -104.3, 40.2], '08'),
}


========================================
FILE: src/tempo_app/core/downloader.py
========================================
"""RSIG Downloader module for TEMPO data with parallel downloading."""

import asyncio
import pandas as pd
import xarray as xr
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import logging
import time
import gc
import traceback

try:
    from pyrsig import RsigApi
except ImportError:
    RsigApi = None

from .status import StatusManager
from .constants import DEFAULT_BBOX

logger = logging.getLogger(__name__)

# Configure parallel download settings
MAX_CONCURRENT_DOWNLOADS = 4  # Number of parallel downloads
DOWNLOAD_TIMEOUT = 180.0      # Timeout per granule in seconds


def format_duration(seconds: float) -> str:
    """Format duration in human-readable format.
    
    Returns:
        < 60s: "45s"
        1m - 60m: "2m 30s"
        > 1h: "1h 15m"
    """
    if seconds < 0:
        return "calculating..."
    
    seconds = int(seconds)
    
    if seconds < 60:
        return f"{seconds}s"
    elif seconds < 3600:
        m, s = divmod(seconds, 60)
        return f"{m}m {s}s" if s else f"{m}m"
    else:
        h, remainder = divmod(seconds, 3600)
        m = remainder // 60
        return f"{h}h {m}m" if m else f"{h}h"





class RSIGDownloader:
    """Handles downloading TEMPO data from EPA RSIG API with parallel execution."""
    
    def __init__(self, workdir: Path, max_concurrent: int = MAX_CONCURRENT_DOWNLOADS, api_key: str = ""):
        self.workdir = workdir
        self.workdir.mkdir(parents=True, exist_ok=True)
        self.max_concurrent = max_concurrent
        self.api_key = api_key  # Configured API key (empty = anonymous)
        
        # Shared state for progress tracking
        self._completed = 0
        self._total = 0
        self._lock = asyncio.Lock()
        
    async def download_granules(self, 
                              dates: list[str], 
                              hours: list[int], 
                              bbox: list[float], 
                              dataset_name: str,
                              max_cloud: float = 0.5, 
                              max_sza: float = 70.0,
                              status: StatusManager = None) -> list[Path]:
        """
        Download TEMPO granules using daily batch requests with controlled parallelism.
        
        This approach uses daily batches (instead of per-hour) to reduce server load,
        while still allowing parallel processing of multiple days for speed.
        
        Args:
            dates: List of date strings (YYYY-MM-DD)
            hours: List of UTC hours (0-23)
            bbox: [west, south, east, north]
            dataset_name: Name of the dataset (for file naming)
            max_cloud: Maximum cloud fraction (0-1)
            max_sza: Maximum solar zenith angle (deg)
            status: StatusManager for UI updates
        
        Returns:
            List of paths to downloaded .nc files
        """
        if RsigApi is None:
            if status: status.emit("error", "pyrsig library not installed!")
            return await self._simulate_download(dates, hours, dataset_name, status)
        
        dataset_dir = self.workdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        
        # Determine hour range for daily requests
        min_hour = min(hours)
        max_hour = max(hours)
        
        self._total = len(dates)  # Counting days
        self._completed = 0
        
        if status:
            status.emit("info", f"üöÄ Daily batch download: {self._total} days √ó {len(hours)} hours, {self.max_concurrent} workers")
        
        logger.info(f"[BATCH] Downloading {self._total} days with {self.max_concurrent} parallel workers")
        
        # Use configured API key or anonymous
        api_key = self.api_key if self.api_key else "anonymous"
        
        start_time = time.time()
        
        # Create semaphore for controlled parallelism
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def download_day_worker(d_str: str, worker_id: int) -> list[Path]:
            """Worker that downloads one day with its own API session."""
            async with semaphore:
                import tempfile
                import shutil
                
                # Each worker gets its own temp directory and API session
                temp_dir = tempfile.mkdtemp(prefix=f"rsig_w{worker_id}_")
                
                try:
                    # Create API session for this worker
                    api = RsigApi(bbox=bbox, workdir=temp_dir, grid_kw='1US1', gridfit=True)
                    api.tempo_kw.update({
                        'minimum_quality': 'normal',
                        'maximum_cloud_fraction': max_cloud,
                        'maximum_solar_zenith_angle': max_sza,
                        'api_key': api_key
                    })
                    
                    if status:
                        async with self._lock:
                            progress = self._completed / self._total
                        status.emit("download", f"‚¨áÔ∏è W{worker_id}: {d_str}", progress)
                    
                    # Download entire day in one request
                    result = await self._download_daily_batch(
                        api, d_str, min_hour, max_hour, hours, dataset_dir, status
                    )
                    
                    async with self._lock:
                        self._completed += 1
                    
                    return result if result else []
                    
                except Exception as e:
                    logger.error(f"[BATCH] Worker {worker_id} failed for {d_str}: {e}")
                    if status:
                        status.emit("error", f"‚ùå W{worker_id}: {d_str} - {e}")
                    async with self._lock:
                        self._completed += 1
                    return []
                    
                finally:
                    try:
                        shutil.rmtree(temp_dir, ignore_errors=True)
                    except:
                        pass
        
        # Launch all day downloads in parallel (semaphore controls actual concurrency)
        tasks = [download_day_worker(d_str, i % self.max_concurrent + 1) for i, d_str in enumerate(dates)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Collect results
        saved_files = []
        errors = 0
        for result in results:
            if isinstance(result, Exception):
                errors += 1
                logger.error(f"[BATCH] Task exception: {result}")
            elif result:
                saved_files.extend(result)
        
        elapsed = time.time() - start_time
        
        logger.info(f"[BATCH] Complete: {len(saved_files)} files, {errors} errors in {elapsed:.1f}s")
        if status:
            status.emit("ok", f"‚úÖ Downloaded {len(saved_files)} granules from {self._total} days in {format_duration(elapsed)}")
        
        return saved_files
    
    async def _download_daily_batch(
        self,
        api: 'RsigApi',
        d_str: str,
        min_hour: int,
        max_hour: int,
        hours: list[int],
        dataset_dir: Path,
        status: StatusManager
    ) -> list[Path]:
        """Download an entire day's worth of data in one request.
        
        Uses a persistent API session to maintain connection and reduce overhead.
        Splits the returned data into per-hour files for consistent processing.
        """
        logger.info(f"[BATCH] Fetching day {d_str} (hours {min_hour:02d}-{max_hour:02d})")
        
        def _fetch_day():
            """Synchronous fetch for entire day range."""
            d_obj = pd.to_datetime(d_str)
            bdate = d_obj + pd.to_timedelta(min_hour, unit='h')
            edate = d_obj + pd.to_timedelta(max_hour, unit='h') + pd.to_timedelta('59m')
            
            logger.info(f"[BATCH] Requesting NO2: {bdate} to {edate}")
            no2ds = api.to_ioapi('tempo.l2.no2.vertical_column_troposphere', bdate=bdate, edate=edate)
            
            logger.info(f"[BATCH] Requesting HCHO: {bdate} to {edate}")
            hchods = api.to_ioapi('tempo.l2.hcho.vertical_column', bdate=bdate, edate=edate)
            
            return no2ds, hchods
        
        try:
            no2ds, hchods = await asyncio.wait_for(
                asyncio.to_thread(_fetch_day),
                timeout=DOWNLOAD_TIMEOUT * 3  # Longer timeout for daily batches
            )
        except asyncio.TimeoutError:
            logger.error(f"[BATCH] Timeout fetching day {d_str}")
            if status:
                status.emit("error", f"‚è±Ô∏è Timeout: {d_str}")
            return []
        except Exception as e:
            error_str = str(e)
            if "Unknown file format" in error_str or "NetCDF: Unknown file format" in error_str:
                logger.info(f"[BATCH] No data available for {d_str}")
                return []
            raise
        
        # Split into per-hour files for consistent downstream processing
        saved = []
        
        try:
            # Get timestamps from the response
            if 'TSTEP' in no2ds.dims:
                timestamps = pd.to_datetime(no2ds.TSTEP.values)
            else:
                # Single timestep, just use requested hours
                timestamps = [pd.to_datetime(d_str) + pd.to_timedelta(h, unit='h') for h in hours]
            
            # Group data by hour
            for hour in hours:
                # Check if this hour has data
                hour_data_mask = [t.hour == hour for t in timestamps]
                if not any(hour_data_mask):
                    continue
                
                filename = f"tempo_{d_str}_{hour:02d}.nc"
                filepath = dataset_dir / filename
                
                # Extract data for this hour
                if 'TSTEP' in no2ds.dims:
                    hour_indices = [i for i, m in enumerate(hour_data_mask) if m]
                    if not hour_indices:
                        continue
                    
                    no2_hour = no2ds.isel(TSTEP=hour_indices)
                    hcho_hour = hchods.isel(TSTEP=hour_indices) if 'TSTEP' in hchods.dims else hchods
                else:
                    no2_hour = no2ds
                    hcho_hour = hchods
                
                # Create output dataset
                outds = xr.Dataset(attrs=dict(no2ds.attrs))
                
                if 'LATITUDE' in no2_hour:
                    lat_data = no2_hour['LATITUDE']
                    if 'TSTEP' in lat_data.dims:
                        lat_data = lat_data.isel(TSTEP=0)
                    if 'LAY' in lat_data.dims:
                        lat_data = lat_data.isel(LAY=0)
                    outds.coords['LAT'] = (('ROW', 'COL'), lat_data.values.copy())
                    
                if 'LONGITUDE' in no2_hour:
                    lon_data = no2_hour['LONGITUDE']
                    if 'TSTEP' in lon_data.dims:
                        lon_data = lon_data.isel(TSTEP=0)
                    if 'LAY' in lon_data.dims:
                        lon_data = lon_data.isel(LAY=0)
                    outds.coords['LON'] = (('ROW', 'COL'), lon_data.values.copy())
                
                n_var = no2_hour.get('NO2_VERTICAL_CO', xr.DataArray(np.nan))
                h_var = hcho_hour.get('VERTICAL_COLUMN', xr.DataArray(np.nan))
                
                if 'LAY' in n_var.dims: n_var = n_var.isel(LAY=0)
                if 'LAY' in h_var.dims: h_var = h_var.isel(LAY=0)
                if 'TSTEP' in n_var.dims: n_var = n_var.mean(dim='TSTEP')
                if 'TSTEP' in h_var.dims: h_var = h_var.mean(dim='TSTEP')
                
                outds['NO2_TropVCD'] = xr.DataArray(n_var.values.copy(), dims=n_var.dims, attrs=dict(n_var.attrs) if hasattr(n_var, 'attrs') else {})
                outds['HCHO_TotVCD'] = xr.DataArray(h_var.values.copy(), dims=h_var.dims, attrs=dict(h_var.attrs) if hasattr(h_var, 'attrs') else {})
                
                # Check validity
                if outds['NO2_TropVCD'].isnull().all() and outds['HCHO_TotVCD'].isnull().all():
                    continue
                
                # Save file
                await asyncio.to_thread(lambda: outds.to_netcdf(filepath, engine='netcdf4', compute=True))
                outds.close()
                
                fsize = filepath.stat().st_size
                if fsize > 1000:
                    saved.append(filepath)
                    logger.info(f"[BATCH] Saved: {filename} ({fsize/1024:.1f} KB)")
                else:
                    filepath.unlink()
                    
        finally:
            no2ds.close()
            hchods.close()
        
        return saved
    
    async def _download_single_granule(
        self, 
        d_str: str, 
        hour: int, 
        bbox: list[float],
        max_cloud: float,
        max_sza: float,
        dataset_dir: Path,
        status: StatusManager
    ) -> Path | None:
        """Legacy method - kept for backwards compatibility but not used."""
        # Now handled by _download_daily_batch
        return None
    
    
    async def _save_granule(
        self, 
        ds: xr.Dataset, 
        filepath: Path, 
        filename: str, 
        progress: float,
        status: StatusManager
    ) -> Path | None:
        """Save a downloaded dataset to disk with robust error handling."""
        
        logger.info(f"[SAVE] Saving: {filepath}")
        
        # Delete existing file if present
        if filepath.exists():
            logger.info(f"[SAVE] Deleting existing file: {filepath}")
            gc.collect()
            try:
                filepath.unlink()
            except PermissionError:
                await asyncio.sleep(0.5)
                gc.collect()
                try:
                    filepath.unlink()
                except PermissionError as e:
                    logger.error(f"[SAVE] Cannot delete locked file: {e}")
                    if status:
                        status.emit("error", f"üîí File locked: {filename}")
                    ds.close()
                    return None
        
        # Save to netCDF
        try:
            await asyncio.to_thread(
                lambda: ds.to_netcdf(filepath, engine='netcdf4', compute=True)
            )
            ds.close()
        except PermissionError as e:
            logger.error(f"[SAVE] Permission denied: {e}")
            if status:
                status.emit("error", f"üîí Permission denied: {filename}")
            ds.close()
            return None
        except Exception as e:
            logger.error(f"[SAVE] Save error: {e}")
            logger.error(f"[SAVE] Traceback:\n{traceback.format_exc()}")
            if status:
                status.emit("error", f"‚ùå Save failed: {filename} - {e}")
            ds.close()
            return None
        
        # Validate saved file
        if not filepath.exists():
            logger.error(f"[SAVE] File missing after save!")
            if status:
                status.emit("error", f"‚ùå File disappeared: {filename}")
            return None
        
        fsize = filepath.stat().st_size
        if fsize < 1000:
            logger.warning(f"[SAVE] File too small: {fsize} bytes")
            if status:
                status.emit("error", f"‚ö†Ô∏è File too small: {filename}")
            filepath.unlink()
            return None
        
        logger.info(f"[SAVE] Success: {filename} ({fsize/1024:.1f} KB)")
        if status:
            status.emit("download", f"‚úÖ {filename} ({fsize/1024:.1f} KB)", progress)
        
        return filepath
    
    async def _simulate_download(self, dates, hours, dataset_name, status):
        """Fallback simulation for testing without credentials/deps."""
        dataset_dir = self.workdir
        dataset_dir.mkdir(parents=True, exist_ok=True)
        saved = []
        total = len(dates) * len(hours)
        curr = 0
        
        for d in dates:
            for h in hours:
                curr += 1
                if status: status.emit("download", f"Simulating: {d} @ {h:02d}:00", curr/total)
                await asyncio.sleep(0.3)  # Faster simulation
                # Create dummy file
                p = dataset_dir / f"tempo_{d}_{h:02d}.nc"
                with open(p, 'w') as f: f.write("dummy netcdf")
                saved.append(p)
                
        return saved


========================================
FILE: src/tempo_app/core/exporter.py
========================================
"""Data exporter module for TEMPO Analyzer."""

import pandas as pd
import xarray as xr
import numpy as np
from pathlib import Path
import logging
from typing import Optional
from math import radians, sin, cos, sqrt, atan2

from .constants import SITES

logger = logging.getLogger(__name__)

MISSING_VALUE = -999


def haversine(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculate great-circle distance between two points in kilometers."""
    R = 6371.0  # Earth radius in kilometers
    lat1_rad, lon1_rad = radians(lat1), radians(lon1)
    lat2_rad, lon2_rad = radians(lat2), radians(lon2)
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad
    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c


def find_n_nearest_cells(lat: float, lon: float, lats_2d: np.ndarray,
                         lons_2d: np.ndarray, n: int) -> list[tuple]:
    """Find the N nearest grid cells to a target location."""
    distances = np.zeros_like(lats_2d)
    rows, cols = lats_2d.shape
    for i in range(rows):
        for j in range(cols):
            distances[i, j] = haversine(lat, lon, lats_2d[i, j], lons_2d[i, j])
    flat_idx = np.argsort(distances, axis=None)[:n]
    row_indices, col_indices = np.unravel_index(flat_idx, distances.shape)
    result = []
    for r, c in zip(row_indices, col_indices):
        result.append((int(r), int(c), float(distances[r, c])))
    return result


def find_cells_within_distance(lat: float, lon: float, lats_2d: np.ndarray,
                               lons_2d: np.ndarray, max_distance_km: float) -> list[tuple]:
    """Find all grid cells within a specified distance from a target location.
    
    Args:
        lat, lon: Target coordinates
        lats_2d, lons_2d: Grid coordinates arrays
        max_distance_km: Maximum distance in kilometers
        
    Returns:
        List of (row, col, distance) tuples sorted by distance
    """
    distances = np.zeros_like(lats_2d)
    rows, cols = lats_2d.shape
    for i in range(rows):
        for j in range(cols):
            distances[i, j] = haversine(lat, lon, lats_2d[i, j], lons_2d[i, j])
    
    # Find all indices where distance <= max_distance_km
    row_indices, col_indices = np.where(distances <= max_distance_km)
    
    result = []
    for r, c in zip(row_indices, col_indices):
        result.append((int(r), int(c), float(distances[r, c])))
        
    # Sort by distance
    result.sort(key=lambda x: x[2])
    
    return result





def filter_sites_in_bbox(sites: dict, dataset: xr.Dataset) -> dict:
    """Filter sites to only those within dataset's bounding box."""
    if 'LAT' not in dataset.coords:
        return {}
    lats = dataset['LAT'].values
    lons = dataset['LON'].values
    min_lat, max_lat = lats.min(), lats.max()
    min_lon, max_lon = lons.min(), lons.max()
    return {
        site: coords for site, coords in sites.items()
        if (min_lat <= coords[0] <= max_lat) and (min_lon <= coords[1] <= max_lon)
    }


def apply_monthly_hourly_fill(df: pd.DataFrame, value_columns: list) -> pd.DataFrame:
    """Fill NaN values using monthly-hourly climatological means."""
    df_filled = df.copy()
    group_cols = []
    if 'Site' in df.columns:
        group_cols.append('Site')
    if 'Month' in df.columns:
        group_cols.append('Month')
    if 'Hour' in df.columns:
        group_cols.append('Hour')
    if not group_cols:
        return df_filled
    means = df_filled.groupby(group_cols)[value_columns].transform('mean')
    df_filled[value_columns] = df_filled[value_columns].fillna(means)
    return df_filled


class DataExporter:
    """Handles exporting processed data to Excel formats."""

    def __init__(self, output_dir: Path):
        """Initialize exporter with output directory."""
        self.output_dir = output_dir / "exports"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export_dataset(self,
                      dataset: xr.Dataset,
                      dataset_name: str,
                      export_format: str,
                      num_points: Optional[int] = None,
                      distance_km: Optional[float] = None,
                      utc_offset: float = -6.0,
                      metadata: Optional[dict] = None,
                      sites: Optional[dict] = None) -> list[str]:
        """Export dataset in specified format.

        Args:
            dataset: xarray.Dataset with LAT, LON coords
            dataset_name: Base name for output files
            export_format: One of "hourly_multicell", "daily_aggregated", "spatial_average"
            num_points: Number of nearest cells to include. Defaults depend on format.
            distance_km: Radius in km to include cells (overrides num_points if set)
            utc_offset: Hours offset from UTC for local time (default -6.0)
            metadata: Optional dictionary of dataset metadata (settings, stats)
            sites: Dict mapping site code to (lat, lon) tuple. Uses database sites if None.

        Returns:
            List of generated file paths
        """
        # Use provided sites or fall back to hardcoded SITES constant
        sites_to_use = sites if sites is not None else SITES

        if export_format == "hourly" or export_format == "hourly_multicell":
            np = num_points if num_points is not None else 9
            return self._export_hourly(dataset, dataset_name, utc_offset, np, distance_km, metadata, sites_to_use)
        elif export_format == "daily" or export_format == "daily_aggregated":
            np = num_points if num_points is not None else 9
            return self._export_daily(dataset, dataset_name, utc_offset, np, distance_km, metadata, sites_to_use)
        elif export_format == "spatial_average":
            np = num_points if num_points is not None else 4 # Default for spatial average
            return self._export_spatial_average(dataset, dataset_name, utc_offset, np, distance_km, metadata, sites_to_use)
        else:
            raise ValueError(f"Unknown export format: {export_format}")

    def _create_metadata_df(self, metadata: dict) -> pd.DataFrame:
        """Create a DataFrame from metadata dictionary for export."""
        if not metadata:
            return pd.DataFrame({'Parameter': ['No metadata available'], 'Value': ['']})
        
        rows = []
        # Add basic settings first
        settings_keys = ['max_cloud', 'max_sza', 'date_start', 'date_end', 'day_filter', 'hour_filter']
        for k in settings_keys:
            if k in metadata:
                rows.append({'Parameter': k, 'Value': str(metadata[k])})
        
        # Add any other keys
        for k, v in metadata.items():
            if k not in settings_keys:
                rows.append({'Parameter': k, 'Value': str(v)})
                
        return pd.DataFrame(rows)

    def _get_time_info(self, dataset: xr.Dataset):
        """Extract time dimension and values from dataset."""
        # Check for datetime dimensions: TIME (new), TSTEP (old), then HOUR
        if 'TIME' in dataset.dims:
            return 'TIME', pd.to_datetime(dataset['TIME'].values)
        elif 'TSTEP' in dataset.dims:
            return 'TSTEP', pd.to_datetime(dataset['TSTEP'].values)
        elif 'HOUR' in dataset.dims:
            # HOUR dimension - check if datetime or just integers
            hour_values = dataset['HOUR'].values
            if np.issubdtype(hour_values.dtype, np.datetime64):
                return 'HOUR', pd.to_datetime(hour_values)
            else:
                # Create datetime index from hour integers (use placeholder date)
                hours = [int(h) for h in hour_values]
                return 'HOUR', hours  # Return raw hours, handle downstream
        else:
            return None, None

    def _export_hourly(self, dataset: xr.Dataset, dataset_name: str,
                       utc_offset: float, num_points: int = 9,
                       distance_km: Optional[float] = None,
                       metadata: Optional[dict] = None,
                       sites: Optional[dict] = None) -> list[str]:
        """Export hourly format - separate file per site with N cells.

        Columns: UTC_Time, Local_Time (UTC-X.0), Cell1_NO2...CellN_NO2, Cell1_HCHO...CellN_HCHO
        """
        time_dim, time_values = self._get_time_info(dataset)
        if time_dim is None or 'LAT' not in dataset.coords:
            logger.warning("Missing required coords/dims")
            return []

        lats = dataset['LAT'].values
        lons = dataset['LON'].values

        sites_to_use = sites if sites is not None else SITES
        valid_sites = filter_sites_in_bbox(sites_to_use, dataset)
        if not valid_sites:
            logger.warning("No sites found within dataset bounds")
            return []

        # Handle time values
        if isinstance(time_values, pd.DatetimeIndex):
            utc_times = time_values
            local_times = utc_times + pd.Timedelta(hours=utc_offset)
            utc_col = utc_times
            local_col = local_times
        else:
            # Hours only - create time columns as strings
            hours = time_values
            utc_col = [f"{h:02d}:00 UTC" for h in hours]
            local_hours = [(h + int(utc_offset)) % 24 for h in hours]
            local_col = [f"{h:02d}:00 Local" for h in local_hours]

        generated_files = []

        for site, (t_lat, t_lon) in valid_sites.items():
            # Find cells
            if distance_km is not None:
                cells = find_cells_within_distance(t_lat, t_lon, lats, lons, distance_km)
            else:
                cells = find_n_nearest_cells(t_lat, t_lon, lats, lons, num_points)
            
            # Build data dictionary
            data = {
                'UTC_Time': utc_col,
                f'Local_Time (UTC{utc_offset:+.1f})': local_col,
            }

            # Extract NO2 and HCHO interleaved for each cell
            for i, (r, c, dist) in enumerate(cells):
                cell_num = i + 1
                if 'NO2_TropVCD' in dataset:
                    values = dataset['NO2_TropVCD'].isel(ROW=r, COL=c).values.flatten()
                    # Keep NaN as empty, not -999
                    data[f'Cell{cell_num}_NO2'] = values
                if 'HCHO_TotVCD' in dataset:
                    values = dataset['HCHO_TotVCD'].isel(ROW=r, COL=c).values.flatten()
                    data[f'Cell{cell_num}_HCHO'] = values

            # Create hourly data DataFrame
            df = pd.DataFrame(data)
            
            # Add Month and Hour columns for fill calculation
            if isinstance(time_values, pd.DatetimeIndex):
                df['Month'] = time_values.month
                df['Hour'] = time_values.hour
            else:
                # If only hours, we need dates from somewhere - assume single day
                df['Month'] = 1  # Placeholder
                df['Hour'] = [int(h) for h in time_values]
            
            # Get value columns (NO2 and HCHO)
            value_cols = [c for c in df.columns if '_NO2' in c or '_HCHO' in c]
            
            # Create NoFill version (replace NaN with -999)
            df_nofill = df.copy()
            for col in value_cols:
                df_nofill[col] = df_nofill[col].fillna(MISSING_VALUE)
            
            # Create Fill version (apply monthly-hourly mean fill)
            df_fill = apply_monthly_hourly_fill(df.copy(), value_cols)
            for col in value_cols:
                df_fill[col] = df_fill[col].fillna(MISSING_VALUE)  # Any remaining NaN -> -999
            
            # Drop Month/Hour from output (they were just for grouping)
            cols_to_drop = ['Month', 'Hour']
            df_nofill_out = df_nofill.drop(columns=cols_to_drop, errors='ignore')
            df_fill_out = df_fill.drop(columns=cols_to_drop, errors='ignore')
            
            # Create Grid_Info sheet with cell metadata
            grid_info = []
            for i, (r, c, dist) in enumerate(cells):
                cell_num = i + 1
                cell_lat = float(lats[r, c])
                cell_lon = float(lons[r, c])
                grid_info.append({
                    'Cell_ID': f'Cell{cell_num}',
                    'Lat': cell_lat,
                    'Lon': cell_lon,
                    'Dist_km': dist,
                    'Grid_Row': int(r),
                    'Grid_Col': int(c),
                })
            df_grid = pd.DataFrame(grid_info)
            
            # Save to Excel with multiple sheets
            fname = self.output_dir / f"{site}_{dataset_name}_hourly_multicell.xlsx"
            with pd.ExcelWriter(fname, engine='openpyxl') as writer:
                df_nofill_out.to_excel(writer, sheet_name='Hourly_NoFill', index=False)
                df_fill_out.to_excel(writer, sheet_name='Hourly_Fill', index=False)
                df_grid.to_excel(writer, sheet_name='Grid_Info', index=False)
                if metadata:
                    # Add site-specific stats to metadata
                    meta_df = self._create_metadata_df(metadata)
                    # Calculate missing data stats for this site
                    total_pts = len(df)
                    no2_missing = df.filter(like='_NO2').isna().sum().sum()
                    no2_total = df.filter(like='_NO2').size
                    hcho_missing = df.filter(like='_HCHO').isna().sum().sum()
                    hcho_total = df.filter(like='_HCHO').size
                    
                    # Count fill values applied
                    nofill_missing = (df_nofill_out.filter(like='_NO2') == MISSING_VALUE).sum().sum()
                    fill_missing = (df_fill_out.filter(like='_NO2') == MISSING_VALUE).sum().sum()
                    filled_count = nofill_missing - fill_missing
                    
                    stats_rows = pd.DataFrame([
                        {'Parameter': 'Site', 'Value': site},
                        {'Parameter': 'Total_Time_Steps', 'Value': total_pts},
                        {'Parameter': 'NO2_Missing_Pct', 'Value': f"{(no2_missing/no2_total)*100:.1f}%" if no2_total else "0%"},
                        {'Parameter': 'HCHO_Missing_Pct', 'Value': f"{(hcho_missing/hcho_total)*100:.1f}%" if hcho_total else "0%"},
                        {'Parameter': 'Fill_Applied_Count', 'Value': filled_count}
                    ])
                    meta_final = pd.concat([meta_df, stats_rows], ignore_index=True)
                    meta_final.to_excel(writer, sheet_name='Metadata', index=False)
            
            generated_files.append(str(fname))

        return generated_files

    def _export_daily(self, dataset: xr.Dataset, dataset_name: str,
                      utc_offset: float, num_points: int = 8,
                      distance_km: Optional[float] = None,
                      metadata: Optional[dict] = None,
                      sites: Optional[dict] = None) -> list[str]:
        """Export daily format - single file with all sites.

        Columns: Date, Site, TMP_NO2_NoFill_Ngridcells, TMP_NO2_NoFill_Ncnt, ...
        Uses hours 08-14 (inclusive) local time.
        Uses configurable num_points (default 8) for cell selection.
        """
        time_dim, time_values = self._get_time_info(dataset)
        if time_dim is None or 'LAT' not in dataset.coords:
            logger.warning("Missing required coords/dims")
            return []

        lats = dataset['LAT'].values
        lons = dataset['LON'].values

        sites_to_use = sites if sites is not None else SITES
        valid_sites = filter_sites_in_bbox(sites_to_use, dataset)
        if not valid_sites:
            logger.warning("No sites found within dataset bounds")
            return []

        # Handle time values - require full timestamps
        if isinstance(time_values, pd.DatetimeIndex):
            utc_times = time_values
            local_times = utc_times + pd.Timedelta(hours=utc_offset)
        else:
            # Dataset doesn't have proper timestamps - needs reprocessing
            logger.warning("Daily export requires full timestamps. Please reprocess the dataset.")
            return []

        all_rows = []

        for site, (t_lat, t_lon) in valid_sites.items():
            # Find cells
            if distance_km is not None:
                cells = find_cells_within_distance(t_lat, t_lon, lats, lons, distance_km)
                # For radius mode, use TEMPO_NoFill_NO2_Xkm style columns
                radius_str = f"{int(distance_km)}km" if distance_km == int(distance_km) else f"{distance_km}km"
                use_radius_naming = True
            else:
                cells = find_n_nearest_cells(t_lat, t_lon, lats, lons, num_points)
                use_radius_naming = False
            
            n_cells = len(cells)
            
            # Extract raw data for all cells
            raw_data = {
                'Local_Time': local_times,
                'Date': local_times.date,
                'Hour': local_times.hour,
                'Month': local_times.month,
                'Site': site,
            }
            
            # Extract NO2 and HCHO for each cell
            for i, (r, c, dist) in enumerate(cells):
                if 'NO2_TropVCD' in dataset:
                    raw_data[f'Cell{i}_NO2'] = dataset['NO2_TropVCD'].isel(ROW=r, COL=c).values.flatten()
                if 'HCHO_TotVCD' in dataset:
                    raw_data[f'Cell{i}_HCHO'] = dataset['HCHO_TotVCD'].isel(ROW=r, COL=c).values.flatten()
            
            df_site = pd.DataFrame(raw_data)
            
            # Filter to hours 8-14 local time
            df_filtered = df_site[(df_site['Hour'] >= 8) & (df_site['Hour'] <= 14)].copy()
            
            if df_filtered.empty:
                continue
            
            # Create filled version
            no2_cols = [f'Cell{i}_NO2' for i in range(n_cells) if f'Cell{i}_NO2' in df_filtered.columns]
            hcho_cols = [f'Cell{i}_HCHO' for i in range(n_cells) if f'Cell{i}_HCHO' in df_filtered.columns]
            value_cols = no2_cols + hcho_cols
            
            df_filled = apply_monthly_hourly_fill(df_filtered, value_cols)
            
            # Aggregate by date
            for date, grp in df_filtered.groupby('Date'):
                row = {'Date': date, 'Site': site}
                grp_filled = df_filled[df_filled['Date'] == date]
                
                # Column naming based on mode
                if use_radius_naming:
                    no2_nofill_col = f'TEMPO_NoFill_NO2_{radius_str}'
                    no2_nofill_cnt_col = 'TEMPO_NoFill_NO2_Cnt'
                    hcho_nofill_col = f'TEMPO_NoFill_HCHO_{radius_str}'
                    hcho_nofill_cnt_col = 'TEMPO_NoFill_HCHO_Cnt'
                    no2_fill_col = f'TEMPO_Fill_NO2_{radius_str}'
                    no2_fill_cnt_col = 'TEMPO_Fill_NO2_Cnt'
                    hcho_fill_col = f'TEMPO_Fill_HCHO_{radius_str}'
                    hcho_fill_cnt_col = 'TEMPO_Fill_HCHO_Cnt'
                else:
                    label = str(n_cells)
                    no2_nofill_col = f'NO2_NoFill_{label}_Avg'
                    no2_nofill_cnt_col = f'NO2_NoFill_{label}_Cnt'
                    hcho_nofill_col = f'HCHO_NoFill_{label}_Avg'
                    hcho_nofill_cnt_col = f'HCHO_NoFill_{label}_Cnt'
                    no2_fill_col = f'NO2_Fill_{label}_Avg'
                    no2_fill_cnt_col = f'NO2_Fill_{label}_Cnt'
                    hcho_fill_col = f'HCHO_Fill_{label}_Avg'
                    hcho_fill_cnt_col = f'HCHO_Fill_{label}_Cnt'
                
                # NoFill
                if no2_cols:
                    no2_vals = grp[no2_cols].values.flatten()
                    no2_valid = no2_vals[~np.isnan(no2_vals)]
                    # Also filter out fill values
                    no2_valid = no2_valid[(no2_valid > -900) & (no2_valid < 1e20)]
                    row[no2_nofill_col] = np.mean(no2_valid) if len(no2_valid) > 0 else MISSING_VALUE
                    row[no2_nofill_cnt_col] = len(no2_valid)
                
                if hcho_cols:
                    hcho_vals = grp[hcho_cols].values.flatten()
                    hcho_valid = hcho_vals[~np.isnan(hcho_vals)]
                    hcho_valid = hcho_valid[(hcho_valid > -900) & (hcho_valid < 1e20)]
                    row[hcho_nofill_col] = np.mean(hcho_valid) if len(hcho_valid) > 0 else MISSING_VALUE
                    row[hcho_nofill_cnt_col] = len(hcho_valid)
                
                # Fill
                if no2_cols:
                    no2_vals_f = grp_filled[no2_cols].values.flatten()
                    no2_valid_f = no2_vals_f[~np.isnan(no2_vals_f)]
                    no2_valid_f = no2_valid_f[(no2_valid_f > -900) & (no2_valid_f < 1e20)]
                    row[no2_fill_col] = np.mean(no2_valid_f) if len(no2_valid_f) > 0 else MISSING_VALUE
                    row[no2_fill_cnt_col] = len(no2_valid_f)
                
                if hcho_cols:
                    hcho_vals_f = grp_filled[hcho_cols].values.flatten()
                    hcho_valid_f = hcho_vals_f[~np.isnan(hcho_vals_f)]
                    hcho_valid_f = hcho_valid_f[(hcho_valid_f > -900) & (hcho_valid_f < 1e20)]
                    row[hcho_fill_col] = np.mean(hcho_valid_f) if len(hcho_valid_f) > 0 else MISSING_VALUE
                    row[hcho_fill_cnt_col] = len(hcho_valid_f)
                
                all_rows.append(row)
        
        if not all_rows:
            return []
        
        # Create final DataFrame and order columns properly
        df_final = pd.DataFrame(all_rows)
        
        # Order: Date, Site, then NoFill columns (smaller cell count first), then Fill columns
        base_cols = ['Date', 'Site']
        other_cols = [c for c in df_final.columns if c not in base_cols]
        
        # Sort columns: NoFill before Fill, smaller cell count first, NO2 before HCHO
        def col_sort_key(col):
            fill_order = 0 if 'NoFill' in col else 1
            no2_order = 0 if 'NO2' in col else 1
            cnt_order = 1 if 'Cnt' in col else 0
            # Extract cell count from column name
            import re
            match = re.search(r'_(\d+)_', col)
            cell_count = int(match.group(1)) if match else 0
            return (fill_order, cell_count, no2_order, cnt_order)
        
        other_cols.sort(key=col_sort_key)
        df_final = df_final[base_cols + other_cols]
        df_final = df_final.sort_values(['Date', 'Site']).reset_index(drop=True)
        
        # Save
        fname = self.output_dir / f"{dataset_name}_daily_aggregated.xlsx"
        with pd.ExcelWriter(fname, engine='openpyxl') as writer:
            df_final.to_excel(writer, sheet_name='Daily_Data', index=False)
            if metadata:
                meta_df = self._create_metadata_df(metadata)
                
                # Calculate stats for daily data
                stats_data = []
                total_rows = len(df_final)
                stats_data.append({'Parameter': 'Total_Site_Days', 'Value': total_rows})
                
                # Check missing data in NoFill columns
                nofill_cols = [c for c in df_final.columns if 'NoFill' in c and 'Avg' in c]
                for col in nofill_cols:
                    # MISSING_VALUE is -999
                    missing_cnt = (df_final[col] == MISSING_VALUE).sum()
                    stats_data.append({
                        'Parameter': f'{col}_Missing_Pct', 
                        'Value': f"{(missing_cnt/total_rows)*100:.1f}%" if total_rows else "0%"
                    })
                
                stats_df = pd.DataFrame(stats_data)
                meta_final = pd.concat([meta_df, stats_df], ignore_index=True)
                meta_final.to_excel(writer, sheet_name='Metadata', index=False)
        
        return [str(fname)]

    def _export_spatial_average(self, dataset: xr.Dataset, dataset_name: str,
                              utc_offset: float, num_points: int = 9,
                              distance_km: Optional[float] = None,
                              metadata: Optional[dict] = None,
                              sites: Optional[dict] = None) -> list[str]:
        """Export spatial average format - single file with spatial means per site."""
        time_dim, time_values = self._get_time_info(dataset)
        if time_dim is None or 'LAT' not in dataset.coords:
            logger.warning("Missing required coords/dims")
            return []

        lats = dataset['LAT'].values
        lons = dataset['LON'].values

        sites_to_use = sites if sites is not None else SITES
        valid_sites = filter_sites_in_bbox(sites_to_use, dataset)
        if not valid_sites:
            logger.warning("No sites found within dataset bounds")
            return []

        # Time handling
        if isinstance(time_values, pd.DatetimeIndex):
            utc_times = time_values
            local_times = utc_times + pd.Timedelta(hours=utc_offset)
        else:
            hours = time_values
            # Create dummy dates for hours
            base_date = pd.Timestamp.now().normalize()
            utc_times = [base_date + pd.Timedelta(hours=int(h)) for h in hours]
            local_times = [t + pd.Timedelta(hours=utc_offset) for t in utc_times]
            utc_times = pd.DatetimeIndex(utc_times)
            local_times = pd.DatetimeIndex(local_times)

        # Prepare DataFrames
        df_raw = pd.DataFrame({
            'UTC': utc_times,
            'Local': local_times,
            'Date': local_times.date,
            'Hour': local_times.hour
        })
        
        grid_cells_info = []
        site_stats = []

        for site, (t_lat, t_lon) in valid_sites.items():
             # Find cells
            if distance_km is not None:
                cells = find_cells_within_distance(t_lat, t_lon, lats, lons, distance_km)
            else:
                cells = find_n_nearest_cells(t_lat, t_lon, lats, lons, num_points)
            
            site_stats.append({'Site': site, 'Points': len(cells)})
            
            # Record grid cells info
            for r, c, dist in cells:
                grid_cells_info.append({
                    'Site': site,
                    'Grid_Lat': float(lats[r, c]),
                    'Grid_Lon': float(lons[r, c]),
                    'Dist (km)': dist
                })

            # Calculate spatial means
            no2_means = []
            hcho_means = []
            
            # Extract data for all timesteps
            for t_idx in range(len(utc_times)):
                # Get values for all cells at this timestep
                no2_vals = []
                hcho_vals = []
                
                for r, c, _ in cells:
                    if 'NO2_TropVCD' in dataset:
                        # Handle TIME, TSTEP, or HOUR dimension
                        val = dataset['NO2_TropVCD'].isel(**{time_dim: t_idx}, ROW=r, COL=c).item()
                        no2_vals.append(val)
                    
                    if 'HCHO_TotVCD' in dataset:
                        val = dataset['HCHO_TotVCD'].isel(**{time_dim: t_idx}, ROW=r, COL=c).item()
                        hcho_vals.append(val)
                
                # Average (ignoring NaNs)
                # Suppress RuntimeWarning for mean of empty slice
                with np.errstate(all='ignore'):
                    no2_mean = np.nanmean(no2_vals) if no2_vals else np.nan
                    hcho_mean = np.nanmean(hcho_vals) if hcho_vals else np.nan
                
                no2_means.append(no2_mean)
                hcho_means.append(hcho_mean)
            
            # Add to DataFrame
            df_raw[f'{site}_NO2'] = no2_means
            df_raw[f'{site}_HCHO'] = hcho_means
            
            # Calculate FNR
            # Avoid division by zero/small numbers
            no2_arr = np.array(no2_means)
            hcho_arr = np.array(hcho_means)
            fnr = np.where(no2_arr > 1e-12, hcho_arr / no2_arr, np.nan)
            df_raw[f'{site}_FNR'] = fnr

        # Create Filled_Data (gap filling)
        df_filled = df_raw.copy()
        value_cols = [c for c in df_filled.columns if '_NO2' in c or '_HCHO' in c or '_FNR' in c]
        
        # Helper to fill by hour
        for col in value_cols:
            means = df_filled.groupby('Hour')[col].transform('mean')
            df_filled[col] = df_filled[col].fillna(means)

        # Save
        fname = self.output_dir / f"FNR_{dataset_name}_spatial_average.xlsx"
        with pd.ExcelWriter(fname, engine='openpyxl') as writer:
            df_raw.to_excel(writer, sheet_name='Raw_Data', index=False)
            df_filled.to_excel(writer, sheet_name='Filled_Data', index=False)
            pd.DataFrame(site_stats).to_excel(writer, sheet_name='Summary', index=False)
            pd.DataFrame(grid_cells_info).to_excel(writer, sheet_name='Grid_Cells', index=False)
            
            if metadata:
                self._create_metadata_df(metadata).to_excel(writer, sheet_name='Metadata', index=False)
                
        return [str(fname)]


========================================
FILE: src/tempo_app/core/geo_utils.py
========================================
"""Geographic utility functions for TEMPO Analyzer.

Provides functions for calculating bounding boxes from center points and radii,
coordinate validation, and distance calculations.
"""

import math
from ..storage.models import BoundingBox


# Approximate km per degree of latitude (constant everywhere on Earth)
KM_PER_DEG_LAT = 111.0


def km_to_degrees_lat(km: float) -> float:
    """Convert kilometers to degrees of latitude.

    Args:
        km: Distance in kilometers

    Returns:
        Equivalent distance in degrees of latitude
    """
    return km / KM_PER_DEG_LAT


def km_to_degrees_lon(km: float, latitude: float) -> float:
    """Convert kilometers to degrees of longitude at a given latitude.

    Longitude degrees shrink toward poles: 1 degree lon ‚âà cos(lat) * 111 km

    Args:
        km: Distance in kilometers
        latitude: Reference latitude in degrees

    Returns:
        Equivalent distance in degrees of longitude
    """
    cos_lat = math.cos(math.radians(latitude))
    if cos_lat < 0.001:  # Near poles, longitude becomes meaningless
        return 180.0
    return km / (KM_PER_DEG_LAT * cos_lat)


def bbox_from_center(lat: float, lon: float, radius_km: float) -> BoundingBox:
    """Calculate bounding box from center point and radius.

    Creates a square bounding box centered on the given coordinates
    with sides equal to 2 * radius_km.

    Args:
        lat: Center latitude in degrees (-90 to 90)
        lon: Center longitude in degrees (-180 to 180)
        radius_km: Radius in kilometers

    Returns:
        BoundingBox with west, south, east, north coordinates

    Example:
        >>> bbox = bbox_from_center(40.0, -111.0, 10.0)
        >>> print(f"W: {bbox.west:.4f}, E: {bbox.east:.4f}")
    """
    delta_lat = km_to_degrees_lat(radius_km)
    delta_lon = km_to_degrees_lon(radius_km, lat)

    return BoundingBox(
        west=lon - delta_lon,
        south=lat - delta_lat,
        east=lon + delta_lon,
        north=lat + delta_lat
    )


def validate_coordinates(lat: float, lon: float) -> tuple[bool, str]:
    """Validate latitude and longitude coordinates.

    Args:
        lat: Latitude in degrees
        lon: Longitude in degrees

    Returns:
        Tuple of (is_valid, error_message)
    """
    if not (-90 <= lat <= 90):
        return False, f"Latitude {lat} must be between -90 and 90"
    if not (-180 <= lon <= 180):
        return False, f"Longitude {lon} must be between -180 and 180"
    return True, ""


def validate_bbox(bbox: BoundingBox) -> tuple[bool, str]:
    """Validate a bounding box.

    Args:
        bbox: BoundingBox to validate

    Returns:
        Tuple of (is_valid, error_message)
    """
    if bbox.west >= bbox.east:
        return False, "West must be less than East"
    if bbox.south >= bbox.north:
        return False, "South must be less than North"
    if not (-180 <= bbox.west <= 180):
        return False, f"West ({bbox.west}) must be between -180 and 180"
    if not (-180 <= bbox.east <= 180):
        return False, f"East ({bbox.east}) must be between -180 and 180"
    if not (-90 <= bbox.south <= 90):
        return False, f"South ({bbox.south}) must be between -90 and 90"
    if not (-90 <= bbox.north <= 90):
        return False, f"North ({bbox.north}) must be between -90 and 90"
    return True, ""


def haversine_distance(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Calculate the great-circle distance between two points in kilometers.

    Uses the Haversine formula for accurate distance on a sphere.

    Args:
        lat1, lon1: First point coordinates in degrees
        lat2, lon2: Second point coordinates in degrees

    Returns:
        Distance in kilometers
    """
    R = 6371.0  # Earth's radius in km

    lat1_rad = math.radians(lat1)
    lat2_rad = math.radians(lat2)
    dlat = math.radians(lat2 - lat1)
    dlon = math.radians(lon2 - lon1)

    a = math.sin(dlat / 2) ** 2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2) ** 2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))

    return R * c


========================================
FILE: src/tempo_app/core/nodes/base.py
========================================
"""Base classes for the Node-Based Pipeline Engine."""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
import pandas as pd
import numpy as np


@dataclass
class NodeConfig:
    """Configuration for a node instance."""
    node_type: str
    params: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> dict:
        return {"type": self.node_type, "params": self.params}
    
    @classmethod
    def from_dict(cls, data: dict) -> "NodeConfig":
        return cls(node_type=data["type"], params=data.get("params", {}))


class Node(ABC):
    """Abstract base class for all pipeline nodes.
    
    A Node is an atomic unit of data transformation.
    It receives a DataFrame (or Series), applies its logic, and returns transformed data.
    """
    
    # Class-level metadata
    node_type: str = "base"
    display_name: str = "Base Node"
    category: str = "base"
    
    def __init__(self, **params):
        """Initialize node with parameters."""
        self.params = params
    
    @abstractmethod
    def execute(self, data: pd.DataFrame, context: dict) -> pd.DataFrame:
        """Execute the node's transformation.
        
        Args:
            data: Input DataFrame from previous node (or source).
            context: Shared context with dataset, sites, etc.
            
        Returns:
            Transformed DataFrame.
        """
        pass
    
    def validate(self) -> List[str]:
        """Validate node configuration. Returns list of error messages."""
        return []
    
    def get_config(self) -> NodeConfig:
        """Get node configuration for serialization."""
        return NodeConfig(node_type=self.node_type, params=self.params)
    
    @classmethod
    def from_config(cls, config: NodeConfig) -> "Node":
        """Create node instance from config."""
        return cls(**config.params)
    
    def __repr__(self):
        return f"{self.__class__.__name__}({self.params})"


# Registry for node types
_NODE_REGISTRY: Dict[str, type] = {}


def register_node(node_class: type) -> type:
    """Decorator to register a node class."""
    _NODE_REGISTRY[node_class.node_type] = node_class
    return node_class


def get_node_class(node_type: str) -> Optional[type]:
    """Get node class by type name."""
    return _NODE_REGISTRY.get(node_type)


def create_node(config: NodeConfig) -> Node:
    """Factory function to create a node from config."""
    node_class = get_node_class(config.node_type)
    if not node_class:
        raise ValueError(f"Unknown node type: {config.node_type}")
    return node_class.from_config(config)


def list_node_types() -> List[str]:
    """List all registered node types."""
    return list(_NODE_REGISTRY.keys())


========================================
FILE: src/tempo_app/core/nodes/pipeline.py
========================================
"""Pipeline and Column Definition classes."""

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
import logging

from .base import Node, NodeConfig, create_node

logger = logging.getLogger(__name__)


@dataclass
class ColumnDefinition:
    """Definition of a single output column with its processing stack."""
    
    name: str
    node_configs: List[NodeConfig] = field(default_factory=list)
    
    def to_dict(self) -> dict:
        return {
            "name": self.name,
            "nodes": [n.to_dict() for n in self.node_configs]
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> "ColumnDefinition":
        return cls(
            name=data["name"],
            node_configs=[NodeConfig.from_dict(n) for n in data.get("nodes", [])]
        )


@dataclass 
class ExportContext:
    """Shared context for pipeline execution."""
    
    dataset: Any  # xr.Dataset
    sites: Dict[str, tuple]  # {code: (lat, lon)}
    date_range: Optional[tuple] = None  # (start, end)
    utc_offset: float = -6.0
    
    # Cached computed values
    _site_pixel_cache: Dict[str, Any] = field(default_factory=dict, repr=False)


class Pipeline:
    """Runs a sequence of nodes on input data."""
    
    def __init__(self, nodes: List[Node] = None):
        self.nodes = nodes or []
    
    def add_node(self, node: Node):
        """Add a node to the pipeline."""
        self.nodes.append(node)
        return self
    
    def run(self, input_data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Execute all nodes in sequence."""
        data = input_data
        
        for i, node in enumerate(self.nodes):
            try:
                data = node.execute(data, context)
                logger.debug(f"Node {i} ({node.__class__.__name__}) output shape: {data.shape if hasattr(data, 'shape') else 'N/A'}")
            except Exception as e:
                logger.error(f"Node {i} ({node.__class__.__name__}) failed: {e}")
                raise
        
        return data
    
    @classmethod
    def from_configs(cls, configs: List[NodeConfig]) -> "Pipeline":
        """Create pipeline from list of node configs."""
        nodes = [create_node(c) for c in configs]
        return cls(nodes=nodes)


class ColumnRunner:
    """Runs a single column's pipeline and produces a Series."""
    
    def __init__(self, column_def: ColumnDefinition):
        self.column_def = column_def
        self.pipeline = Pipeline.from_configs(column_def.node_configs)
    
    def run(self, context: ExportContext) -> pd.Series:
        """Execute the column pipeline."""
        # Start with an empty DataFrame that will be populated by source nodes
        initial_data = pd.DataFrame()
        
        result_df = self.pipeline.run(initial_data, context)
        
        # The pipeline should produce a DataFrame with a single value column
        # We take the first non-index column as the result
        value_cols = [c for c in result_df.columns if c not in ['Site', 'Date', 'Hour', 'Month', 'UTC_Time', 'Local_Time']]
        
        if value_cols:
            return result_df[value_cols[0]].rename(self.column_def.name)
        else:
            logger.warning(f"Column '{self.column_def.name}' produced no value column")
            return pd.Series(name=self.column_def.name)


========================================
FILE: src/tempo_app/core/nodes/source_nodes.py
========================================
"""Source nodes for data loading and variable selection."""

import pandas as pd
import numpy as np
from typing import List, Optional

from .base import Node, register_node
from .pipeline import ExportContext


@register_node
class SelectVariableNode(Node):
    """Selects a variable from the dataset and extracts values for all sites."""
    
    node_type = "select_variable"
    display_name = "Select Variable"
    category = "source"
    
    def __init__(self, variable: str = "NO2_TropVCD", **kwargs):
        super().__init__(variable=variable, **kwargs)
        self.variable = variable
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Extract variable data for all sites from the dataset."""
        ds = context.dataset
        sites = context.sites
        
        if self.variable not in ds:
            raise ValueError(f"Variable '{self.variable}' not found in dataset")
        
        # Get coordinates
        lats = ds['LAT'].values
        lons = ds['LON'].values
        
        # Get time info
        if 'TSTEP' in ds.dims:
            utc_times = pd.to_datetime(ds['TSTEP'].values)
        elif 'hour' in ds.dims:
            hours = ds['hour'].values
            utc_times = pd.to_datetime([f"2000-01-01 {h:02d}:00:00" for h in hours])
        else:
            raise ValueError("No time dimension found in dataset")
        
        local_times = utc_times + pd.Timedelta(hours=context.utc_offset)
        
        # Build rows for all sites
        rows = []
        for site_code, (t_lat, t_lon) in sites.items():
            # Find nearest pixel
            dist = (lats - t_lat)**2 + (lons - t_lon)**2
            flat_idx = np.argmin(dist)
            row_idx, col_idx = np.unravel_index(flat_idx, dist.shape)
            
            # Extract values at this pixel
            values = ds[self.variable].isel(ROW=row_idx, COL=col_idx).values
            
            for i, (utc, local, val) in enumerate(zip(utc_times, local_times, values)):
                rows.append({
                    'Site': site_code,
                    'UTC_Time': utc,
                    'Local_Time': local,
                    'Date': local.date(),
                    'Month': local.month,
                    'Hour': local.hour,
                    'Value': val,
                    '_lat': t_lat,
                    '_lon': t_lon,
                    '_row': row_idx,
                    '_col': col_idx,
                })
        
        return pd.DataFrame(rows)
    
    def validate(self) -> List[str]:
        errors = []
        if not self.variable:
            errors.append("Variable name is required")
        return errors


========================================
FILE: src/tempo_app/core/nodes/spatial_nodes.py
========================================
"""Spatial aggregation nodes."""

import pandas as pd
import numpy as np
from typing import List

from .base import Node, register_node
from .pipeline import ExportContext


@register_node
class NearestPixelNode(Node):
    """Uses the single nearest pixel value (no aggregation)."""
    
    node_type = "nearest_pixel"
    display_name = "Nearest Pixel"
    category = "spatial"
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Pass through - SelectVariableNode already extracts nearest pixel."""
        # The source node already extracts nearest pixel, so this is a no-op
        return data


@register_node
class NPixelAvgNode(Node):
    """Averages the N nearest pixels."""
    
    node_type = "n_pixel_avg"
    display_name = "N-Pixel Average"
    category = "spatial"
    
    def __init__(self, n_pixels: int = 4, **kwargs):
        super().__init__(n_pixels=n_pixels, **kwargs)
        self.n_pixels = n_pixels
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Re-extract values using N nearest pixels and average."""
        ds = context.dataset
        sites = context.sites
        
        # Get variable name from data (we need to re-extract)
        # This node expects data from SelectVariableNode with '_row', '_col' columns
        if data.empty:
            return data
        
        lats = ds['LAT'].values
        lons = ds['LON'].values
        
        # Get the variable from the first row's metadata or infer
        # Actually, we need to know which variable to extract
        # For now, assume 'Value' column exists from source node
        
        result_rows = []
        
        for site_code, (t_lat, t_lon) in sites.items():
            # Find N nearest pixels
            dist = (lats - t_lat)**2 + (lons - t_lon)**2
            flat_indices = np.argsort(dist, axis=None)[:self.n_pixels]
            pixel_coords = [np.unravel_index(idx, dist.shape) for idx in flat_indices]
            
            # Get site rows from input data
            site_data = data[data['Site'] == site_code].copy()
            
            if site_data.empty:
                continue
            
            # For each timestep, average across N pixels
            # We need to re-extract from dataset - get the variable name
            # The Value column has single-pixel values, we need to recalculate
            
            # Get variable from context or infer from first source node
            var_names = [v for v in ds.data_vars if 'VCD' in v or 'FNR' in v]
            if not var_names:
                # Just pass through
                result_rows.append(site_data)
                continue
            
            var_name = var_names[0]  # Use first available
            
            # Extract all N pixel values
            pixel_values = []
            for r, c in pixel_coords:
                vals = ds[var_name].isel(ROW=r, COL=c).values
                pixel_values.append(vals)
            
            pixel_array = np.array(pixel_values)  # Shape: (N_pixels, Time)
            
            # Average across pixels (axis 0)
            with np.errstate(invalid='ignore'):
                avg_values = np.nanmean(pixel_array, axis=0)
            
            site_data['Value'] = avg_values
            site_data['_n_pixels'] = self.n_pixels
            result_rows.append(site_data)
        
        if result_rows:
            return pd.concat(result_rows, ignore_index=True)
        return data


@register_node
class RadiusAvgNode(Node):
    """Averages all pixels within a radius."""
    
    node_type = "radius_avg"
    display_name = "Radius Average"
    category = "spatial"
    
    def __init__(self, radius_km: float = 10.0, min_coverage: float = 0.5, **kwargs):
        super().__init__(radius_km=radius_km, min_coverage=min_coverage, **kwargs)
        self.radius_km = radius_km
        self.min_coverage = min_coverage
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Average all pixels within radius."""
        ds = context.dataset
        sites = context.sites
        
        if data.empty:
            return data
        
        lats = ds['LAT'].values
        lons = ds['LON'].values
        
        result_rows = []
        
        for site_code, (t_lat, t_lon) in sites.items():
            # Calculate distances in km
            cos_lat = np.cos(np.deg2rad(t_lat))
            dy_km = (lats - t_lat) * 111.0
            dx_km = (lons - t_lon) * 111.0 * cos_lat
            dist_km = np.sqrt(dx_km**2 + dy_km**2)
            
            # Find pixels within radius
            mask = dist_km <= self.radius_km
            rows_in_radius, cols_in_radius = np.where(mask)
            
            if len(rows_in_radius) == 0:
                continue
            
            site_data = data[data['Site'] == site_code].copy()
            if site_data.empty:
                continue
            
            # Get variable
            var_names = [v for v in ds.data_vars if 'VCD' in v or 'FNR' in v]
            if not var_names:
                result_rows.append(site_data)
                continue
            
            var_name = var_names[0]
            
            # Extract all pixel values within radius
            pixel_values = []
            for r, c in zip(rows_in_radius, cols_in_radius):
                vals = ds[var_name].isel(ROW=r, COL=c).values
                pixel_values.append(vals)
            
            pixel_array = np.array(pixel_values)  # Shape: (N_pixels, Time)
            
            # Calculate averages and coverage
            with np.errstate(invalid='ignore'):
                avg_values = np.nanmean(pixel_array, axis=0)
                valid_counts = np.sum(np.isfinite(pixel_array), axis=0)
                coverage = valid_counts / len(rows_in_radius)
            
            # Apply min coverage threshold
            avg_values = np.where(coverage >= self.min_coverage, avg_values, np.nan)
            
            site_data['Value'] = avg_values
            site_data['_radius_km'] = self.radius_km
            site_data['_pixel_count'] = len(rows_in_radius)
            result_rows.append(site_data)
        
        if result_rows:
            return pd.concat(result_rows, ignore_index=True)
        return data


========================================
FILE: src/tempo_app/core/nodes/temporal_nodes.py
========================================
"""Temporal aggregation nodes."""

import pandas as pd
import numpy as np
from typing import List

from .base import Node, register_node
from .pipeline import ExportContext


@register_node
class HourlyNode(Node):
    """Pass-through node - keeps data at hourly resolution."""
    
    node_type = "hourly"
    display_name = "Hourly (Raw)"
    category = "temporal"
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """No aggregation, return as-is."""
        return data


@register_node  
class DailyMeanNode(Node):
    """Aggregates to daily mean values."""
    
    node_type = "daily_mean"
    display_name = "Daily Mean"
    category = "temporal"
    
    def __init__(self, min_hours: int = 1, **kwargs):
        super().__init__(min_hours=min_hours, **kwargs)
        self.min_hours = min_hours
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Aggregate to daily mean per site."""
        if data.empty or 'Value' not in data.columns:
            return data
        
        # Group by Site and Date
        grouped = data.groupby(['Site', 'Date']).agg({
            'Value': 'mean',
            'UTC_Time': 'first',
            'Local_Time': 'first',
            'Month': 'first',
        })
        
        # Count hours per day
        hour_counts = data.groupby(['Site', 'Date']).size()
        grouped['_hour_count'] = hour_counts
        
        # Apply min hours filter
        if self.min_hours > 0:
            valid_mask = grouped['_hour_count'] >= self.min_hours
            grouped = grouped[valid_mask]
        
        return grouped.reset_index()


@register_node
class DiurnalCycleNode(Node):
    """Aggregates to monthly-hourly averages (diurnal cycle)."""
    
    node_type = "diurnal_cycle"
    display_name = "Diurnal Cycle"
    category = "temporal"
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Aggregate to monthly-hourly average per site."""
        if data.empty or 'Value' not in data.columns:
            return data
        
        # Group by Site, Month, Hour
        grouped = data.groupby(['Site', 'Month', 'Hour']).agg({
            'Value': 'mean',
        })
        
        # Count samples
        counts = data.groupby(['Site', 'Month', 'Hour']).size()
        grouped['_sample_count'] = counts
        
        return grouped.reset_index()


========================================
FILE: src/tempo_app/core/nodes/transform_nodes.py
========================================
"""Transform nodes for filtering and post-processing."""

import pandas as pd
import numpy as np
from typing import List, Optional

from .base import Node, register_node
from .pipeline import ExportContext


@register_node
class GapFillNode(Node):
    """Fills gaps using monthly-hourly mean values."""
    
    node_type = "gap_fill"
    display_name = "Gap Fill (Monthly Mean)"
    category = "transform"
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Fill NaN values using monthly-hourly mean."""
        if data.empty or 'Value' not in data.columns:
            return data
        
        # Need Month and Hour columns
        if 'Month' not in data.columns or 'Hour' not in data.columns:
            return data
        
        result = data.copy()
        
        # Calculate monthly-hourly means per site
        means = result.groupby(['Site', 'Month', 'Hour'])['Value'].transform('mean')
        
        # Fill NaNs
        result['Value'] = result['Value'].fillna(means)
        result['_gap_filled'] = data['Value'].isna()
        
        return result


@register_node
class FilterNode(Node):
    """Filters rows based on a condition."""
    
    node_type = "filter"
    display_name = "Filter"
    category = "transform"
    
    def __init__(self, column: str = "Value", operator: str = ">", threshold: float = 0, **kwargs):
        super().__init__(column=column, operator=operator, threshold=threshold, **kwargs)
        self.column = column
        self.operator = operator
        self.threshold = threshold
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Filter rows based on condition."""
        if data.empty or self.column not in data.columns:
            return data
        
        col = data[self.column]
        
        if self.operator == ">":
            mask = col > self.threshold
        elif self.operator == ">=":
            mask = col >= self.threshold
        elif self.operator == "<":
            mask = col < self.threshold
        elif self.operator == "<=":
            mask = col <= self.threshold
        elif self.operator == "==":
            mask = col == self.threshold
        elif self.operator == "!=":
            mask = col != self.threshold
        else:
            return data
        
        return data[mask].copy()


@register_node
class RenameValueNode(Node):
    """Renames the Value column to a custom name."""
    
    node_type = "rename_value"
    display_name = "Rename Output"
    category = "transform"
    
    def __init__(self, new_name: str = "Value", **kwargs):
        super().__init__(new_name=new_name, **kwargs)
        self.new_name = new_name
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Rename the Value column."""
        if 'Value' in data.columns:
            return data.rename(columns={'Value': self.new_name})
        return data


@register_node
class StatisticsNode(Node):
    """Computes additional statistics (std, min, max, count)."""
    
    node_type = "statistics"
    display_name = "Add Statistics"
    category = "transform"
    
    def __init__(self, include_std: bool = True, include_min: bool = False, 
                 include_max: bool = False, include_count: bool = True, **kwargs):
        super().__init__(include_std=include_std, include_min=include_min,
                        include_max=include_max, include_count=include_count, **kwargs)
        self.include_std = include_std
        self.include_min = include_min
        self.include_max = include_max
        self.include_count = include_count
    
    def execute(self, data: pd.DataFrame, context: ExportContext) -> pd.DataFrame:
        """Add statistics columns based on grouping."""
        # This node is a placeholder - statistics are typically computed
        # during spatial aggregation. Here we just add metadata flags.
        result = data.copy()
        result['_stats_std'] = self.include_std
        result['_stats_min'] = self.include_min
        result['_stats_max'] = self.include_max
        result['_stats_count'] = self.include_count
        return result


========================================
FILE: src/tempo_app/core/nodes/__init__.py
========================================
"""Node-Based Pipeline Engine for TEMPO Analyzer.

This package provides a composable, column-centric data processing system.
"""

from .base import (
    Node,
    NodeConfig,
    register_node,
    get_node_class,
    create_node,
    list_node_types,
)

from .pipeline import (
    ColumnDefinition,
    ExportContext,
    Pipeline,
    ColumnRunner,
)

# Import all node types to register them
from . import source_nodes
from . import spatial_nodes
from . import temporal_nodes
from . import transform_nodes

__all__ = [
    'Node',
    'NodeConfig',
    'register_node',
    'get_node_class',
    'create_node',
    'list_node_types',
    'ColumnDefinition',
    'ExportContext',
    'Pipeline',
    'ColumnRunner',
]


========================================
FILE: src/tempo_app/core/plotter.py
========================================
"""Map plotter module for TEMPO Analyzer."""

import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib.colors import LinearSegmentedColormap, Normalize
from matplotlib.lines import Line2D

import numpy as np
import xarray as xr
from pathlib import Path
import logging
import sys
from typing import Optional

try:
    import cartopy.crs as ccrs
    import cartopy.feature as cfeature
    import cartopy.io.shapereader as shapereader
    from pyproj import Proj
except ImportError:
    pass

from .constants import DEFAULT_BBOX, SITES

logger = logging.getLogger(__name__)

class MapPlotter:
    def __init__(self, cache_dir: Path):
        self.cache_dir = cache_dir / "plots"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.geometry_cache = {}
        self._temp_file_counter = 0
        
    def generate_map(self, 
                    dataset: xr.Dataset, 
                    hour: int, 
                    variable: str, 
                    dataset_name: str,
                    bbox: list[float] = None,
                    road_detail: str = 'primary',
                    sites: dict[str, tuple[float, float]] = None,
                    font_size: int = 10,
                    title_size: int = 14,
                    colormap: str = None,
                    border_width: float = 1.5,
                    road_scale: float = 1.0,
                    vmin: float = None,
                    vmax: float = None) -> str:
        """
        Generate a map plot for a specific hour and variable.
        
        Args:
            dataset: xarray Dataset with 'NO2_TropVCD', 'HCHO_TotVCD', or 'FNR'
            hour: UTC hour
            variable: 'NO2', 'HCHO', or 'FNR'
            dataset_name: Name of dataset (for display)
            bbox: [west, south, east, north]
            road_detail: 'primary', 'major', or 'all'
            sites: Dict mapping site code to (lat, lon). If None, uses default SITES.
                   If empty dict {}, no markers shown.
            font_size: Base font size for labels (default 10)
            title_size: Title font size (default 14)
            colormap: Matplotlib colormap name (optional, overrides default)
            border_width: Width of political borders (default 1.5)
            road_scale: Scale factor for road widths (default 1.0)
            vmin: Minimum value for color scale
            vmax: Maximum value for color scale
            
        Returns:
            Path to the temporary PNG file (always regenerated fresh)
        """
        import tempfile
        import time

        try:
            logger.info(f"Generating map for {variable} @ H{hour}")
            if 'cartopy.crs' not in sys.modules:
                logger.warning("Cartopy not loaded, using fallback.")
                return self._generate_dummy_map(variable, hour)

            # Extract date range and available hours info for title
            import pandas as pd
            date_range_str = ""
            
            # Extract data for the specific hour
            # Handle processed data with TIME (new), TSTEP (old datetime), or HOUR (aggregated) dims
            if 'TIME' in dataset.dims:
                timestamps = pd.to_datetime(dataset.TIME.values)
                date_range_str = f"{timestamps.min().strftime('%Y-%m-%d')} to {timestamps.max().strftime('%Y-%m-%d')}"
                available_hours = sorted(set(timestamps.hour.tolist()))
                
                # New format: TIME is datetime, need to extract by hour and average
                if hour not in dataset.TIME.dt.hour.values:
                    print(f"DEBUG: Hour {hour} not found in dataset TIME: {dataset.TIME.dt.hour.values}")
                    return None
                ds_hour = dataset.sel(TIME=dataset.TIME.dt.hour == hour).mean(dim='TIME')
            elif 'TSTEP' in dataset.dims:
                timestamps = pd.to_datetime(dataset.TSTEP.values)
                date_range_str = f"{timestamps.min().strftime('%Y-%m-%d')} to {timestamps.max().strftime('%Y-%m-%d')}"
                available_hours = sorted(set(timestamps.hour.tolist()))
                
                # Old format: TSTEP is datetime, need to extract by hour
                if hour not in dataset.TSTEP.dt.hour.values:
                    print(f"DEBUG: Hour {hour} not found in dataset TSTEP: {dataset.TSTEP.dt.hour.values}")
                    return None
                ds_hour = dataset.sel(TSTEP=dataset.TSTEP.dt.hour == hour).mean(dim='TSTEP')
            elif 'HOUR' in dataset.dims:
                available_hours = sorted(dataset.HOUR.values.tolist())
                
                # Aggregated format: dimension is integer hours
                if hour not in dataset.HOUR.values:
                    print(f"DEBUG: Hour {hour} not found in dataset HOURs: {dataset.HOUR.values}")
                    return None
                ds_hour = dataset.sel(HOUR=hour)
            elif 'hour' in dataset.dims:
                available_hours = sorted(dataset.hour.values.tolist())
                
                # Legacy format: dimension is integer hours
                if hour not in dataset.hour.values:
                    print(f"DEBUG: Hour {hour} not found in dataset hours: {dataset.hour.values}")
                    return None
                ds_hour = dataset.sel(hour=hour)
            else:
                logger.error(f"Dataset has no recognized time dimension. Dims: {list(dataset.dims)}")
                return None
            logger.debug(f"Extracted hour {hour} slice.")
            
            if variable == 'FNR':
                data = ds_hour['FNR']
                if colormap:
                    cmap = colormap
                else:
                    # Blue-Grey-Red colormap
                    colors = [(0.3, 0.5, 1), 'silver', (1, 0.4, 0.4)]
                    cmap = LinearSegmentedColormap.from_list('bgr', colors, N=256)
                
                # Default FNR range is 2-8, or custom
                norm_vmin = vmin if vmin is not None else 2
                norm_vmax = vmax if vmax is not None else 8
                norm = Normalize(vmin=norm_vmin, vmax=norm_vmax)
                
                label = 'FNR (HCHO/NO2)'
                data = data.where(data > 0)
            elif variable == 'NO2':
                data = ds_hour['NO2_TropVCD']
                cmap = colormap if colormap else 'viridis'
                norm = Normalize(vmin=vmin, vmax=vmax) if (vmin is not None or vmax is not None) else None
                label = 'NO2 Trop VCD'
            elif variable == 'HCHO':
                data = ds_hour['HCHO_TotVCD']
                cmap = colormap if colormap else 'magma'
                norm = Normalize(vmin=vmin, vmax=vmax) if (vmin is not None or vmax is not None) else None
                label = 'HCHO Total VCD'
            else:
                return None
                
            data = data.squeeze(drop=True)
            if data.isnull().all():
                logger.warning(f"Data for {variable} is all NaN (empty) for hour {hour}.")
                return None

            # Setup plot
            fig = plt.figure(figsize=(9, 8))
            ax = fig.add_subplot(1, 1, 1, projection=ccrs.Mercator())
            
            # Coordinates
            if 'LAT' in ds_hour.coords:
                lats = ds_hour['LAT'].values
                lons = ds_hour['LON'].values
            else:
                # Fallback if no lat/lon arrays (projected)
                # Simplified for now
                return None

            if bbox is None:
                bbox = DEFAULT_BBOX
                
            ax.set_extent([bbox[0], bbox[2], bbox[1], bbox[3]], crs=ccrs.PlateCarree())
            ax.add_feature(cfeature.LAND, facecolor='white')
            ax.add_feature(cfeature.LAKES, facecolor='none', edgecolor='black')
            # Borders: Solid lines, thicker than roads (roads are max 1.0)
            ax.add_feature(cfeature.BORDERS, linestyle='-', linewidth=border_width, alpha=0.8, edgecolor='black')
            ax.add_feature(cfeature.STATES, linestyle='-', linewidth=border_width * 0.8, alpha=0.6, edgecolor='gray')
            
            # Add road overlays based on road_detail level
            try:
                self._add_road_overlay(ax, bbox, road_detail, road_scale)
            except Exception as road_err:
                logger.warning(f"Road overlay failed (non-fatal): {road_err}")
            
            # Plot Data
            mesh = ax.pcolormesh(lons, lats, data, cmap=cmap, norm=norm, 
                               transform=ccrs.PlateCarree(), shading='auto')
                               
            # Plot Sites - use provided sites or fall back to default SITES
            sites_to_plot = SITES if sites is None else sites
            site_idx = 0
            # distinct colors
            marker_colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6']
            
            legend_elements = []
            for name, (lat, lon) in sites_to_plot.items():
                if bbox[0] <= lon <= bbox[2] and bbox[1] <= lat <= bbox[3]:
                    c = marker_colors[site_idx % len(marker_colors)]
                    ax.plot(lon, lat, marker='*', markersize=14, markerfacecolor=c, 
                          markeredgecolor='black', transform=ccrs.PlateCarree())
                    legend_elements.append(Line2D([0], [0], marker='*', color='w', label=name,
                                                markerfacecolor=c, markeredgecolor='black', markersize=10))
                    site_idx += 1
            
            if legend_elements:
                ax.legend(handles=legend_elements, loc='upper right', title="Sites", 
                        fancybox=True, shadow=True, fontsize=font_size)
            
            # Decoration
            cbar = plt.colorbar(mesh, ax=ax, shrink=0.7, label=label)
            cbar.ax.tick_params(labelsize=font_size)
            cbar.set_label(label, size=font_size)
            ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.3, linestyle='--')
            # Build title with extra info
            title_parts = [f"{variable} - {dataset_name}"]
            if date_range_str:
                title_parts.append(date_range_str)
            
            # Add current hour line
            title_parts.append(f"{hour:02d}:00 UTC")
            
            ax.set_title("\n".join(title_parts), fontsize=title_size)
            
            # Save to temp file with descriptive name + unique timestamp
            import time
            safe_dataset_name = "".join(c if c.isalnum() or c in "._-" else "_" for c in dataset_name)
            timestamp = int(time.time() * 1000)
            temp_filename = f"{safe_dataset_name}_{variable}_H{hour:02d}_{road_detail}_{timestamp}.png"
            temp_path = self.cache_dir / temp_filename
            plt.savefig(temp_path, dpi=100, bbox_inches='tight')
            plt.close(fig)
            
            logger.info(f"Map saved to {temp_path}")
            return str(temp_path)
            
        except Exception as e:
            logger.error(f"Plotting failed: {e}")
            print(f"DEBUG: Plotting Exception: {e}")
            import traceback
            traceback.print_exc()
            plt.close('all')
            return None

    def _generate_dummy_map(self, variable, hour):
        """Generate a placeholder image if Cartopy is missing."""
        import time
        fig, ax = plt.subplots(figsize=(5, 4))
        ax.text(0.5, 0.5, f"{variable} Map\nHour {hour}\n(Cartopy Missing)", 
              ha='center', va='center')
        ax.axis('off')
        
        self._temp_file_counter += 1
        temp_filename = f"dummy_{int(time.time() * 1000)}_{self._temp_file_counter}.png"
        temp_path = self.cache_dir / temp_filename
        plt.savefig(temp_path)
        plt.close(fig)
        
        return str(temp_path)

    def _add_road_overlay(self, ax, bbox: list[float], road_detail: str = 'primary', road_scale: float = 1.0):
        """
        Add road overlay to the map axis.
        
        Args:
            ax: Matplotlib/Cartopy axis
            bbox: [west, south, east, north]
            road_detail: 'primary' (interstates), 'major' (+highways), 'all' (+secondary)
            road_scale: Multiplier for road linewidths
        """
        # Use Natural Earth roads - automatically downloaded and cached by Cartopy
        # 10m resolution has the most detail for road features
        
        if road_detail == 'none':
            return
            
        # Interstate highways (always shown unless 'none')
        try:
            roads_10m = shapereader.natural_earth(
                resolution='10m',
                category='cultural',
                name='roads'
            )
            
            reader = shapereader.Reader(roads_10m)
            
            for record in reader.records():
                road_type = record.attributes.get('type', '')
                
                # Filter by road type based on detail level
                if road_detail == 'primary':
                    # Only major highways/interstates
                    if road_type not in ['Major Highway', 'Beltway', 'Interstate']:
                        continue
                    linewidth = 1.0 * road_scale
                    color = '#444444'
                    alpha = 0.7
                elif road_detail == 'major':
                    # Major highways + secondary
                    if road_type not in ['Major Highway', 'Beltway', 'Interstate', 'Secondary Highway']:
                        continue
                    linewidth = (0.8 if road_type in ['Major Highway', 'Beltway', 'Interstate'] else 0.5) * road_scale
                    color = '#444444' if road_type in ['Major Highway', 'Beltway', 'Interstate'] else '#666666'
                    alpha = 0.7 if road_type in ['Major Highway', 'Beltway', 'Interstate'] else 0.5
                else:  # 'all'
                    # Include all roads
                    if road_type in ['Major Highway', 'Beltway', 'Interstate']:
                        linewidth = 0.8 * road_scale
                        color = '#333333'
                        alpha = 0.8
                    elif road_type == 'Secondary Highway':
                        linewidth = 0.5 * road_scale
                        color = '#555555'
                        alpha = 0.6
                    else:
                        linewidth = 0.3 * road_scale
                        color = '#777777'
                        alpha = 0.4
                
                # Check if geometry intersects bbox (approximate)
                geom = record.geometry
                bounds = geom.bounds  # (minx, miny, maxx, maxy)
                if (bounds[2] < bbox[0] or bounds[0] > bbox[2] or 
                    bounds[3] < bbox[1] or bounds[1] > bbox[3]):
                    continue
                
                ax.add_geometries(
                    [geom],
                    ccrs.PlateCarree(),
                    facecolor='none',
                    edgecolor=color,
                    linewidth=linewidth,
                    alpha=alpha
                )
                
            print(f"DEBUG: Added road overlay (detail={road_detail})")
            
        except Exception as e:
            print(f"DEBUG: Natural Earth roads failed: {e}")
            # Fallback: try US states borders as visual reference
            try:
                ax.add_feature(cfeature.STATES, linewidth=0.5, edgecolor='gray')
            except Exception:
                pass



========================================
FILE: src/tempo_app/core/processor.py
========================================
"""Data processor module for TEMPO Analyzer."""

import xarray as xr
import numpy as np
from pathlib import Path
import logging
import pandas as pd

logger = logging.getLogger(__name__)

class DataProcessor:
    """Handles processing of TEMPO data (averaging, FNR calculation)."""
    
    @staticmethod
    def process_dataset(file_paths: list[Path]) -> xr.Dataset:
        """
        Load multiple NetCDF files and combine them preserving date-hour.
        
        Args:
            file_paths: List of paths to .nc files
            
        Returns:
            Processed xarray Dataset with each date-hour as separate timestep
        """
        if not file_paths:
            return None
            
        datasets = []
        try:
            # Load all datasets, extracting date and hour from filename
            for p in file_paths:
                try:
                    ds = xr.open_dataset(p)
                    
                    # Drop existing TSTEP variable if it exists (conflicts with dimension)
                    if 'TSTEP' in ds.variables:
                        ds = ds.drop_vars('TSTEP')
                    
                    # Extract date and hour from filename pattern: tempo_YYYY-MM-DD_HH.nc
                    fname = p.stem  # e.g., tempo_2024-12-01_17
                    parts = fname.split('_')
                    if len(parts) >= 3:
                        date_str = parts[1]  # 2024-12-01
                        hour = int(parts[-1])  # 17
                        
                        # Create full datetime and use TIME as dimension name
                        timestamp = pd.Timestamp(f"{date_str} {hour:02d}:00:00")
                        ds = ds.assign_coords(TIME=timestamp)
                        ds = ds.expand_dims('TIME')
                        
                        logger.info(f"Loaded {p.name} -> {timestamp}")
                    
                    datasets.append(ds)
                except Exception as e:
                    logger.warning(f"Failed to open {p}: {e}")
            
            if not datasets:
                return None
                
            # Combine along TIME dimension (each file is one timestep)
            combined = xr.concat(datasets, dim='TIME')
            
            # Sort by timestamp
            combined = combined.sortby('TIME')
            
            # Calculate FNR (HCHO / NO2)
            # Filter low NO2 to avoid division by zero or noise
            combined['FNR'] = xr.where(
                (combined['NO2_TropVCD'] > 1e-12) & (combined['HCHO_TotVCD'] > -9e30),
                combined['HCHO_TotVCD'] / combined['NO2_TropVCD'],
                np.nan
            )
            
            # Load data into memory so we can close source files
            combined.load()
            
            logger.info(f"Combined dataset: {len(combined.TIME)} timesteps from {combined.TIME.values[0]} to {combined.TIME.values[-1]}")
            
            return combined
            
        except Exception as e:
            logger.error(f"Processing failed: {e}")
            raise
        finally:
            # Close all opened datasets
            for ds in datasets:
                try:
                    ds.close()
                except Exception:
                    pass

    @staticmethod
    def save_processed(dataset: xr.Dataset, output_path: Path):
        """Save processed dataset to NetCDF."""
        dataset.to_netcdf(output_path)



========================================
FILE: src/tempo_app/core/status.py
========================================
"""Status manager for verbose progress updates.

Provides real-time feedback to the UI so the app never looks stuck.
"""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Callable, Optional
import time


class StatusCategory(Enum):
    """Categories for status messages."""
    DOWNLOAD = "download"
    PROCESS = "process"
    PLOT = "plot"
    EXPORT = "export"
    ASSETS = "assets"
    SYSTEM = "system"


class StatusLevel(Enum):
    """Severity level of status messages."""
    INFO = "info"          # Normal operation
    SUCCESS = "success"    # Completed successfully
    WARNING = "warning"    # Non-fatal issue
    ERROR = "error"        # Failed operation
    PROGRESS = "progress"  # Progress update with bar


@dataclass
class StatusEvent:
    """A single status update event."""
    category: StatusCategory
    message: str
    level: StatusLevel = StatusLevel.INFO
    progress: Optional[float] = None  # 0.0-1.0 for progress bar
    timestamp: datetime = field(default_factory=datetime.now)
    details: Optional[str] = None  # Additional details
    
    @property
    def icon(self) -> str:
        """Get emoji icon for this status level."""
        icons = {
            StatusLevel.INFO: "üîÑ",
            StatusLevel.SUCCESS: "‚úì",
            StatusLevel.WARNING: "‚ö†Ô∏è",
            StatusLevel.ERROR: "‚ùå",
            StatusLevel.PROGRESS: "üì•",
        }
        return icons.get(self.level, "‚Ä¢")
    
    @property
    def time_str(self) -> str:
        """Formatted timestamp."""
        return self.timestamp.strftime("%H:%M:%S")
    
    def format_message(self) -> str:
        """Format message with icon and timestamp."""
        return f"‚è±Ô∏è {self.time_str}  {self.icon} {self.message}"


class StatusManager:
    """Central manager for status updates throughout the application.
    
    Usage:
        status = StatusManager()
        status.on_update = lambda event: print(event.format_message())
        
        status.info("download", "Starting download...")
        status.progress("download", "Downloading file...", 0.5)
        status.success("download", "Download complete!")
    """
    
    def __init__(self):
        self._listeners: list[Callable[[StatusEvent], None]] = []
        self._history: list[StatusEvent] = []
        self._max_history = 100
        
        # Current operation tracking
        self._current_operation: Optional[str] = None
        self._operation_start: Optional[float] = None
        self._total_steps: int = 0
        self._completed_steps: int = 0
    
    def add_listener(self, callback: Callable[[StatusEvent], None]) -> None:
        """Add a listener for status updates."""
        self._listeners.append(callback)
    
    def remove_listener(self, callback: Callable[[StatusEvent], None]) -> None:
        """Remove a status listener."""
        if callback in self._listeners:
            self._listeners.remove(callback)
    
    def _emit(self, event: StatusEvent) -> None:
        """Emit an event to all listeners."""
        self._history.append(event)
        if len(self._history) > self._max_history:
            self._history = self._history[-self._max_history:]
        
        for listener in self._listeners:
            try:
                listener(event)
            except Exception:
                pass  # Don't let listener errors break the app
    
    # ==========================================================================
    # Convenience methods for different status levels
    # ==========================================================================
    
    def info(self, category: str, message: str, details: str = None) -> None:
        """Emit an info status."""
        self._emit(StatusEvent(
            category=StatusCategory(category),
            message=message,
            level=StatusLevel.INFO,
            details=details,
        ))
    
    def success(self, category: str, message: str, details: str = None) -> None:
        """Emit a success status."""
        self._emit(StatusEvent(
            category=StatusCategory(category),
            message=message,
            level=StatusLevel.SUCCESS,
            details=details,
        ))
    
    def warning(self, category: str, message: str, details: str = None) -> None:
        """Emit a warning status."""
        self._emit(StatusEvent(
            category=StatusCategory(category),
            message=message,
            level=StatusLevel.WARNING,
            details=details,
        ))
    
    def error(self, category: str, message: str, details: str = None) -> None:
        """Emit an error status."""
        self._emit(StatusEvent(
            category=StatusCategory(category),
            message=message,
            level=StatusLevel.ERROR,
            details=details,
        ))
    
    def progress(self, category: str, message: str, progress: float, details: str = None) -> None:
        """Emit a progress update (0.0-1.0)."""
        self._emit(StatusEvent(
            category=StatusCategory(category),
            message=message,
            level=StatusLevel.PROGRESS,
            progress=min(1.0, max(0.0, progress)),
            details=details,
        ))
    
    # ==========================================================================
    # Operation tracking
    # ==========================================================================
    
    def start_operation(self, name: str, total_steps: int) -> None:
        """Start tracking a multi-step operation."""
        self._current_operation = name
        self._operation_start = time.time()
        self._total_steps = total_steps
        self._completed_steps = 0
        self.info("system", f"Starting: {name}")
    
    def step_completed(self, message: str = None) -> None:
        """Mark a step as completed."""
        self._completed_steps += 1
        if message:
            progress = self._completed_steps / max(1, self._total_steps)
            self.progress("system", message, progress)
    
    def end_operation(self, success: bool = True) -> None:
        """End the current operation."""
        if self._current_operation and self._operation_start:
            elapsed = time.time() - self._operation_start
            if success:
                self.success("system", f"Completed: {self._current_operation} ({elapsed:.1f}s)")
            else:
                self.error("system", f"Failed: {self._current_operation} ({elapsed:.1f}s)")
        
        self._current_operation = None
        self._operation_start = None
        self._total_steps = 0
        self._completed_steps = 0
    
    @property
    def current_progress(self) -> float:
        """Get current operation progress (0.0-1.0)."""
        if self._total_steps == 0:
            return 0.0
        return self._completed_steps / self._total_steps
    
    @property
    def estimated_remaining(self) -> Optional[float]:
        """Estimate remaining time in seconds."""
        if not self._operation_start or self._completed_steps == 0:
            return None
        
        elapsed = time.time() - self._operation_start
        rate = self._completed_steps / elapsed
        remaining_steps = self._total_steps - self._completed_steps
        
        if rate > 0:
            return remaining_steps / rate
        return None
    
    def get_history(self, limit: int = 50) -> list[StatusEvent]:
        """Get recent status history."""
        return self._history[-limit:]
    
    def clear_history(self) -> None:
        """Clear status history."""
        self._history.clear()


# Global status manager instance
_global_status: Optional[StatusManager] = None


def get_status_manager() -> StatusManager:
    """Get the global status manager instance."""
    global _global_status
    if _global_status is None:
        _global_status = StatusManager()
    return _global_status


========================================
FILE: src/tempo_app/core/__init__.py
========================================
"""Core module - Business logic for data processing."""


========================================
FILE: src/tempo_app/main.py
========================================
"""TEMPO Analyzer - Main Application Entry Point.

A modern desktop application for analyzing NASA TEMPO satellite data.
"""

import flet as ft
from pathlib import Path
import sys

from .ui.theme import create_dark_theme, create_light_theme, Colors, LightColors, Sizing
from .ui.shell import AppShell
from .ui.pages.create import CreatePage
from .ui.pages.library import LibraryPage
from .ui.pages.inspect import InspectPage
from .ui.pages.sites import SitesPage
from .ui.pages.plot import PlotPage
from .ui.pages.export import ExportPage
from .ui.pages.workspace import WorkspacePage
from .ui.pages.settings import SettingsPage
from .ui.pages.batch_import import BatchImportPage
from .storage.database import Database
from .core.status import get_status_manager
from .core.config import ConfigManager
from .core.batch_scheduler import recover_interrupted_jobs


class App:
    """Main application class."""
    
    def __init__(self, page: ft.Page):
        self.page = page
        self.config = ConfigManager()
        self.data_dir = self._init_data_dir()
        self.db = Database(self.data_dir / "tempo.db")
        self.status = get_status_manager()

        # Recover interrupted batch jobs on startup
        recovered = recover_interrupted_jobs(self.db)
        if recovered > 0:
            print(f"Recovered {recovered} interrupted batch job(s)")

        # Page content cache
        self._pages: dict[str, ft.Control] = {}
        
        self._setup_page()
        self._build_ui()
    
    def _init_data_dir(self) -> Path:
        """Initialize and return data directory."""
        # Check config first
        if self.config.data_dir:
            return Path(self.config.data_dir)
            
        # Default logic
        if getattr(sys, 'frozen', False):
            base_path = Path(sys.executable).parent
        else:
            base_path = Path(__file__).parent.parent.parent
        
        data_dir = base_path / "data"
        self._ensure_dirs(data_dir)
        return data_dir

    def _ensure_dirs(self, data_dir: Path):
        """Ensure data directories exist."""
        data_dir.mkdir(parents=True, exist_ok=True)
        (data_dir / "cache").mkdir(exist_ok=True)
        (data_dir / "exports").mkdir(exist_ok=True)
        (data_dir / "plots").mkdir(exist_ok=True)
        (data_dir / "assets").mkdir(exist_ok=True)
        

    
    def _setup_page(self):
        """Configure the page settings."""
        self.page.title = "TEMPO Analyzer"
        self.page.theme = create_light_theme(font_scale=self.config.font_scale)
        self.page.dark_theme = create_dark_theme()
        self.page.theme_mode = ft.ThemeMode.LIGHT
        self.page.bgcolor = LightColors.BACKGROUND
        
        # Window sizing
        self.page.window.width = Sizing.WINDOW_DEFAULT_WIDTH
        self.page.window.height = Sizing.WINDOW_DEFAULT_HEIGHT
        self.page.window.min_width = Sizing.WINDOW_MIN_WIDTH
        self.page.window.min_height = Sizing.WINDOW_MIN_HEIGHT
        # Note: window.center() is async in newer Flet, skip for now
        
        # Padding is handled by shell
        self.page.padding = 0
        self.page.spacing = 0
    
    def _build_ui(self):
        """Build the main UI."""
        # Create shell with navigation
        self.shell = AppShell(
            page=self.page,
            on_route_change=self._on_route_change,
        )
        
        # Add shell to page
        self.page.add(self.shell)
        
        # Navigate to default route - Library is now home
        self._on_route_change("/library")
    
    def _on_route_change(self, route: str):
        """Handle route changes."""
        # Get or create page content
        content = self._get_page_content(route)
        self.shell.set_content(content)
    
    def _get_page_content(self, route: str) -> ft.Control:
        """Get content for a route (lazy loading)."""
        if route in self._pages:
            return self._pages[route]

        # Create page based on route
        # Extract base route and any parameters (e.g., /workspace/123)
        route_parts = route.split("/")
        base_route = "/" + route_parts[1] if len(route_parts) > 1 else route
        route_param = route_parts[2] if len(route_parts) > 2 else None
        
        # Use base route for caching
        cache_key = route
        
        if base_route == "/library":
            content = LibraryPage(db=self.db)
        elif base_route == "/new":
            # New Dataset page (formerly Create)
            content = CreatePage(db=self.db, config=self.config)
        elif base_route == "/batch":
            content = BatchImportPage(db=self.db, config=self.config, data_dir=self.data_dir)
        elif base_route == "/workspace":
            # Workspace page - unified Map/Export/Sites view
            content = WorkspacePage(db=self.db, data_dir=self.data_dir, dataset_id=route_param)
        elif base_route == "/sites":
            content = SitesPage(db=self.db)
        elif base_route == "/settings":
            content = SettingsPage(
                config=self.config,
                on_restart_request=self._show_restart_dialog
            )
        elif base_route == "/export":
            content = ExportPage(db=self.db, data_dir=self.data_dir, dataset_id=route_param)
        # Legacy routes (keep for backward compatibility during transition)
        elif route == "/create":
            content = CreatePage(db=self.db, config=self.config)
        elif route == "/plot":
            content = PlotPage(db=self.db, data_dir=self.data_dir)
        elif route == "/inspect":
            content = InspectPage(db=self.db)
        else:
            content = self._create_page_placeholder("Unknown Page", "‚ùì")

        self._pages[route] = content
        return content
    
    def _create_page_placeholder(self, title: str, icon: str) -> ft.Control:
        """Create a placeholder page (to be replaced with real implementations)."""
        return ft.Container(
            content=ft.Column(
                controls=[
                    ft.Text(
                        f"{icon} {title}",
                        size=32,
                        weight=ft.FontWeight.BOLD,
                        color=Colors.ON_SURFACE,
                    ),
                    ft.Container(height=20),
                    ft.Text(
                        "This page is under construction.",
                        size=16,
                        color=Colors.ON_SURFACE_VARIANT,
                    ),
                    ft.Container(height=40),
                    ft.Button(
                        "Coming Soon",
                        icon=ft.Icons.CONSTRUCTION,
                        disabled=True,
                    ),
                ],
                horizontal_alignment=ft.CrossAxisAlignment.CENTER,
                alignment=ft.MainAxisAlignment.CENTER,
            ),
            expand=True,
            alignment=ft.Alignment(0, 0),  # Center
        )
    
    def _show_restart_dialog(self, message: str):
        """Show a dialog requesting restart."""
        dlg = ft.AlertDialog(
            title=ft.Text("Restart Required"),
            content=ft.Text(message),
            actions=[
                ft.TextButton("OK", on_click=lambda e: self._close_dialog(dlg))
            ],
            actions_alignment=ft.MainAxisAlignment.END,
        )
        self.page.dialog = dlg
        dlg.open = True
        self.page.update()
        
    def _close_dialog(self, dlg):
        dlg.open = False
        self.page.update()


def main(page: ft.Page):
    """Flet application entry point."""
    App(page)


if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)

    try:
        logger.info("Starting TEMPO Analyzer...")
        # WARNING: ft.app is deprecated, using ft.run as recommended
        ft.run(main)
    except Exception as e:
        logger.error(f"Failed to start app: {e}", exc_info=True)
        input("Press Enter to exit...")  # Keep window open if it crashes immediately


========================================
FILE: src/tempo_app/storage/database.py
========================================
"""SQLite database operations for TEMPO Analyzer.

Handles all CRUD operations for datasets, granules, and exports.
"""

import sqlite3
import json
from datetime import datetime, date
from pathlib import Path
from typing import Optional
from contextlib import contextmanager
import uuid

from .models import (
    Dataset, Granule, ExportRecord, Site, DatasetStatus, BoundingBox, SITES,
    BatchJob, BatchSite, BatchJobStatus, BatchSiteStatus
)


def _parse_date(value) -> date:
    """Parse a date from database, handling both date and datetime strings."""
    if isinstance(value, date):
        return value
    if isinstance(value, datetime):
        return value.date()
    if isinstance(value, str):
        # Handle datetime strings like "2024-06-01 00:00:00"
        if ' ' in value:
            value = value.split(' ')[0]
        return date.fromisoformat(value)
    if isinstance(value, bytes):
        # Handle bytes from SQLite
        value_str = value.decode('utf-8')
        if ' ' in value_str:
            value_str = value_str.split(' ')[0]
        return date.fromisoformat(value_str)
    raise ValueError(f"Cannot parse date from {type(value)}: {value}")


def _robust_date_converter(val: bytes) -> date:
    """SQLite converter for DATE that handles malformed datetime strings."""
    val_str = val.decode('utf-8') if isinstance(val, bytes) else val
    # Handle datetime strings like "2024-06-01 00:00:00"
    if ' ' in val_str:
        val_str = val_str.split(' ')[0]
    return date.fromisoformat(val_str)


# Register our robust converter to override SQLite's built-in date converter
sqlite3.register_converter("DATE", _robust_date_converter)

class Database:
    """SQLite database manager for TEMPO Analyzer."""
    
    def __init__(self, db_path: Path):
        """Initialize database connection.
        
        Args:
            db_path: Path to SQLite database file
        """
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_schema()
    
    @contextmanager
    def _get_connection(self):
        """Context manager for database connections."""
        conn = sqlite3.connect(self.db_path, detect_types=sqlite3.PARSE_DECLTYPES)
        conn.execute("PRAGMA foreign_keys = ON")
        conn.row_factory = sqlite3.Row
        try:
            yield conn
            conn.commit()
        except Exception:
            conn.rollback()
            raise
        finally:
            conn.close()
    
    def _init_schema(self):
        """Create database tables if they don't exist."""
        with self._get_connection() as conn:
            conn.executescript("""
                -- Datasets table
                CREATE TABLE IF NOT EXISTS datasets (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    batch_job_id TEXT,
                    created_at TIMESTAMP NOT NULL,
                    bbox_west REAL NOT NULL,
                    bbox_south REAL NOT NULL,
                    bbox_east REAL NOT NULL,
                    bbox_north REAL NOT NULL,
                    date_start DATE NOT NULL,
                    date_end DATE NOT NULL,
                    day_filter TEXT NOT NULL,
                    hour_filter TEXT NOT NULL,
                    max_cloud REAL NOT NULL,
                    max_sza REAL NOT NULL,
                    status TEXT NOT NULL DEFAULT 'pending',
                    file_path TEXT,
                    file_hash TEXT,
                    file_size_mb REAL DEFAULT 0,
                    last_accessed TIMESTAMP,
                    granule_count INTEGER DEFAULT 0,
                    granules_downloaded INTEGER DEFAULT 0
                );
                
                -- Granules table
                CREATE TABLE IF NOT EXISTS granules (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT NOT NULL REFERENCES datasets(id) ON DELETE CASCADE,
                    date DATE NOT NULL,
                    hour INTEGER NOT NULL,
                    bbox_west REAL NOT NULL,
                    bbox_south REAL NOT NULL,
                    bbox_east REAL NOT NULL,
                    bbox_north REAL NOT NULL,
                    max_cloud REAL NOT NULL,
                    max_sza REAL NOT NULL,
                    downloaded BOOLEAN DEFAULT 0,
                    downloaded_at TIMESTAMP,
                    content_hash TEXT,
                    no2_valid_pixels INTEGER DEFAULT 0,
                    hcho_valid_pixels INTEGER DEFAULT 0,
                    no2_mean REAL,
                    hcho_mean REAL,
                    file_path TEXT,
                    file_size_bytes INTEGER DEFAULT 0,
                    UNIQUE(dataset_id, date, hour)
                );
                
                -- Exports table
                CREATE TABLE IF NOT EXISTS exports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    dataset_id TEXT NOT NULL REFERENCES datasets(id) ON DELETE CASCADE,
                    format TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    file_size_bytes INTEGER DEFAULT 0
                );
                
                -- Sites table
                CREATE TABLE IF NOT EXISTS sites (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    code TEXT NOT NULL UNIQUE,
                    name TEXT,
                    latitude REAL NOT NULL,
                    longitude REAL NOT NULL,
                    created_at TIMESTAMP NOT NULL
                );

                -- Batch Jobs table (for batch site imports)
                CREATE TABLE IF NOT EXISTS batch_jobs (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP NOT NULL,
                    status TEXT NOT NULL DEFAULT 'pending',
                    source_file TEXT,
                    total_sites INTEGER DEFAULT 0,
                    completed_sites INTEGER DEFAULT 0,
                    failed_sites INTEGER DEFAULT 0,
                    default_radius_km REAL DEFAULT 10.0,
                    date_start DATE NOT NULL,
                    date_end DATE NOT NULL,
                    day_filter TEXT NOT NULL,
                    hour_filter TEXT NOT NULL,
                    max_cloud REAL DEFAULT 0.3,
                    max_sza REAL DEFAULT 70.0,
                    batch_size INTEGER DEFAULT 5,
                    last_processed_at TIMESTAMP,
                    error_message TEXT
                );

                -- Batch Sites table (individual sites within a batch job)
                CREATE TABLE IF NOT EXISTS batch_sites (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    batch_job_id TEXT NOT NULL REFERENCES batch_jobs(id) ON DELETE CASCADE,
                    site_name TEXT NOT NULL,
                    latitude REAL NOT NULL,
                    longitude REAL NOT NULL,
                    bbox_west REAL NOT NULL,
                    bbox_south REAL NOT NULL,
                    bbox_east REAL NOT NULL,
                    bbox_north REAL NOT NULL,
                    status TEXT NOT NULL DEFAULT 'pending',
                    dataset_id TEXT REFERENCES datasets(id),
                    error_message TEXT,
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    sequence_number INTEGER NOT NULL,
                    UNIQUE(batch_job_id, sequence_number)
                );

                -- Indexes for common queries
                CREATE INDEX IF NOT EXISTS idx_granules_dataset ON granules(dataset_id);
                CREATE INDEX IF NOT EXISTS idx_granules_hash ON granules(content_hash);
                CREATE INDEX IF NOT EXISTS idx_exports_dataset ON exports(dataset_id);
                CREATE INDEX IF NOT EXISTS idx_batch_sites_job ON batch_sites(batch_job_id);
                CREATE INDEX IF NOT EXISTS idx_batch_sites_status ON batch_sites(status);
            """)

            # Run migrations for existing databases
            self._run_migrations(conn)

    def _run_migrations(self, conn):
        """Run schema migrations for existing databases."""
        # Check if batch_job_id column exists in datasets table
        cursor = conn.execute("PRAGMA table_info(datasets)")
        columns = [row[1] for row in cursor.fetchall()]

        if "batch_job_id" not in columns:
            conn.execute("ALTER TABLE datasets ADD COLUMN batch_job_id TEXT")

    # ==========================================================================
    # Dataset Operations
    # ==========================================================================
    
    def create_dataset(self, dataset: Dataset) -> Dataset:
        """Insert a new dataset into the database."""
        if not dataset.id:
            dataset.id = str(uuid.uuid4())
        
        with self._get_connection() as conn:
            conn.execute("""
                INSERT INTO datasets (
                    id, name, batch_job_id, created_at, bbox_west, bbox_south, bbox_east, bbox_north,
                    date_start, date_end, day_filter, hour_filter, max_cloud, max_sza,
                    status, file_path, file_hash, file_size_mb, last_accessed,
                    granule_count, granules_downloaded
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                dataset.id, dataset.name, dataset.batch_job_id, dataset.created_at,
                dataset.bbox.west, dataset.bbox.south, dataset.bbox.east, dataset.bbox.north,
                dataset.date_start, dataset.date_end,
                json.dumps(dataset.day_filter), json.dumps(dataset.hour_filter),
                dataset.max_cloud, dataset.max_sza,
                dataset.status.value, dataset.file_path, dataset.file_hash, dataset.file_size_mb,
                dataset.last_accessed, dataset.granule_count, dataset.granules_downloaded
            ))
        return dataset
    
    def get_dataset(self, dataset_id: str) -> Optional[Dataset]:
        """Get a dataset by ID."""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM datasets WHERE id = ?", (dataset_id,)
            ).fetchone()
            return self._row_to_dataset(row) if row else None
    
    def get_dataset_by_name(self, name: str) -> Optional[Dataset]:
        """Get a dataset by name."""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM datasets WHERE name = ?", (name,)
            ).fetchone()
            return self._row_to_dataset(row) if row else None
    
    def get_all_datasets(self) -> list[Dataset]:
        """Get all datasets ordered by creation date (newest first)."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM datasets ORDER BY created_at DESC"
            ).fetchall()
            return [self._row_to_dataset(row) for row in rows]
    
    def update_dataset(self, dataset: Dataset) -> None:
        """Update an existing dataset."""
        with self._get_connection() as conn:
            conn.execute("""
                UPDATE datasets SET
                    name = ?, status = ?, file_path = ?, file_hash = ?, file_size_mb = ?,
                    last_accessed = ?, granule_count = ?, granules_downloaded = ?
                WHERE id = ?
            """, (
                dataset.name, dataset.status.value, dataset.file_path, dataset.file_hash,
                dataset.file_size_mb, dataset.last_accessed, dataset.granule_count,
                dataset.granules_downloaded, dataset.id
            ))
    
    def delete_dataset(self, dataset_id: str) -> None:
        """Delete a dataset and all its granules/exports (including files)."""
        dataset = self.get_dataset(dataset_id)
        if not dataset:
            return

        # Get all related files to delete
        files_to_delete = []
        if dataset.file_path:
            files_to_delete.append(Path(dataset.file_path))
            
        granules = self.get_granules_for_dataset(dataset_id)
        for g in granules:
            if g.file_path:
                files_to_delete.append(Path(g.file_path))
                
        exports = self.get_exports_for_dataset(dataset_id)
        for e in exports:
            if e.file_path:
                files_to_delete.append(Path(e.file_path))
        
        # Delete files from disk
        for file_path in files_to_delete:
            try:
                if file_path.exists():
                    file_path.unlink()
            except Exception as e:
                print(f"Error deleting file {file_path}: {e}")
        
        # Delete the dataset folder if it exists
        if dataset.file_path:
            dataset_dir = Path(dataset.file_path).parent
            try:
                if dataset_dir.exists() and dataset_dir.is_dir():
                    import shutil
                    shutil.rmtree(dataset_dir, ignore_errors=True)
                    print(f"Deleted dataset folder: {dataset_dir}")
            except Exception as e:
                print(f"Error deleting folder {dataset_dir}: {e}")

        # Delete from database (cascade will handle granules/exports)
        # But we must manually unlink batch_sites which don't cascade
        with self._get_connection() as conn:
            conn.execute("UPDATE batch_sites SET dataset_id = NULL, status = 'pending' WHERE dataset_id = ?", (dataset_id,))
            conn.execute("DELETE FROM datasets WHERE id = ?", (dataset_id,))
    
    def touch_dataset(self, dataset_id: str) -> None:
        """Update last_accessed timestamp."""
        with self._get_connection() as conn:
            conn.execute(
                "UPDATE datasets SET last_accessed = ? WHERE id = ?",
                (datetime.now(), dataset_id)
            )
    
    def _row_to_dataset(self, row: sqlite3.Row) -> Dataset:
        """Convert a database row to a Dataset object."""
        return Dataset(
            id=row["id"],
            name=row["name"],
            batch_job_id=row["batch_job_id"] if "batch_job_id" in row.keys() else None,
            created_at=row["created_at"] if isinstance(row["created_at"], datetime) 
                       else datetime.fromisoformat(row["created_at"]),
            bbox=BoundingBox(
                row["bbox_west"], row["bbox_south"], row["bbox_east"], row["bbox_north"]
            ),
            date_start=_parse_date(row["date_start"]),
            date_end=_parse_date(row["date_end"]),
            day_filter=json.loads(row["day_filter"]),
            hour_filter=json.loads(row["hour_filter"]),
            max_cloud=row["max_cloud"],
            max_sza=row["max_sza"],
            status=DatasetStatus(row["status"]),
            file_path=row["file_path"],
            file_hash=row["file_hash"],
            file_size_mb=row["file_size_mb"] or 0,
            last_accessed=row["last_accessed"],
            granule_count=row["granule_count"] or 0,
            granules_downloaded=row["granules_downloaded"] or 0,
        )
    
    # ==========================================================================
    # Granule Operations
    # ==========================================================================
    
    def create_granule(self, granule: Granule) -> Granule:
        """Insert a new granule."""
        # Compute hash if not set
        if not granule.content_hash:
            granule.content_hash = granule.compute_content_hash()
        
        with self._get_connection() as conn:
            cursor = conn.execute("""
                INSERT INTO granules (
                    dataset_id, date, hour, bbox_west, bbox_south, bbox_east, bbox_north,
                    max_cloud, max_sza, downloaded, downloaded_at, content_hash,
                    no2_valid_pixels, hcho_valid_pixels, no2_mean, hcho_mean,
                    file_path, file_size_bytes
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                granule.dataset_id, granule.date, granule.hour,
                granule.bbox_west, granule.bbox_south, granule.bbox_east, granule.bbox_north,
                granule.max_cloud, granule.max_sza, granule.downloaded, granule.downloaded_at,
                granule.content_hash, granule.no2_valid_pixels, granule.hcho_valid_pixels,
                granule.no2_mean, granule.hcho_mean, granule.file_path, granule.file_size_bytes
            ))
            granule.id = cursor.lastrowid
        return granule
    
    def create_granules_batch(self, granules: list[Granule]) -> None:
        """Insert multiple granules efficiently."""
        for g in granules:
            if not g.content_hash:
                g.content_hash = g.compute_content_hash()
        
        with self._get_connection() as conn:
            conn.executemany("""
                INSERT OR IGNORE INTO granules (
                    dataset_id, date, hour, bbox_west, bbox_south, bbox_east, bbox_north,
                    max_cloud, max_sza, downloaded, downloaded_at, content_hash,
                    no2_valid_pixels, hcho_valid_pixels, no2_mean, hcho_mean,
                    file_path, file_size_bytes
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, [
                (
                    g.dataset_id, g.date, g.hour,
                    g.bbox_west, g.bbox_south, g.bbox_east, g.bbox_north,
                    g.max_cloud, g.max_sza, g.downloaded, g.downloaded_at,
                    g.content_hash, g.no2_valid_pixels, g.hcho_valid_pixels,
                    g.no2_mean, g.hcho_mean, g.file_path, g.file_size_bytes
                )
                for g in granules
            ])
    
    def get_granules_for_dataset(self, dataset_id: str) -> list[Granule]:
        """Get all granules for a dataset."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM granules WHERE dataset_id = ? ORDER BY date, hour",
                (dataset_id,)
            ).fetchall()
            return [self._row_to_granule(row) for row in rows]
    
    def get_pending_granules(self, dataset_id: str) -> list[Granule]:
        """Get granules that haven't been downloaded yet."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM granules WHERE dataset_id = ? AND downloaded = 0 ORDER BY date, hour",
                (dataset_id,)
            ).fetchall()
            return [self._row_to_granule(row) for row in rows]
    
    def find_granule_by_hash(self, content_hash: str) -> Optional[Granule]:
        """Find a granule with matching content hash (for deduplication)."""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM granules WHERE content_hash = ? AND downloaded = 1 LIMIT 1",
                (content_hash,)
            ).fetchone()
            return self._row_to_granule(row) if row else None
    
    def update_granule(self, granule: Granule) -> None:
        """Update a granule after download."""
        with self._get_connection() as conn:
            conn.execute("""
                UPDATE granules SET
                    downloaded = ?, downloaded_at = ?, no2_valid_pixels = ?,
                    hcho_valid_pixels = ?, no2_mean = ?, hcho_mean = ?,
                    file_path = ?, file_size_bytes = ?
                WHERE id = ?
            """, (
                granule.downloaded, granule.downloaded_at, granule.no2_valid_pixels,
                granule.hcho_valid_pixels, granule.no2_mean, granule.hcho_mean,
                granule.file_path, granule.file_size_bytes, granule.id
            ))
    
    def mark_granules_downloaded(self, dataset_id: str) -> None:
        """Mark all granules for a dataset as downloaded."""
        with self._get_connection() as conn:
            conn.execute("""
                UPDATE granules SET
                    downloaded = 1,
                    downloaded_at = ?
                WHERE dataset_id = ?
            """, (datetime.now(), dataset_id))
    
    def _row_to_granule(self, row: sqlite3.Row) -> Granule:
        """Convert a database row to a Granule object."""
        return Granule(
            id=row["id"],
            dataset_id=row["dataset_id"],
            date=_parse_date(row["date"]),
            hour=row["hour"],
            bbox_west=row["bbox_west"],
            bbox_south=row["bbox_south"],
            bbox_east=row["bbox_east"],
            bbox_north=row["bbox_north"],
            max_cloud=row["max_cloud"],
            max_sza=row["max_sza"],
            downloaded=bool(row["downloaded"]),
            downloaded_at=row["downloaded_at"],
            content_hash=row["content_hash"],
            no2_valid_pixels=row["no2_valid_pixels"] or 0,
            hcho_valid_pixels=row["hcho_valid_pixels"] or 0,
            no2_mean=row["no2_mean"],
            hcho_mean=row["hcho_mean"],
            file_path=row["file_path"],
            file_size_bytes=row["file_size_bytes"] or 0,
        )
    
    # ==========================================================================
    # Export Operations
    # ==========================================================================
    
    def create_export(self, export: ExportRecord) -> ExportRecord:
        """Insert a new export record."""
        with self._get_connection() as conn:
            cursor = conn.execute("""
                INSERT INTO exports (dataset_id, format, file_path, created_at, file_size_bytes)
                VALUES (?, ?, ?, ?, ?)
            """, (
                export.dataset_id, export.format, export.file_path,
                export.created_at, export.file_size_bytes
            ))
            export.id = cursor.lastrowid
        return export
    
    def get_exports_for_dataset(self, dataset_id: str) -> list[ExportRecord]:
        """Get all exports for a dataset."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM exports WHERE dataset_id = ? ORDER BY created_at DESC",
                (dataset_id,)
            ).fetchall()
            return [
                ExportRecord(
                    id=row["id"],
                    dataset_id=row["dataset_id"],
                    format=row["format"],
                    file_path=row["file_path"],
                    created_at=row["created_at"],
                    file_size_bytes=row["file_size_bytes"] or 0,
                )
                for row in rows
            ]
    
    # ==========================================================================
    # Site Operations
    # ==========================================================================
    
    def create_site(self, site: Site) -> Site:
        """Insert a new site."""
        with self._get_connection() as conn:
            cursor = conn.execute("""
                INSERT INTO sites (code, name, latitude, longitude, created_at)
                VALUES (?, ?, ?, ?, ?)
            """, (
                site.code, site.name, site.latitude, site.longitude, site.created_at
            ))
            site.id = cursor.lastrowid
        return site
    
    def get_all_sites(self) -> list[Site]:
        """Get all sites ordered by code."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM sites ORDER BY code"
            ).fetchall()
            return [self._row_to_site(row) for row in rows]
    
    def get_sites_in_bbox(self, bbox: BoundingBox) -> list[Site]:
        """Get sites within a bounding box."""
        with self._get_connection() as conn:
            rows = conn.execute("""
                SELECT * FROM sites 
                WHERE latitude >= ? AND latitude <= ?
                  AND longitude >= ? AND longitude <= ?
                ORDER BY code
            """, (bbox.south, bbox.north, bbox.west, bbox.east)).fetchall()
            return [self._row_to_site(row) for row in rows]
    
    def get_sites_as_dict(self,  bbox: BoundingBox = None) -> dict[str, tuple[float, float]]:
        """Get sites as dict format for plotter compatibility.

        Returns:
            Dict mapping site code to (latitude, longitude) tuple
        """
        if bbox:
            sites = self.get_sites_in_bbox(bbox)
        else:
            sites = self.get_all_sites()
        return {s.code: s.to_tuple() for s in sites}

    def get_site_by_code(self, code: str) -> Optional[Site]:
        """Get a site by its code."""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM sites WHERE code = ?", (code,)
            ).fetchone()
            return self._row_to_site(row) if row else None

    def delete_site(self, site_id: int) -> None:
        """Delete a site by ID."""
        with self._get_connection() as conn:
            conn.execute("DELETE FROM sites WHERE id = ?", (site_id,))
    
    def delete_site_by_code(self, code: str) -> None:
        """Delete a site by code."""
        with self._get_connection() as conn:
            conn.execute("DELETE FROM sites WHERE code = ?", (code,))
    
    def seed_default_sites(self) -> int:
        """Import hardcoded default sites from models.SITES.
        
        Skips sites that already exist (by code).
        
        Returns:
            Number of sites added
        """
        added = 0
        for code, (lat, lon) in SITES.items():
            try:
                site = Site(
                    code=code,
                    name="",  # No name in hardcoded data
                    latitude=lat,
                    longitude=lon,
                    created_at=datetime.now()
                )
                self.create_site(site)
                added += 1
            except sqlite3.IntegrityError:
                # Site already exists (code is UNIQUE)
                pass
        return added
    
    def _row_to_site(self, row: sqlite3.Row) -> Site:
        """Convert a database row to a Site object."""
        return Site(
            id=row["id"],
            code=row["code"],
            name=row["name"] or "",
            latitude=row["latitude"],
            longitude=row["longitude"],
            created_at=row["created_at"] if isinstance(row["created_at"], datetime)
                       else datetime.fromisoformat(row["created_at"]),
        )
    
    # ==========================================================================
    # Storage Analytics
    # ==========================================================================
    
    def get_storage_stats(self) -> dict:
        """Get storage usage statistics."""
        with self._get_connection() as conn:
            # Total dataset size
            dataset_size = conn.execute(
                "SELECT COALESCE(SUM(file_size_mb), 0) FROM datasets"
            ).fetchone()[0]
            
            # Total granule size
            granule_size = conn.execute(
                "SELECT COALESCE(SUM(file_size_bytes), 0) FROM granules"
            ).fetchone()[0] / (1024 * 1024)  # Convert to MB
            
            # Export size
            export_size = conn.execute(
                "SELECT COALESCE(SUM(file_size_bytes), 0) FROM exports"
            ).fetchone()[0] / (1024 * 1024)
            
            # Counts
            dataset_count = conn.execute("SELECT COUNT(*) FROM datasets").fetchone()[0]
            granule_count = conn.execute("SELECT COUNT(*) FROM granules").fetchone()[0]
            
            return {
                "dataset_size_mb": dataset_size,
                "granule_size_mb": granule_size,
                "export_size_mb": export_size,
                "total_size_mb": dataset_size + granule_size + export_size,
                "dataset_count": dataset_count,
                "granule_count": granule_count,
            }

    # ==========================================================================
    # Batch Job Operations
    # ==========================================================================

    def create_batch_job(self, job: BatchJob) -> BatchJob:
        """Insert a new batch job."""
        if not job.id:
            job.id = str(uuid.uuid4())

        with self._get_connection() as conn:
            conn.execute("""
                INSERT INTO batch_jobs (
                    id, name, created_at, status, source_file,
                    total_sites, completed_sites, failed_sites,
                    default_radius_km, date_start, date_end, day_filter, hour_filter,
                    max_cloud, max_sza, batch_size, last_processed_at, error_message
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                job.id, job.name, job.created_at, job.status.value, job.source_file,
                job.total_sites, job.completed_sites, job.failed_sites,
                job.default_radius_km, job.date_start, job.date_end,
                json.dumps(job.day_filter), json.dumps(job.hour_filter),
                job.max_cloud, job.max_sza, job.batch_size, job.last_processed_at,
                job.error_message
            ))
        return job

    def get_batch_job(self, job_id: str) -> Optional[BatchJob]:
        """Get a batch job by ID."""
        with self._get_connection() as conn:
            row = conn.execute(
                "SELECT * FROM batch_jobs WHERE id = ?", (job_id,)
            ).fetchone()
            return self._row_to_batch_job(row) if row else None

    def get_all_batch_jobs(self) -> list[BatchJob]:
        """Get all batch jobs ordered by creation date (newest first)."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM batch_jobs ORDER BY created_at DESC"
            ).fetchall()
            return [self._row_to_batch_job(row) for row in rows]

    def get_resumable_batch_jobs(self) -> list[BatchJob]:
        """Get batch jobs that can be resumed (PAUSED or ERROR status)."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM batch_jobs WHERE status IN ('paused', 'error', 'running') ORDER BY created_at DESC"
            ).fetchall()
            return [self._row_to_batch_job(row) for row in rows]

    def update_batch_job(self, job: BatchJob) -> None:
        """Update an existing batch job."""
        with self._get_connection() as conn:
            conn.execute("""
                UPDATE batch_jobs SET
                    status = ?, total_sites = ?, completed_sites = ?, failed_sites = ?,
                    batch_size = ?, last_processed_at = ?, error_message = ?
                WHERE id = ?
            """, (
                job.status.value, job.total_sites, job.completed_sites, job.failed_sites,
                job.batch_size, job.last_processed_at, job.error_message, job.id
            ))

    def delete_batch_job(self, job_id: str) -> None:
        """Delete a batch job and all its sites (cascade)."""
        with self._get_connection() as conn:
            conn.execute("DELETE FROM batch_jobs WHERE id = ?", (job_id,))

    def delete_batch_job_full(self, job_id: str) -> None:
        """Delete a batch job, all associated datasets, files, and sites."""
        import shutil
        from pathlib import Path

        # Get the batch job to find the folder name
        job = self.get_batch_job(job_id)
        if not job:
            return

        # Get all datasets in this batch
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM datasets WHERE batch_job_id = ?", (job_id,)
            ).fetchall()

        dataset_ids = [row["id"] for row in rows]

        # Delete dataset files and folders
        deleted_folders = set()
        for row in rows:
            file_path = row["file_path"]
            if file_path:
                fp = Path(file_path)
                # Delete the parent folder (site folder)
                if fp.parent.exists() and fp.parent not in deleted_folders:
                    try:
                        shutil.rmtree(fp.parent)
                        deleted_folders.add(fp.parent)
                    except Exception as e:
                        print(f"Error deleting folder {fp.parent}: {e}")

        # Try to delete the job folder (parent of site folders)
        if deleted_folders:
            job_folder = next(iter(deleted_folders)).parent
            if job_folder.exists() and not any(job_folder.iterdir()):
                try:
                    job_folder.rmdir()
                except Exception:
                    pass

        # Delete from database - use raw connection to disable FK checks
        conn = sqlite3.connect(self.db_path)
        try:
            conn.execute("PRAGMA foreign_keys = OFF")
            cursor = conn.cursor()
            # Delete granules for each dataset
            for ds_id in dataset_ids:
                cursor.execute("DELETE FROM granules WHERE dataset_id = ?", (ds_id,))
            # Delete exports for each dataset
            for ds_id in dataset_ids:
                cursor.execute("DELETE FROM exports WHERE dataset_id = ?", (ds_id,))
            # Delete batch sites
            cursor.execute("DELETE FROM batch_sites WHERE batch_job_id = ?", (job_id,))
            # Delete datasets
            cursor.execute("DELETE FROM datasets WHERE batch_job_id = ?", (job_id,))
            # Delete the batch job
            cursor.execute("DELETE FROM batch_jobs WHERE id = ?", (job_id,))
            conn.commit()
        finally:
            conn.close()

    def _row_to_batch_job(self, row: sqlite3.Row) -> BatchJob:
        """Convert a database row to a BatchJob object."""
        return BatchJob(
            id=row["id"],
            name=row["name"],
            created_at=row["created_at"] if isinstance(row["created_at"], datetime)
                       else datetime.fromisoformat(row["created_at"]),
            status=BatchJobStatus(row["status"]),
            source_file=row["source_file"],
            total_sites=row["total_sites"] or 0,
            completed_sites=row["completed_sites"] or 0,
            failed_sites=row["failed_sites"] or 0,
            default_radius_km=row["default_radius_km"] or 10.0,
            date_start=_parse_date(row["date_start"]),
            date_end=_parse_date(row["date_end"]),
            day_filter=json.loads(row["day_filter"]),
            hour_filter=json.loads(row["hour_filter"]),
            max_cloud=row["max_cloud"] or 0.3,
            max_sza=row["max_sza"] or 70.0,
            batch_size=row["batch_size"] or 5,
            last_processed_at=row["last_processed_at"],
            error_message=row["error_message"],
        )

    # ==========================================================================
    # Batch Site Operations
    # ==========================================================================

    def create_batch_sites(self, sites: list[BatchSite]) -> None:
        """Insert multiple batch sites efficiently."""
        with self._get_connection() as conn:
            conn.executemany("""
                INSERT INTO batch_sites (
                    batch_job_id, site_name, latitude, longitude, radius_km,
                    bbox_west, bbox_south, bbox_east, bbox_north,
                    status, dataset_id, error_message, started_at, completed_at, sequence_number
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, [
                (
                    s.batch_job_id, s.site_name, s.latitude, s.longitude, s.radius_km,
                    s.bbox_west, s.bbox_south, s.bbox_east, s.bbox_north,
                    s.status.value, s.dataset_id, s.error_message, s.started_at, s.completed_at,
                    s.sequence_number
                )
                for s in sites
            ])

    def get_batch_sites(self, job_id: str) -> list[BatchSite]:
        """Get all sites for a batch job."""
        with self._get_connection() as conn:
            rows = conn.execute(
                "SELECT * FROM batch_sites WHERE batch_job_id = ? ORDER BY sequence_number",
                (job_id,)
            ).fetchall()
            return [self._row_to_batch_site(row) for row in rows]

    def get_pending_batch_sites(self, job_id: str) -> list[BatchSite]:
        """Get pending/queued sites for a batch job (for resume)."""
        with self._get_connection() as conn:
            rows = conn.execute("""
                SELECT * FROM batch_sites
                WHERE batch_job_id = ? AND status IN ('pending', 'queued')
                ORDER BY sequence_number
            """, (job_id,)).fetchall()
            return [self._row_to_batch_site(row) for row in rows]

    def update_batch_site(self, site: BatchSite) -> None:
        """Update a batch site status."""
        with self._get_connection() as conn:
            conn.execute("""
                UPDATE batch_sites SET
                    status = ?, dataset_id = ?, error_message = ?,
                    started_at = ?, completed_at = ?
                WHERE id = ?
            """, (
                site.status.value, site.dataset_id, site.error_message,
                site.started_at, site.completed_at, site.id
            ))

    def reset_interrupted_batch_sites(self, job_id: str) -> int:
        """Reset sites that were interrupted (downloading/processing) back to pending.

        Returns number of sites reset.
        """
        with self._get_connection() as conn:
            cursor = conn.execute("""
                UPDATE batch_sites SET
                    status = 'pending',
                    error_message = 'Interrupted by app restart'
                WHERE batch_job_id = ? AND status IN ('downloading', 'processing', 'queued')
            """, (job_id,))
            return cursor.rowcount

    def _row_to_batch_site(self, row: sqlite3.Row) -> BatchSite:
        """Convert a database row to a BatchSite object."""
        return BatchSite(
            id=row["id"],
            batch_job_id=row["batch_job_id"],
            site_name=row["site_name"],
            latitude=row["latitude"],
            longitude=row["longitude"],
            radius_km=row["radius_km"],
            bbox_west=row["bbox_west"],
            bbox_south=row["bbox_south"],
            bbox_east=row["bbox_east"],
            bbox_north=row["bbox_north"],
            status=BatchSiteStatus(row["status"]),
            dataset_id=row["dataset_id"],
            error_message=row["error_message"],
            started_at=row["started_at"],
            completed_at=row["completed_at"],
            sequence_number=row["sequence_number"],
        )


========================================
FILE: src/tempo_app/storage/models.py
========================================
"""Data models for TEMPO Analyzer storage system.

Defines dataclasses for Dataset, Granule, and ExportRecord that map to SQLite tables.
"""

from dataclasses import dataclass, field
from datetime import datetime, date
from enum import Enum
from typing import Optional
import json
import hashlib


class DatasetStatus(Enum):
    """Status of a dataset download."""
    PENDING = "pending"
    DOWNLOADING = "downloading"
    PARTIAL = "partial"
    COMPLETE = "complete"
    ERROR = "error"


class BatchJobStatus(Enum):
    """Status of a batch import job."""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    ERROR = "error"


class BatchSiteStatus(Enum):
    """Status of a single site within a batch job."""
    PENDING = "pending"
    QUEUED = "queued"
    DOWNLOADING = "downloading"
    PROCESSING = "processing"
    COMPLETED = "completed"
    ERROR = "error"
    SKIPPED = "skipped"


@dataclass
class BoundingBox:
    """Geographic bounding box."""
    west: float
    south: float
    east: float
    north: float
    
    def to_list(self) -> list[float]:
        """Return as [west, south, east, north] list."""
        return [self.west, self.south, self.east, self.north]
    
    @classmethod
    def from_list(cls, coords: list[float]) -> "BoundingBox":
        """Create from [west, south, east, north] list."""
        return cls(west=coords[0], south=coords[1], east=coords[2], north=coords[3])
    
    def contains_point(self, lat: float, lon: float) -> bool:
        """Check if a point is inside this bounding box."""
        return (self.west <= lon <= self.east) and (self.south <= lat <= self.north)


@dataclass
class Dataset:
    """A user-created dataset configuration and its download status."""
    id: str                          # UUID
    name: str                        # User-friendly name
    created_at: datetime
    
    # Geographic region
    bbox: BoundingBox
    
    # Temporal filters
    date_start: date
    date_end: date
    day_filter: list[int]            # 0=Mon, 1=Tue, ..., 6=Sun
    hour_filter: list[int]           # UTC hours (0-23)
    
    # Quality filters
    max_cloud: float                 # 0.0-1.0
    max_sza: float                   # Solar zenith angle in degrees
    
    # Download status
    status: DatasetStatus = DatasetStatus.PENDING
    batch_job_id: Optional[str] = None # ID of batch job if part of one
    file_path: Optional[str] = None  # Path to combined .nc file
    file_hash: Optional[str] = None  # SHA256 of file content
    file_size_mb: float = 0.0
    
    # Metadata
    last_accessed: Optional[datetime] = None
    granule_count: int = 0
    granules_downloaded: int = 0
    
    @property
    def progress(self) -> float:
        """Download progress as 0.0-1.0."""
        if self.granule_count == 0:
            return 0.0
        return self.granules_downloaded / self.granule_count
    
    @property
    def is_complete(self) -> bool:
        """Check if all granules are downloaded."""
        return self.status == DatasetStatus.COMPLETE
    
    def day_filter_str(self) -> str:
        """Human-readable day filter string."""
        day_names = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
        if self.day_filter == [0, 1, 2, 3, 4]:
            return "Weekdays"
        elif self.day_filter == [5, 6]:
            return "Weekends"
        elif self.day_filter == [0, 1, 2, 3, 4, 5, 6]:
            return "All Days"
        else:
            return ", ".join(day_names[d] for d in sorted(self.day_filter))
    
    def hour_filter_str(self) -> str:
        """Human-readable hour filter string."""
        if not self.hour_filter:
            return "None"
        hours = sorted(self.hour_filter)
        return f"{hours[0]:02d}:00-{hours[-1]:02d}:00 UTC"


@dataclass
class Granule:
    """A single hourly data granule (one hour of data for a region)."""
    id: Optional[int] = None         # Auto-increment ID
    dataset_id: str = ""             # Parent dataset UUID
    
    # Temporal
    date: date = field(default_factory=date.today)
    hour: int = 0                    # UTC hour (0-23)
    
    # Spatial (copied from dataset for inspection)
    bbox_west: float = 0.0
    bbox_south: float = 0.0
    bbox_east: float = 0.0
    bbox_north: float = 0.0
    
    # Filters (copied from dataset for inspection)
    max_cloud: float = 0.5
    max_sza: float = 70.0
    
    # Status
    downloaded: bool = False
    downloaded_at: Optional[datetime] = None
    
    # Content info
    content_hash: str = ""           # SHA256 of request params
    no2_valid_pixels: int = 0
    hcho_valid_pixels: int = 0
    no2_mean: Optional[float] = None
    hcho_mean: Optional[float] = None
    
    # File reference
    file_path: Optional[str] = None
    file_size_bytes: int = 0
    
    def compute_content_hash(self) -> str:
        """Compute SHA256 hash of request parameters for deduplication."""
        hash_input = {
            "bbox": [self.bbox_west, self.bbox_south, self.bbox_east, self.bbox_north],
            "date": self.date.isoformat(),
            "hour": self.hour,
            "max_cloud_fraction": round(self.max_cloud, 4),
            "max_solar_zenith_angle": round(self.max_sza, 2),
            "grid_kw": "1US1"
        }
        json_str = json.dumps(hash_input, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
    
    @property
    def datetime_str(self) -> str:
        """Formatted datetime string."""
        return f"{self.date.isoformat()} @ {self.hour:02d}:00 UTC"


@dataclass
class ExportRecord:
    """Record of an export operation."""
    id: Optional[int] = None
    dataset_id: str = ""
    format: str = "Legacy"           # 'Legacy', 'V1_Hourly', 'V2_Daily_Wide'
    file_path: str = ""
    created_at: datetime = field(default_factory=datetime.now)
    file_size_bytes: int = 0


@dataclass
class Site:
    """A monitoring site to mark on maps."""
    id: Optional[int] = None
    code: str = ""                   # Short code (e.g., "BV", "LC")
    name: str = ""                   # Full name (e.g., "Bountiful, UT")
    latitude: float = 0.0
    longitude: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)

    def to_tuple(self) -> tuple[float, float]:
        """Return as (latitude, longitude) tuple for plotter compatibility."""
        return (self.latitude, self.longitude)


@dataclass
class BatchJob:
    """A batch import job for processing multiple sites from Excel/CSV."""
    id: str                          # UUID
    name: str                        # Job name
    created_at: datetime
    status: BatchJobStatus = BatchJobStatus.PENDING
    source_file: Optional[str] = None  # Path to source Excel/CSV file

    # Progress counts
    total_sites: int = 0
    completed_sites: int = 0
    failed_sites: int = 0

    # Default settings (can be overridden per site)
    default_radius_km: float = 10.0
    date_start: date = field(default_factory=date.today)
    date_end: date = field(default_factory=date.today)
    day_filter: list[int] = field(default_factory=lambda: [0, 1, 2, 3, 4])  # Weekdays
    hour_filter: list[int] = field(default_factory=lambda: [16, 17, 18, 19, 20])  # UTC hours
    max_cloud: float = 0.3
    max_sza: float = 70.0

    # Processing config
    batch_size: int = 5              # Sites to process in parallel
    last_processed_at: Optional[datetime] = None
    error_message: Optional[str] = None

    @property
    def progress(self) -> float:
        """Batch progress as 0.0-1.0."""
        if self.total_sites == 0:
            return 0.0
        return (self.completed_sites + self.failed_sites) / self.total_sites

    @property
    def is_resumable(self) -> bool:
        """Check if this job can be resumed."""
        return self.status in (BatchJobStatus.PAUSED, BatchJobStatus.ERROR)

    @property
    def is_complete(self) -> bool:
        """Check if all sites have been processed."""
        return self.status == BatchJobStatus.COMPLETED


@dataclass
class BatchSite:
    """A single site within a batch import job."""
    id: Optional[int] = None         # Auto-increment ID
    batch_job_id: str = ""           # Parent batch job UUID
    site_name: str = ""              # Site identifier from Excel
    latitude: float = 0.0
    longitude: float = 0.0
    radius_km: float = 10.0          # Radius used (from job default)

    # Computed bounding box
    bbox_west: float = 0.0
    bbox_south: float = 0.0
    bbox_east: float = 0.0
    bbox_north: float = 0.0

    # Processing status
    status: BatchSiteStatus = BatchSiteStatus.PENDING
    dataset_id: Optional[str] = None  # Link to created dataset
    error_message: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None

    # Ordering for resume
    sequence_number: int = 0

    @property
    def bbox(self) -> "BoundingBox":
        """Return computed bbox as BoundingBox object."""
        return BoundingBox(self.bbox_west, self.bbox_south, self.bbox_east, self.bbox_north)


# Region presets with FIPS codes for auto-downloading road data
REGION_PRESETS: dict[str, tuple[BoundingBox, str]] = {
    "Southern California": (BoundingBox(-119.68, 32.23, -116.38, 35.73), "06"),
    "Utah (Salt Lake)": (BoundingBox(-112.8, 40.0, -111.5, 41.5), "49"),
    "Texas (Houston)": (BoundingBox(-96.5, 29.0, -94.5, 30.5), "48"),
    "Arizona (Phoenix)": (BoundingBox(-113.3, 32.8, -111.0, 34.2), "04"),
    "Colorado (Denver)": (BoundingBox(-105.5, 39.3, -104.3, 40.2), "08"),
    "New York City": (BoundingBox(-74.3, 40.4, -73.7, 41.0), "36"),
    "Florida (Miami)": (BoundingBox(-80.5, 25.5, -80.0, 26.0), "12"),
}

# Site locations for marking on maps
SITES: dict[str, tuple[float, float]] = {
    # Utah
    "BV": (40.903, -111.884),
    "HW": (40.736, -111.872),
    "RB": (40.767, -111.828),
    "ER": (40.601, -112.356),
    # Colorado
    "LC": (39.779, -105.005),
    # Arizona
    "PX": (33.504, -112.096),
    # Texas
    "HA": (29.9, -95.33),
    "HB": (29.67, -95.5),
    # California
    "PR": (34.01, -118.069),
    "BN": (33.921, -116.858),
    "PS": (33.853, -116.541),
    "SB": (34.107, -117.274),
}

# FIPS codes for US states
STATE_FIPS: dict[str, str] = {
    "06": "California",
    "48": "Texas",
    "49": "Utah",
    "04": "Arizona",
    "08": "Colorado",
    "36": "New York",
    "12": "Florida",
    "32": "Nevada",
    "35": "New Mexico",
    "41": "Oregon",
    "53": "Washington",
}


========================================
FILE: src/tempo_app/storage/__init__.py
========================================
"""Storage module - Smart caching and database operations."""


========================================
FILE: src/tempo_app/ui/components/column_lane.py
========================================
"""Column Lane - Container for a single column definition."""

import flet as ft
from typing import Callable, List

from ..theme import Colors
from .stack_editor import StackEditor
from ...core.nodes import NodeConfig, ColumnDefinition


class ColumnLane(ft.Container):
    """A vertical lane representing one output column."""
    
    def __init__(
        self,
        column_def: ColumnDefinition = None,
        on_change: Callable = None,
        on_delete: Callable = None,
        index: int = 0,
    ):
        self.column_def = column_def or ColumnDefinition(name="New Column")
        self.on_change = on_change
        self.on_delete = on_delete
        self.index = index
        
        self._name_field = ft.TextField(
            value=self.column_def.name,
            text_size=14,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE, weight=ft.FontWeight.BOLD),
            border_color="transparent",
            focused_border_color=Colors.PRIMARY,
            content_padding=ft.padding.symmetric(horizontal=8, vertical=4),
            on_blur=self._on_name_change,
            expand=True,
        )
        
        self._stack_editor = StackEditor(
            node_configs=self.column_def.node_configs,
            on_change=self._on_stack_change,
        )
        
        super().__init__(
            content=self._build_content(),
            width=280,
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.border.all(1, Colors.BORDER),
            padding=0,
        )
    
    def _build_content(self):
        """Build the lane content."""
        header = ft.Container(
            content=ft.Row([
                self._name_field,
                ft.IconButton(
                    icon=ft.Icons.FULLSCREEN,
                    icon_size=16,
                    icon_color=Colors.ON_SURFACE_VARIANT,
                    tooltip="Expand Column",
                    on_click=self._open_fullscreen,
                ),
                ft.IconButton(
                    icon=ft.Icons.CONTENT_COPY,
                    icon_size=16,
                    icon_color=Colors.ON_SURFACE_VARIANT,
                    tooltip="Duplicate Column",
                    on_click=self._duplicate,
                ),
                ft.IconButton(
                    icon=ft.Icons.DELETE_OUTLINE,
                    icon_size=16,
                    icon_color=Colors.ERROR,
                    tooltip="Delete Column",
                    on_click=lambda e: self.on_delete(self.index) if self.on_delete else None,
                ),
            ], spacing=0),
            bgcolor=Colors.SURFACE_VARIANT,
            padding=ft.padding.symmetric(horizontal=8, vertical=4),
            border_radius=ft.border_radius.only(top_left=12, top_right=12),
        )
        
        return ft.Column([
            header,
            ft.Container(
                content=self._stack_editor,
                padding=8,
                expand=True,
            ),
        ], spacing=0)
    
    def _on_name_change(self, e):
        """Handle column name change."""
        self.column_def.name = self._name_field.value
        if self.on_change:
            self.on_change(self.index, self.column_def)
    
    def _on_stack_change(self, configs: List[NodeConfig]):
        """Handle stack change."""
        self.column_def.node_configs = configs
        if self.on_change:
            self.on_change(self.index, self.column_def)
    
    def _duplicate(self, e):
        """Request duplication of this column."""
        # Emit a change event with a special flag
        if self.on_change:
            new_def = ColumnDefinition(
                name=f"{self.column_def.name}_copy",
                node_configs=[NodeConfig(c.node_type, c.params.copy()) for c in self.column_def.node_configs],
            )
            self.on_change(self.index, new_def, duplicate=True)
    
    def _open_fullscreen(self, e):
        """Open this column's stack in a fullscreen dialog."""
        # Get page from the event or self
        page = e.page if hasattr(e, 'page') and e.page else self.page
        if not page:
            print("ERROR: No page reference available for fullscreen dialog")
            return
        
        # Create a larger stack editor for fullscreen
        self._fullscreen_stack = StackEditor(
            node_configs=[NodeConfig(c.node_type, c.params.copy()) for c in self.column_def.node_configs],
            on_change=self._on_fullscreen_change,
        )
        
        self._fullscreen_dlg = ft.AlertDialog(
            modal=True,
            title=ft.Text(f"Edit: {self.column_def.name}", size=18, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
            content=ft.Container(
                content=ft.Column([
                    self._fullscreen_stack,
                ], scroll=ft.ScrollMode.AUTO),
                width=400,
                height=500,
                padding=8,
            ),
            actions=[
                ft.FilledButton("Done", on_click=self._close_fullscreen),
            ],
        )
        
        page.dialog = self._fullscreen_dlg
        self._fullscreen_dlg.open = True
        page.update()
    
    def _close_fullscreen(self, e):
        """Close fullscreen dialog and sync changes."""
        # Sync changes back to main stack
        if hasattr(self, '_fullscreen_stack'):
            self._stack_editor.configs = self._fullscreen_stack.configs
            self.column_def.node_configs = self._fullscreen_stack.configs
            if self.on_change:
                self.on_change(self.index, self.column_def)
        
        # Close dialog
        if hasattr(self, '_fullscreen_dlg'):
            self._fullscreen_dlg.open = False
        self.page.update()
    
    def _on_fullscreen_change(self, configs: List[NodeConfig]):
        """Handle changes from fullscreen editor (just update the temp config)."""
        # Fullscreen stack manages its own state, we sync on close
        pass
    
    def get_definition(self) -> ColumnDefinition:
        """Get the current column definition."""
        self.column_def.name = self._name_field.value
        self.column_def.node_configs = self._stack_editor.configs
        return self.column_def



========================================
FILE: src/tempo_app/ui/components/map_selector.py
========================================
"""Interactive map component for bounding box selection."""

import flet as ft
import flet.canvas as cv
from typing import Callable, Optional
from dataclasses import dataclass

from ..theme import Colors, Spacing


@dataclass
class MapBounds:
    """Geographic bounds for the map display."""
    west: float = -130.0   # Western edge of visible map
    east: float = -60.0    # Eastern edge
    south: float = 20.0    # Southern edge
    north: float = 55.0    # Northern edge
    
    @property
    def width(self) -> float:
        return self.east - self.west
    
    @property
    def height(self) -> float:
        return self.north - self.south


# North America base map bounds
NA_BOUNDS = MapBounds(west=-130, east=-60, south=20, north=55)


class MapBBoxSelector(ft.Container):
    """Interactive map for selecting a geographic bounding box.
    
    Shows a simplified map of North America with a draggable rectangle
    that users can resize to select their area of interest.
    """
    
    def __init__(
        self,
        initial_bbox: tuple[float, float, float, float] = (-119.68, 32.23, -116.38, 35.73),
        on_change: Optional[Callable[[float, float, float, float], None]] = None,
        width: int = 400,
        height: int = 280,
    ):
        super().__init__()
        
        self._map_bounds = NA_BOUNDS
        self._bbox_west, self._bbox_south, self._bbox_east, self._bbox_north = initial_bbox
        self._on_change = on_change
        self._map_width = width
        self._map_height = height
        
        # Drag state
        self._dragging = False
        self._drag_handle: Optional[str] = None  # 'nw', 'ne', 'sw', 'se', 'move'
        self._drag_start_x = 0
        self._drag_start_y = 0
        self._drag_start_bbox = (0, 0, 0, 0)
        
        self._build()
    
    def _build(self):
        """Build the map component."""
        # Canvas for drawing
        self._canvas = cv.Canvas(
            shapes=self._get_shapes(),
            width=self._map_width,
            height=self._map_height,
        )
        
        # Gesture detector for mouse interactions
        self._gesture = ft.GestureDetector(
            content=self._canvas,
            on_pan_start=self._on_pan_start,
            on_pan_update=self._on_pan_update,
            on_pan_end=self._on_pan_end,
        )
        
        # Coordinate display
        self._coord_text = ft.Text(
            self._format_coords(),
            size=12,
            color=Colors.ON_SURFACE_VARIANT,
            text_align=ft.TextAlign.CENTER,
        )
        
        self.content = ft.Column(
            controls=[
                ft.Container(
                    content=self._gesture,
                    bgcolor="#1a2744",  # Ocean blue
                    border_radius=8,
                    clip_behavior=ft.ClipBehavior.HARD_EDGE,
                ),
                ft.Container(height=8),
                self._coord_text,
                ft.Text(
                    "Drag corners to resize | Drag center to move",
                    size=11,
                    color=Colors.ON_SURFACE_VARIANT,
                    italic=True,
                    text_align=ft.TextAlign.CENTER,
                ),
            ],
            horizontal_alignment=ft.CrossAxisAlignment.CENTER,
            spacing=4,
        )
    
    def _get_shapes(self) -> list:
        """Generate canvas shapes for the map and bbox."""
        shapes = []
        
        # Background fill (ocean) - already done via container bgcolor
        
        # Draw simplified land masses as rectangles (for CONUS approximation)
        shapes.extend(self._draw_land())
        
        # Draw the selection bbox
        shapes.extend(self._draw_bbox())
        
        return shapes
    
    def _draw_land(self) -> list:
        """Draw simplified land representation."""
        shapes = []
        
        # Draw a simplified representation of continental US
        # Using rectangles to approximate major regions
        land_paint = ft.Paint(color="#2d4a3e", style=ft.PaintingStyle.FILL)
        border_paint = ft.Paint(color="#4a6b5a", style=ft.PaintingStyle.STROKE, stroke_width=1)
        
        # Main CONUS landmass (simplified rectangle)
        x1, y1 = self._geo_to_pixel(-125, 49)
        x2, y2 = self._geo_to_pixel(-67, 25)
        shapes.append(cv.Rect(x=x1, y=y1, width=x2-x1, height=y2-y1, paint=land_paint))
        shapes.append(cv.Rect(x=x1, y=y1, width=x2-x1, height=y2-y1, paint=border_paint))
        
        # Florida peninsula
        fx1, fy1 = self._geo_to_pixel(-88, 31)
        fx2, fy2 = self._geo_to_pixel(-80, 24)
        shapes.append(cv.Rect(x=fx1, y=fy1, width=fx2-fx1, height=fy2-fy1, paint=land_paint))
        
        # Texas extension
        tx1, ty1 = self._geo_to_pixel(-106, 32)
        tx2, ty2 = self._geo_to_pixel(-93, 26)
        shapes.append(cv.Rect(x=tx1, y=ty1, width=tx2-tx1, height=ty2-ty1, paint=land_paint))
        
        # Canada (simplified)
        cx1, cy1 = self._geo_to_pixel(-130, 55)
        cx2, cy2 = self._geo_to_pixel(-60, 49)
        shapes.append(cv.Rect(x=cx1, y=cy1, width=cx2-cx1, height=cy2-cy1, paint=land_paint))
        shapes.append(cv.Rect(x=cx1, y=cy1, width=cx2-cx1, height=cy2-cy1, paint=border_paint))
        
        # Mexico (partial)
        mx1, my1 = self._geo_to_pixel(-118, 25)
        mx2, my2 = self._geo_to_pixel(-86, 20)
        shapes.append(cv.Rect(x=mx1, y=my1, width=mx2-mx1, height=my2-my1, paint=land_paint))
        
        # Add grid lines
        grid_paint = ft.Paint(color="#ffffff15", style=ft.PaintingStyle.STROKE, stroke_width=0.5)
        
        # Longitude lines every 10 degrees
        for lon in range(-130, -59, 10):
            x1, y1 = self._geo_to_pixel(lon, self._map_bounds.south)
            x2, y2 = self._geo_to_pixel(lon, self._map_bounds.north)
            shapes.append(cv.Line(x1, y1, x2, y2, paint=grid_paint))
        
        # Latitude lines every 10 degrees
        for lat in range(20, 56, 10):
            x1, y1 = self._geo_to_pixel(self._map_bounds.west, lat)
            x2, y2 = self._geo_to_pixel(self._map_bounds.east, lat)
            shapes.append(cv.Line(x1, y1, x2, y2, paint=grid_paint))
        
        # Add some city markers for reference
        cities = [
            (-118.24, 34.05, "LA"),
            (-122.42, 37.77, "SF"),
            (-73.94, 40.67, "NY"),
            (-87.63, 41.88, "CHI"),
            (-95.37, 29.76, "HOU"),
        ]
        
        marker_paint = ft.Paint(color="#ffffff80")
        for lon, lat, name in cities:
            x, y = self._geo_to_pixel(lon, lat)
            shapes.append(cv.Circle(x, y, 3, paint=marker_paint))
        
        return shapes
    
    def _draw_bbox(self) -> list:
        """Draw the selection bounding box with handles."""
        shapes = []
        
        # Get pixel coordinates
        x1, y1 = self._geo_to_pixel(self._bbox_west, self._bbox_north)
        x2, y2 = self._geo_to_pixel(self._bbox_east, self._bbox_south)
        
        # Selection rectangle fill
        shapes.append(
            cv.Rect(
                x=x1, y=y1,
                width=x2 - x1,
                height=y2 - y1,
                paint=ft.Paint(color="#7C4DFF40"),  # Semi-transparent purple
            )
        )
        
        # Selection rectangle border
        shapes.append(
            cv.Rect(
                x=x1, y=y1,
                width=x2 - x1,
                height=y2 - y1,
                paint=ft.Paint(
                    color=Colors.PRIMARY,
                    style=ft.PaintingStyle.STROKE,
                    stroke_width=2,
                ),
            )
        )
        
        # Corner handles
        handle_size = 8
        handle_paint = ft.Paint(color=Colors.PRIMARY, style=ft.PaintingStyle.FILL)
        handle_border = ft.Paint(color="#ffffff", style=ft.PaintingStyle.STROKE, stroke_width=1)
        
        corners = [
            (x1, y1),  # NW
            (x2, y1),  # NE
            (x1, y2),  # SW
            (x2, y2),  # SE
        ]
        
        for cx, cy in corners:
            shapes.append(
                cv.Rect(
                    x=cx - handle_size/2,
                    y=cy - handle_size/2,
                    width=handle_size,
                    height=handle_size,
                    paint=handle_paint,
                )
            )
            shapes.append(
                cv.Rect(
                    x=cx - handle_size/2,
                    y=cy - handle_size/2,
                    width=handle_size,
                    height=handle_size,
                    paint=handle_border,
                )
            )
        
        return shapes
    
    def _geo_to_pixel(self, lon: float, lat: float) -> tuple[float, float]:
        """Convert geographic coordinates to pixel coordinates."""
        x = ((lon - self._map_bounds.west) / self._map_bounds.width) * self._map_width
        y = ((self._map_bounds.north - lat) / self._map_bounds.height) * self._map_height
        return x, y
    
    def _pixel_to_geo(self, x: float, y: float) -> tuple[float, float]:
        """Convert pixel coordinates to geographic coordinates."""
        lon = (x / self._map_width) * self._map_bounds.width + self._map_bounds.west
        lat = self._map_bounds.north - (y / self._map_height) * self._map_bounds.height
        return lon, lat
    
    def _get_handle_at(self, x: float, y: float) -> Optional[str]:
        """Determine which handle (if any) is at the given pixel position."""
        handle_radius = 12  # Click tolerance
        
        x1, y1 = self._geo_to_pixel(self._bbox_west, self._bbox_north)
        x2, y2 = self._geo_to_pixel(self._bbox_east, self._bbox_south)
        
        # Check corners
        if abs(x - x1) < handle_radius and abs(y - y1) < handle_radius:
            return 'nw'
        if abs(x - x2) < handle_radius and abs(y - y1) < handle_radius:
            return 'ne'
        if abs(x - x1) < handle_radius and abs(y - y2) < handle_radius:
            return 'sw'
        if abs(x - x2) < handle_radius and abs(y - y2) < handle_radius:
            return 'se'
        
        # Check if inside bbox (for move)
        if x1 < x < x2 and y1 < y < y2:
            return 'move'
        
        return None
    
    def _on_pan_start(self, e: ft.DragStartEvent):
        """Handle drag start."""
        # In Flet 0.80+, use e.x and e.y for position
        x = getattr(e, 'local_x', None) or getattr(e, 'x', 0)
        y = getattr(e, 'local_y', None) or getattr(e, 'y', 0)
        
        handle = self._get_handle_at(x, y)
        if handle:
            self._dragging = True
            self._drag_handle = handle
            self._drag_start_x = x
            self._drag_start_y = y
            self._drag_start_bbox = (
                self._bbox_west, self._bbox_south,
                self._bbox_east, self._bbox_north
            )
    
    def _on_pan_update(self, e: ft.DragUpdateEvent):
        """Handle drag movement."""
        if not self._dragging:
            return
        
        # In Flet 0.80+, use e.x and e.y for position
        x = getattr(e, 'local_x', None) or getattr(e, 'x', 0)
        y = getattr(e, 'local_y', None) or getattr(e, 'y', 0)
        
        # Calculate delta in geographic coordinates
        start_lon, start_lat = self._pixel_to_geo(self._drag_start_x, self._drag_start_y)
        current_lon, current_lat = self._pixel_to_geo(x, y)
        delta_lon = current_lon - start_lon
        delta_lat = current_lat - start_lat
        
        w, s, e_coord, n = self._drag_start_bbox
        
        if self._drag_handle == 'move':
            self._bbox_west = w + delta_lon
            self._bbox_east = e_coord + delta_lon
            self._bbox_south = s + delta_lat
            self._bbox_north = n + delta_lat
        elif self._drag_handle == 'nw':
            self._bbox_west = w + delta_lon
            self._bbox_north = n + delta_lat
        elif self._drag_handle == 'ne':
            self._bbox_east = e_coord + delta_lon
            self._bbox_north = n + delta_lat
        elif self._drag_handle == 'sw':
            self._bbox_west = w + delta_lon
            self._bbox_south = s + delta_lat
        elif self._drag_handle == 'se':
            self._bbox_east = e_coord + delta_lon
            self._bbox_south = s + delta_lat
        
        # Ensure valid bbox
        self._clamp_bbox()
        
        # Update display
        self._canvas.shapes = self._get_shapes()
        self._coord_text.value = self._format_coords()
        self.update()
        
        # Notify callback
        if self._on_change:
            self._on_change(
                self._bbox_west, self._bbox_south,
                self._bbox_east, self._bbox_north
            )
    
    def _on_pan_end(self, e: ft.DragEndEvent):
        """Handle drag end."""
        self._dragging = False
        self._drag_handle = None
    
    def _clamp_bbox(self):
        """Ensure bbox is valid and within map bounds."""
        # Ensure west < east and south < north
        if self._bbox_west > self._bbox_east:
            self._bbox_west, self._bbox_east = self._bbox_east, self._bbox_west
        if self._bbox_south > self._bbox_north:
            self._bbox_south, self._bbox_north = self._bbox_north, self._bbox_south
        
        # Clamp to map bounds
        self._bbox_west = max(self._map_bounds.west, min(self._bbox_west, self._map_bounds.east - 1))
        self._bbox_east = max(self._map_bounds.west + 1, min(self._bbox_east, self._map_bounds.east))
        self._bbox_south = max(self._map_bounds.south, min(self._bbox_south, self._map_bounds.north - 1))
        self._bbox_north = max(self._map_bounds.south + 1, min(self._bbox_north, self._map_bounds.north))
    
    def _format_coords(self) -> str:
        """Format coordinates for display."""
        return (
            f"W: {self._bbox_west:.2f}  |  E: {self._bbox_east:.2f}  |  "
            f"S: {self._bbox_south:.2f}  |  N: {self._bbox_north:.2f}"
        )
    
    def get_bbox(self) -> tuple[float, float, float, float]:
        """Get the current bounding box as (west, south, east, north)."""
        return (self._bbox_west, self._bbox_south, self._bbox_east, self._bbox_north)
    
    def set_bbox(self, west: float, south: float, east: float, north: float):
        """Set the bounding box programmatically."""
        self._bbox_west = west
        self._bbox_south = south
        self._bbox_east = east
        self._bbox_north = north
        self._clamp_bbox()
        self._canvas.shapes = self._get_shapes()
        self._coord_text.value = self._format_coords()
        self.update()


========================================
FILE: src/tempo_app/ui/components/node_widgets.py
========================================
"""UI widgets for the Node-Based Pipeline Editor."""

import flet as ft
from typing import Callable, Optional, Dict, Any, List

from ..theme import Colors, Spacing


class NodeCard(ft.Container):
    """Visual card representing a single node in the pipeline stack."""
    
    # Icon mapping for node types
    NODE_ICONS = {
        "select_variable": ft.Icons.DATA_OBJECT,
        "nearest_pixel": ft.Icons.LOCATION_ON,
        "n_pixel_avg": ft.Icons.GRID_ON,
        "radius_avg": ft.Icons.RADIO_BUTTON_CHECKED,
        "hourly": ft.Icons.ACCESS_TIME,
        "daily_mean": ft.Icons.TODAY,
        "diurnal_cycle": ft.Icons.LOOP,
        "gap_fill": ft.Icons.AUTO_FIX_HIGH,
        "filter": ft.Icons.FILTER_ALT,
        "statistics": ft.Icons.ANALYTICS,
    }
    
    # Color mapping for categories
    CATEGORY_COLORS = {
        "source": Colors.INFO,
        "spatial": Colors.SUCCESS,
        "temporal": Colors.WARNING,
        "transform": Colors.PRIMARY,
    }
    
    def __init__(
        self,
        node_type: str,
        display_name: str,
        category: str = "base",
        params: Dict[str, Any] = None,
        on_edit: Callable = None,
        on_delete: Callable = None,
        index: int = 0,
    ):
        self.node_type = node_type
        self.display_name = display_name
        self.category = category
        self.params = params or {}
        self.on_edit = on_edit
        self.on_delete = on_delete
        self.index = index
        
        super().__init__(
            content=self._build_content(),
            bgcolor=Colors.SURFACE,
            border_radius=8,
            border=ft.border.all(1, self.CATEGORY_COLORS.get(category, Colors.BORDER)),
            padding=12,
        )
    
    def _build_content(self):
        """Build the card content."""
        icon = self.NODE_ICONS.get(self.node_type, ft.Icons.SETTINGS)
        color = self.CATEGORY_COLORS.get(self.category, Colors.PRIMARY)
        
        # Parameter summary
        param_text = ""
        if self.params:
            param_parts = [f"{k}={v}" for k, v in self.params.items() if not k.startswith('_')]
            param_text = ", ".join(param_parts[:3])  # Limit to 3 params
        
        return ft.Row([
            ft.Container(
                content=ft.Icon(icon, size=20, color=color),
                bgcolor=f"{color}20",  # 20% opacity
                padding=8,
                border_radius=6,
            ),
            ft.Column([
                ft.Text(self.display_name, size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                ft.Text(param_text or "Default settings", size=11, color=Colors.ON_SURFACE_VARIANT),
            ], spacing=2, expand=True),
            ft.Row([
                ft.IconButton(
                    icon=ft.Icons.EDIT,
                    icon_size=16,
                    icon_color=Colors.ON_SURFACE_VARIANT,
                    tooltip="Edit",
                    on_click=lambda e: self.on_edit(self.index) if self.on_edit else None,
                ),
                ft.IconButton(
                    icon=ft.Icons.DELETE_OUTLINE,
                    icon_size=16,
                    icon_color=Colors.ERROR,
                    tooltip="Remove",
                    on_click=lambda e: self.on_delete(self.index) if self.on_delete else None,
                ),
            ], spacing=0),
        ], spacing=12)


class AddNodeButton(ft.Container):
    """Button to add a new node to the stack."""
    
    def __init__(self, on_add: Callable = None):
        self.on_add = on_add
        
        super().__init__(
            content=ft.Row([
                ft.Icon(ft.Icons.ADD, size=16, color=Colors.PRIMARY),
                ft.Text("Add Step", size=12, color=Colors.PRIMARY),
            ], spacing=6, alignment=ft.MainAxisAlignment.CENTER),
            bgcolor=f"{Colors.PRIMARY}10",
            border_radius=8,
            border=ft.border.all(1, f"{Colors.PRIMARY}50"),
            padding=ft.padding.symmetric(horizontal=16, vertical=8),
            on_click=self._show_menu,
        )
    
    def _show_menu(self, e):
        """Show node type selection menu."""
        if not self.on_add:
            return
        
        # Create menu items for each node category
        menu_items = [
            ("Source", [
                ("select_variable", "Select Variable"),
            ]),
            ("Spatial", [
                ("nearest_pixel", "Nearest Pixel"),
                ("n_pixel_avg", "N-Pixel Average"),
                ("radius_avg", "Radius Average"),
            ]),
            ("Temporal", [
                ("hourly", "Hourly (Raw)"),
                ("daily_mean", "Daily Mean"),
                ("diurnal_cycle", "Diurnal Cycle"),
            ]),
            ("Transform", [
                ("gap_fill", "Gap Fill"),
                ("filter", "Filter"),
            ]),
        ]
        
        # Build menu
        menu_controls = []
        for category, items in menu_items:
            menu_controls.append(
                ft.Text(category, size=11, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE_VARIANT)
            )
            for node_type, name in items:
                menu_controls.append(
                    ft.TextButton(
                        content=ft.Text(name),
                        on_click=lambda e, nt=node_type: self._add_node(nt),
                    )
                )
            menu_controls.append(ft.Divider(height=8))
        
        # Show as bottom sheet
        bs = ft.BottomSheet(
            content=ft.Container(
                content=ft.Column(menu_controls, spacing=4, scroll=ft.ScrollMode.AUTO),
                padding=16,
            ),
            open=True,
        )
        self.page.overlay.append(bs)
        self.page.update()
    
    def _add_node(self, node_type: str):
        """Add the selected node type."""
        # Close bottom sheet
        if self.page.overlay:
            self.page.overlay.pop()
            self.page.update()
        
        if self.on_add:
            self.on_add(node_type)


class NodeParamEditor(ft.AlertDialog):
    """Dialog for editing node parameters."""
    
    def __init__(self, node_type: str, params: Dict[str, Any], on_save: Callable):
        self.node_type = node_type
        self.params = params.copy()
        self.on_save = on_save
        self._fields = {}
        
        super().__init__(
            title=ft.Text(f"Edit: {node_type}"),
            content=self._build_form(),
            actions=[
                ft.TextButton("Cancel", on_click=lambda e: self._close()),
                ft.FilledButton("Save", on_click=lambda e: self._save()),
            ],
        )
    
    def _build_form(self):
        """Build parameter edit form based on node type."""
        controls = []
        
        # Common parameters by node type
        param_defs = {
            "select_variable": [
                ("variable", "Variable", ["NO2_TropVCD", "HCHO_TotVCD", "FNR"]),
            ],
            "n_pixel_avg": [
                ("n_pixels", "Number of Pixels", [1, 4, 9, 16]),
            ],
            "radius_avg": [
                ("radius_km", "Radius (km)", None),
                ("min_coverage", "Min Coverage (%)", None),
            ],
            "daily_mean": [
                ("min_hours", "Min Hours Required", None),
            ],
            "filter": [
                ("column", "Column", ["Value"]),
                ("operator", "Operator", [">", ">=", "<", "<=", "==", "!="]),
                ("threshold", "Threshold", None),
            ],
        }
        
        if self.node_type in param_defs:
            for param_name, label, options in param_defs[self.node_type]:
                current_value = self.params.get(param_name, "")
                
                if options:
                    # Dropdown
                    field = ft.Dropdown(
                        label=label,
                        options=[ft.DropdownOption(str(o)) for o in options],
                        value=str(current_value) if current_value else str(options[0]),
                        text_style=ft.TextStyle(color=Colors.ON_SURFACE),
                    )
                else:
                    # Text field
                    field = ft.TextField(
                        label=label,
                        value=str(current_value),
                        text_style=ft.TextStyle(color=Colors.ON_SURFACE),
                    )
                
                self._fields[param_name] = field
                controls.append(field)
        else:
            controls.append(ft.Text("No parameters to edit", color=Colors.ON_SURFACE_VARIANT))
        
        return ft.Container(
            content=ft.Column(controls, spacing=16),
            width=300,
            padding=16,
        )
    
    def _save(self):
        """Save parameters and close."""
        for name, field in self._fields.items():
            value = field.value
            # Try to convert to appropriate type
            try:
                if '.' in str(value):
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                pass
            self.params[name] = value
        
        self.on_save(self.params)
        self._close()
    
    def _close(self):
        """Close the dialog."""
        self.open = False
        if self.page and self in self.page.overlay:
            self.page.overlay.remove(self)
        if self.page:
            self.page.update()


========================================
FILE: src/tempo_app/ui/components/stack_editor.py
========================================
"""Stack Editor - Vertical list of nodes for a single column."""

import flet as ft
from typing import Callable, List, Dict, Any

from ..theme import Colors
from .node_widgets import NodeCard, AddNodeButton, NodeParamEditor
from ...core.nodes import NodeConfig, get_node_class


# Node metadata lookup
NODE_INFO = {
    "select_variable": ("Select Variable", "source"),
    "nearest_pixel": ("Nearest Pixel", "spatial"),
    "n_pixel_avg": ("N-Pixel Average", "spatial"),
    "radius_avg": ("Radius Average", "spatial"),
    "hourly": ("Hourly (Raw)", "temporal"),
    "daily_mean": ("Daily Mean", "temporal"),
    "diurnal_cycle": ("Diurnal Cycle", "temporal"),
    "gap_fill": ("Gap Fill", "transform"),
    "filter": ("Filter", "transform"),
    "statistics": ("Add Statistics", "transform"),
}


class StackEditor(ft.Container):
    """Vertical stack editor for a column's pipeline."""
    
    def __init__(
        self,
        node_configs: List[NodeConfig] = None,
        on_change: Callable = None,
    ):
        self._configs = node_configs or []
        self.on_change = on_change
        self._node_cards = []
        
        super().__init__(
            content=self._build_content(),
            bgcolor=Colors.BACKGROUND,
            border_radius=8,
            padding=8,
        )
    
    def _build_content(self):
        """Build the stack UI."""
        self._node_cards = []
        controls = []
        
        for i, config in enumerate(self._configs):
            info = NODE_INFO.get(config.node_type, (config.node_type, "base"))
            card = NodeCard(
                node_type=config.node_type,
                display_name=info[0],
                category=info[1],
                params=config.params,
                on_edit=self._edit_node,
                on_delete=self._delete_node,
                index=i,
            )
            self._node_cards.append(card)
            controls.append(card)
            
            # Add connector line between nodes
            if i < len(self._configs) - 1:
                controls.append(
                    ft.Container(
                        content=ft.Icon(ft.Icons.ARROW_DOWNWARD, size=16, color=Colors.BORDER),
                        alignment=ft.Alignment(0, 0),
                        height=24,
                    )
                )
        
        # Add button at the end
        controls.append(ft.Container(height=8))
        controls.append(AddNodeButton(on_add=self._add_node))
        
        return ft.Column(controls, spacing=4, horizontal_alignment=ft.CrossAxisAlignment.CENTER)
    
    def _add_node(self, node_type: str):
        """Add a new node to the stack."""
        # Default params
        default_params = {}
        if node_type == "select_variable":
            default_params = {"variable": "NO2_TropVCD"}
        elif node_type == "n_pixel_avg":
            default_params = {"n_pixels": 4}
        elif node_type == "radius_avg":
            default_params = {"radius_km": 10.0, "min_coverage": 0.5}
        elif node_type == "daily_mean":
            default_params = {"min_hours": 1}
        
        self._configs.append(NodeConfig(node_type, default_params))
        self._refresh()
        
        if self.on_change:
            self.on_change(self._configs)
    
    def _edit_node(self, index: int):
        """Open editor for a node."""
        if not self.page:
            return
            
        if 0 <= index < len(self._configs):
            config = self._configs[index]
            
            def save_params(params):
                self._configs[index] = NodeConfig(config.node_type, params)
                self._refresh()
                if self.on_change:
                    self.on_change(self._configs)
            
            dlg = NodeParamEditor(
                node_type=config.node_type,
                params=config.params,
                on_save=save_params,
            )
            self.page.overlay.append(dlg)
            dlg.open = True
            self.page.update()
    
    def _delete_node(self, index: int):
        """Remove a node from the stack."""
        if 0 <= index < len(self._configs):
            self._configs.pop(index)
            self._refresh()
            
            if self.on_change:
                self.on_change(self._configs)
    
    def _refresh(self):
        """Rebuild the UI."""
        self.content = self._build_content()
        self.update()
    
    @property
    def configs(self) -> List[NodeConfig]:
        """Get current node configurations."""
        return self._configs.copy()
    
    @configs.setter
    def configs(self, value: List[NodeConfig]):
        """Set node configurations."""
        self._configs = list(value)
        self._refresh()


========================================
FILE: src/tempo_app/ui/components/widgets.py
========================================
"""Reusable UI components for TEMPO Analyzer."""

import flet as ft
from ..theme import Colors, Spacing


class HelpTooltip(ft.IconButton):
    """A help icon (?) that shows help on hover or click."""
    
    def __init__(self, help_text: str, icon_size: int = 18):
        self._help_text = help_text
        super().__init__(
            icon=ft.Icons.HELP_OUTLINE,
            icon_size=icon_size,
            icon_color=Colors.ON_SURFACE_VARIANT,
            tooltip=help_text,
            style=ft.ButtonStyle(padding=0),
            on_click=self._show_help,
        )
    
    def _show_help(self, e):
        """Show help dialog on click."""
        def close_dlg(e):
            dlg.open = False
            self.page.update()
        
        dlg = ft.AlertDialog(
            modal=True,
            title=ft.Text("Help"),
            content=ft.Text(self._help_text, size=14),
            actions=[ft.TextButton("OK", on_click=close_dlg)],
        )
        self.page.overlay.append(dlg)
        dlg.open = True
        self.page.update()


class LabeledField(ft.Column):
    """A form field with a label and optional help tooltip.
    
    Usage:
        LabeledField(
            label="Hour Range",
            help_text="Select the UTC hours to download...",
            field=ft.Dropdown(...)
        )
    """
    
    def __init__(
        self,
        label: str,
        field: ft.Control,
        help_text: str = None,
        required: bool = False,
    ):
        label_row = ft.Row(
            controls=[
                ft.Text(
                    label + ("*" if required else ""),
                    size=14,
                    weight=ft.FontWeight.W_500,
                    color=Colors.ON_SURFACE,
                ),
            ],
            spacing=6,
        )
        
        if help_text:
            label_row.controls.append(HelpTooltip(help_text))
        
        super().__init__(
            controls=[label_row, field],
            spacing=6,
        )


class SectionCard(ft.Container):
    """A card container for grouping related form fields.
    
    Usage:
        SectionCard(
            title="Geographic Region",
            icon=ft.Icons.MAP,
            help_text="Define the area to download data for...",
            content=ft.Column([...])
        )
    """
    
    def __init__(
        self,
        title: str,
        content: ft.Control,
        icon: str = None,
        help_text: str = None,
        collapsed: bool = False,
    ):
        self._collapsed = collapsed
        self._content = content
        
        # Header row
        header_controls = []
        if icon:
            header_controls.append(ft.Icon(icon, size=20, color=Colors.PRIMARY))
        
        header_controls.append(
            ft.Text(
                title,
                size=16,
                weight=ft.FontWeight.W_600,
                color=Colors.ON_SURFACE,
            )
        )
        
        if help_text:
            header_controls.append(HelpTooltip(help_text))
        
        header_controls.append(ft.Container(expand=True))  # Spacer
        
        # Collapse toggle
        self._collapse_btn = ft.IconButton(
            icon=ft.Icons.EXPAND_LESS if not collapsed else ft.Icons.EXPAND_MORE,
            icon_size=20,
            icon_color=Colors.ON_SURFACE_VARIANT,
            on_click=self._toggle_collapse,
            tooltip="Collapse/Expand",
        )
        header_controls.append(self._collapse_btn)
        
        header = ft.Row(controls=header_controls, spacing=8)
        
        # Content wrapper
        self._content_wrapper = ft.Container(
            content=content,
            visible=not collapsed,
            padding=ft.padding.only(top=12),
        )
        
        super().__init__(
            content=ft.Column(
                controls=[header, self._content_wrapper],
                spacing=0,
            ),
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.Border(
                left=ft.BorderSide(1, Colors.BORDER),
                top=ft.BorderSide(1, Colors.BORDER),
                right=ft.BorderSide(1, Colors.BORDER),
                bottom=ft.BorderSide(1, Colors.BORDER),
            ),
            padding=16,
        )
    
    def _toggle_collapse(self, e):
        self._collapsed = not self._collapsed
        self._content_wrapper.visible = not self._collapsed
        self._collapse_btn.icon = (
            ft.Icons.EXPAND_LESS if not self._collapsed else ft.Icons.EXPAND_MORE
        )
        self.update()


class StatusLogPanel(ft.Container):
    """A panel showing real-time activity log with status updates.
    
    Usage:
        log = StatusLogPanel()
        log.add_info("Starting download...")
        log.add_success("Download complete!")
    """
    
    def __init__(self, max_entries: int = 50):
        self._entries: list[ft.Control] = []
        self._max_entries = max_entries
        
        self._log_column = ft.Column(
            controls=[],
            spacing=4,
            scroll=ft.ScrollMode.AUTO,
            auto_scroll=True,
        )
        
        super().__init__(
            content=ft.Column(
                controls=[
                    ft.Row(
                        controls=[
                            ft.Icon(ft.Icons.TERMINAL, size=18, color=Colors.ON_SURFACE_VARIANT),
                            ft.Text(
                                "Activity Log",
                                size=14,
                                weight=ft.FontWeight.W_500,
                                color=Colors.ON_SURFACE,
                            ),
                            HelpTooltip(
                                "Real-time updates showing what the application is doing. "
                                "Each line is timestamped so you can track progress. "
                                "Check mark = completed, Hourglass = waiting, X = error."
                            ),
                            ft.Container(expand=True),
                            ft.IconButton(
                                icon=ft.Icons.DELETE_SWEEP,
                                icon_size=18,
                                icon_color=Colors.ON_SURFACE_VARIANT,
                                tooltip="Clear log",
                                on_click=self._clear_log,
                            ),
                        ],
                        spacing=6,
                    ),
                    ft.Container(
                        content=self._log_column,
                        bgcolor=Colors.BACKGROUND,
                        border_radius=8,
                        padding=12,
                        expand=True,
                        height=200,
                    ),
                ],
                spacing=8,
            ),
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.Border(
                left=ft.BorderSide(1, Colors.BORDER),
                top=ft.BorderSide(1, Colors.BORDER),
                right=ft.BorderSide(1, Colors.BORDER),
                bottom=ft.BorderSide(1, Colors.BORDER),
            ),
            padding=16,
            expand=True,
        )
    
    def _add_entry(self, icon: str, message: str, color: str):
        """Add an entry to the log."""
        from datetime import datetime
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        entry = ft.Row(
            controls=[
                ft.Text(timestamp, size=11, color=Colors.ON_SURFACE_VARIANT, font_family="monospace"),
                ft.Text(icon, size=12),
                ft.Text(message, size=12, color=color, expand=True),
            ],
            spacing=8,
        )
        
        self._entries.append(entry)
        if len(self._entries) > self._max_entries:
            self._entries = self._entries[-self._max_entries:]
        
        self._log_column.controls = self._entries.copy()
        self.update()
    
    def add_info(self, message: str):
        """Add an info message."""
        self._add_entry("[>]", message, Colors.ON_SURFACE_VARIANT)
    
    def add_success(self, message: str):
        """Add a success message."""
        self._add_entry("[OK]", message, Colors.SUCCESS)
    
    def add_warning(self, message: str):
        """Add a warning message."""
        self._add_entry("[!]", message, Colors.WARNING)
    
    def add_error(self, message: str):
        """Add an error message."""
        self._add_entry("[X]", message, Colors.ERROR)
    
    def add_progress(self, message: str):
        """Add a progress/waiting message."""
        self._add_entry("[...]", message, Colors.INFO)
    
    def _clear_log(self, e):
        """Clear all log entries."""
        self._entries.clear()
        self._log_column.controls.clear()
        self.update()

    def get_handler(self):
        """Get a logging handler that outputs to this panel."""
        return StatusLogHandler(self)


class StatusLogHandler:
    """A logging handler that outputs to a StatusLogPanel."""
    # This is a dummy class definition that will be replaced by the actual inheritance
    # But since we can't import logging at the top level without potential circular imports or
    # messing up the file, we'll do it inside.
    # update: Actually standard library imports are fine.
    pass

import logging

class StatusLogHandler(logging.Handler):
    """A logging handler that outputs to a StatusLogPanel."""
    
    def __init__(self, panel: StatusLogPanel):
        super().__init__()
        self.panel = panel
        self.setFormatter(logging.Formatter('%(message)s'))
        
    def emit(self, record):
        try:
            msg = self.format(record)
            # Map log levels to panel methods
            if record.levelno >= logging.ERROR:
                self.panel.add_error(msg)
            elif record.levelno >= logging.WARNING:
                self.panel.add_warning(msg)
            elif record.levelno >= logging.INFO:
                self.panel.add_info(msg)
            else:
                self.panel.add_info(msg) # DEBUG etc
        except Exception:
            self.handleError(record)


class ProgressPanel(ft.Container):
    """A progress panel showing overall download progress.
    
    Shows:
    - Progress bar
    - Current step description
    - Estimated time remaining
    """
    
    def __init__(self):
        self._progress_bar = ft.ProgressBar(
            value=0,
            color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
        )
        
        self._status_text = ft.Text(
            "Ready to start",
            size=14,
            color=Colors.ON_SURFACE,
        )
        
        self._detail_text = ft.Text(
            "",
            size=12,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        self._eta_text = ft.Text(
            "",
            size=12,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        super().__init__(
            content=ft.Column(
                controls=[
                    self._status_text,
                    self._progress_bar,
                    ft.Row(
                        controls=[
                            self._detail_text,
                            ft.Container(expand=True),
                            self._eta_text,
                        ],
                    ),
                ],
                spacing=8,
            ),
            visible=False,
            padding=ft.padding.only(top=16),
        )
    
    def show(self):
        """Show the progress panel."""
        self.visible = True
        self.update()
    
    def hide(self):
        """Hide the progress panel."""
        self.visible = False
        self.update()
    
    def update_progress(
        self,
        progress: float,
        status: str,
        detail: str = "",
        eta_seconds: float = None,
    ):
        """Update the progress display."""
        self._progress_bar.value = progress
        self._status_text.value = status
        self._detail_text.value = detail
        
        if eta_seconds is not None:
            if eta_seconds < 60:
                self._eta_text.value = f"~{int(eta_seconds)}s remaining"
            else:
                self._eta_text.value = f"~{int(eta_seconds / 60)}m remaining"
        else:
            self._eta_text.value = ""
        
        self.update()


class WorkerProgressPanel(ft.Container):
    """Multi-worker progress panel showing individual bars per download worker.
    
    Shows:
    - Overall progress bar
    - Individual worker status rows
    - Completed/Total count
    """
    
    def __init__(self, num_workers: int = 4):
        self._num_workers = num_workers
        self._worker_rows: list[ft.Container] = []
        
        # Overall progress
        self._overall_bar = ft.ProgressBar(
            value=0,
            color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
        )
        
        self._overall_text = ft.Text(
            "Ready to download",
            size=14,
            weight=ft.FontWeight.W_500,
            color=Colors.ON_SURFACE,
        )
        
        self._count_text = ft.Text(
            "0/0",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        # Create worker rows
        for i in range(num_workers):
            worker_bar = ft.ProgressBar(
                value=None,  # Indeterminate
                color=Colors.INFO,
                bgcolor=Colors.SURFACE_VARIANT,
                width=200,
            )
            worker_label = ft.Text(
                f"Worker {i+1}: Idle",
                size=12,
                color=Colors.ON_SURFACE_VARIANT,
                width=250,
            )
            row = ft.Container(
                content=ft.Row([
                    worker_label,
                    worker_bar,
                ], spacing=12),
                visible=False,
            )
            self._worker_rows.append({
                "container": row,
                "label": worker_label,
                "bar": worker_bar,
            })
        
        worker_column = ft.Column(
            controls=[r["container"] for r in self._worker_rows],
            spacing=6,
        )
        
        super().__init__(
            content=ft.Column(
                controls=[
                    ft.Row([
                        ft.Icon(ft.Icons.CLOUD_DOWNLOAD, size=18, color=Colors.PRIMARY),
                        ft.Text("Download Progress", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                        ft.Container(expand=True),
                        self._count_text,
                    ], spacing=8),
                    self._overall_text,
                    self._overall_bar,
                    ft.Container(height=8),
                    worker_column,
                ],
                spacing=6,
            ),
            visible=False,
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.Border.all(1, Colors.BORDER),
            padding=16,
        )
    
    def show(self, total: int = 0):
        """Show the panel and reset state."""
        self.visible = True
        self._overall_bar.value = 0
        self._overall_text.value = "Starting download..."
        self._count_text.value = f"0/{total}"
        # Show worker rows
        for r in self._worker_rows:
            r["container"].visible = True
            r["label"].value = "Idle"
            r["bar"].value = None  # Indeterminate
        self.update()
    
    def hide(self):
        """Hide the panel."""
        self.visible = False
        for r in self._worker_rows:
            r["container"].visible = False
        self.update()
    
    def update_overall(self, completed: int, total: int, status: str = ""):
        """Update overall progress."""
        progress = completed / total if total > 0 else 0
        self._overall_bar.value = progress
        self._count_text.value = f"{completed}/{total}"
        if status:
            self._overall_text.value = status
        self.update()
    
    def update_worker(self, worker_id: int, status: str, active: bool = True):
        """Update a specific worker's status."""
        if 0 <= worker_id < len(self._worker_rows):
            row = self._worker_rows[worker_id]
            row["label"].value = f"W{worker_id+1}: {status}"
            if active:
                row["bar"].value = None  # Indeterminate = active
                row["label"].color = Colors.ON_SURFACE
            else:
                row["bar"].value = 1.0  # Full = done
                row["label"].color = Colors.ON_SURFACE_VARIANT
            self.update()
    
    def complete(self, success_count: int, total: int):
        """Mark download as complete."""
        self._overall_bar.value = 1.0
        self._overall_text.value = f"‚úÖ Downloaded {success_count}/{total} granules"
        self._overall_text.color = Colors.SUCCESS
        for r in self._worker_rows:
            r["container"].visible = False
        self.update()


class DaySelector(ft.Row):
    """Toggle chips for selecting days of the week.
    
    Usage:
        days = DaySelector(value=[0,1,2,3,4])  # Mon-Fri selected
        selected = days.value  # Returns list of indices
    """
    
    DAY_LABELS = ["M", "T", "W", "T", "F", "S", "S"]
    DAY_NAMES = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
    
    def __init__(self, value: list[int] = None, on_change=None):
        self._selected = set(value) if value else {0, 1, 2, 3, 4}  # Default weekdays
        self._on_change = on_change
        self._chips = []
        
        for i, label in enumerate(self.DAY_LABELS):
            chip = ft.Container(
                content=ft.Text(
                    label,
                    size=13,
                    weight=ft.FontWeight.W_600,
                    text_align=ft.TextAlign.CENTER,
                ),
                width=32,
                height=32,
                border_radius=16,
                alignment=ft.Alignment(0, 0),
                on_click=lambda e, idx=i: self._toggle(idx),
                tooltip=self.DAY_NAMES[i],
            )
            self._chips.append(chip)
        
        self._update_chip_styles()
        
        super().__init__(
            controls=self._chips,
            spacing=4,
        )
    
    def _toggle(self, idx: int):
        """Toggle a day on/off."""
        if idx in self._selected:
            self._selected.discard(idx)
        else:
            self._selected.add(idx)
        self._update_chip_styles()
        if self._on_change:
            self._on_change(self.value)
        self.update()
    
    def _update_chip_styles(self):
        """Update visual styles based on selection."""
        for i, chip in enumerate(self._chips):
            if i in self._selected:
                chip.bgcolor = Colors.PRIMARY
                chip.content.color = Colors.ON_PRIMARY
            else:
                chip.bgcolor = Colors.SURFACE_VARIANT
                chip.content.color = Colors.ON_SURFACE_VARIANT
    
    @property
    def value(self) -> list[int]:
        """Get selected days as sorted list of indices."""
        return sorted(self._selected)
    
    @value.setter
    def value(self, days: list[int]):
        """Set selected days."""
        self._selected = set(days)
        self._update_chip_styles()
        self.update()
    
    def select_weekdays(self):
        """Select Mon-Fri."""
        self.value = [0, 1, 2, 3, 4]
    
    def select_weekends(self):
        """Select Sat-Sun."""
        self.value = [5, 6]
    
    def select_all(self):
        """Select all days."""
        self.value = [0, 1, 2, 3, 4, 5, 6]


class MultiSelectChipGroup(ft.Container):
    """A row of toggleable chips for multi-selection.
    
    Usage:
        chips = MultiSelectChipGroup(["Mean", "Std", "Count"])
        selected = chips.value
    """
    
    def __init__(self, options: list[str], initial_value: list[str] = None):
        self._options = options
        self._selected = set(initial_value) if initial_value else set(options[:1])
        self._chips = []
        
        for opt in options:
            chip = ft.Container(
                content=ft.Text(opt, size=12, weight=ft.FontWeight.W_500),
                padding=ft.padding.symmetric(horizontal=12, vertical=6),
                border_radius=16,
                on_click=lambda e, o=opt: self._toggle(o),
                data=opt,
            )
            self._chips.append(chip)
            
        self._update_styles()
        
        super().__init__(
            content=ft.Row(self._chips, spacing=8, wrap=True)
        )
        
    def _toggle(self, option: str):
        if option in self._selected:
            if len(self._selected) > 1: # Prevent empty selection
                self._selected.discard(option)
        else:
            self._selected.add(option)
        self._update_styles()
        self.update()
        
    def _update_styles(self):
        for chip in self._chips:
            opt = chip.data
            if opt in self._selected:
                chip.bgcolor = Colors.PRIMARY_CONTAINER
                chip.content.color = Colors.ON_PRIMARY_CONTAINER
                chip.border = ft.border.all(1, Colors.PRIMARY)
            else:
                chip.bgcolor = Colors.SURFACE_VARIANT
                chip.content.color = Colors.ON_SURFACE_VARIANT
                chip.border = ft.border.all(1, Colors.BORDER)
                
    @property
    def value(self) -> list[str]:
        return [opt for opt in self._options if opt in self._selected]



========================================
FILE: src/tempo_app/ui/components/__init__.py
========================================
"""UI Components - Reusable UI widgets."""


========================================
FILE: src/tempo_app/ui/pages/batch_import.py
========================================
"""Batch Import Page - Import and process multiple sites from Excel/CSV.

This page allows users to:
1. Import sites from an Excel or CSV file
2. Configure default settings for all sites
3. Start batch processing with parallel downloads
4. Monitor progress and resume interrupted jobs
"""

import flet as ft
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional
import asyncio
import uuid

from ..theme import Colors, Spacing
from ..components.widgets import (
    HelpTooltip,
    LabeledField,
    SectionCard,
    StatusLogPanel,
    ProgressPanel,
    DaySelector,
)
from ...storage.models import BatchJob, BatchSite, BatchJobStatus, BatchSiteStatus
from ...storage.database import Database
from ...core.config import ConfigManager
from ...core.batch_parser import parse_import_file, ParseResult, ParsedSite
from ...core.batch_scheduler import BatchScheduler
from ...core.geo_utils import bbox_from_center


HELP_TEXTS = {
    "file_import": (
        "Upload an Excel (.xlsx) or CSV file containing site information.\n\n"
        "Required columns:\n"
        "- name (or site_name): Unique identifier for each site\n"
        "- latitude (or lat): Site latitude in decimal degrees\n"
        "- longitude (or lon): Site longitude in decimal degrees\n\n"
        "Optional columns:\n"
        "- date_start, date_end: Custom date range for this site\n"
        "- max_cloud, max_sza: Custom quality filters"
    ),
    "default_radius": (
        "Default radius in kilometers for calculating bounding boxes.\n\n"
        "A bounding box will be created around each site's coordinates\n"
        "using this radius.\n\n"
        "Typical values: 5-20 km depending on area of interest."
    ),
    "batch_size": (
        "Number of sites to process in parallel.\n\n"
        "Higher values = faster processing but more memory/network usage.\n"
        "Lower values = slower but more stable.\n\n"
        "Recommended: 3-5 for most systems."
    ),
}


class BatchImportPage(ft.Container):
    """Batch import page for processing multiple sites from Excel/CSV."""

    def __init__(self, db: Database, config: ConfigManager = None, data_dir: Path = None):
        super().__init__()
        self.db = db
        self.config = config
        self.data_dir = data_dir or Path("data")

        self._scheduler: Optional[BatchScheduler] = None
        self._current_job: Optional[BatchJob] = None
        self._parse_result: Optional[ParseResult] = None
        self._is_processing = False

        self._build()

    def did_mount(self):
        """Called when the control is added to the page."""
        # Add date pickers to overlay
        if hasattr(self, "_start_picker") and self._start_picker not in self.page.overlay:
            self.page.overlay.append(self._start_picker)
        if hasattr(self, "_end_picker") and self._end_picker not in self.page.overlay:
            self.page.overlay.append(self._end_picker)
        self.page.update()

        # Check for resumable jobs
        self._check_resumable_jobs()

    def _build(self):
        """Build the page layout."""
        # =================================================================
        # File Import Section
        # =================================================================
        self._file_picker = ft.FilePicker()
        self._file_path_text = ft.Text(
            "No file selected",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
            italic=True,
        )

        self._browse_btn = ft.ElevatedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.FOLDER_OPEN, size=18),
                ft.Text("Browse..."),
            ], spacing=8, tight=True),
            on_click=lambda e: asyncio.create_task(self._open_file_picker()),
        )

        # Sites preview table
        self._sites_table = ft.DataTable(
            columns=[
                ft.DataColumn(ft.Text("Row", size=12, color=Colors.ON_SURFACE)),
                ft.DataColumn(ft.Text("Name", size=12, color=Colors.ON_SURFACE)),
                ft.DataColumn(ft.Text("Latitude", size=12, color=Colors.ON_SURFACE)),
                ft.DataColumn(ft.Text("Longitude", size=12, color=Colors.ON_SURFACE)),

                ft.DataColumn(ft.Text("Status", size=12, color=Colors.ON_SURFACE)),
            ],
            rows=[],
            border=ft.border.all(1, Colors.BORDER),
            border_radius=8,
            heading_row_color=Colors.SURFACE_VARIANT,
            heading_row_height=40,
            data_row_min_height=36,
            data_row_max_height=36,
        )

        self._sites_table_container = ft.Container(
            content=ft.Column([
                self._sites_table,
            ], scroll=ft.ScrollMode.AUTO),
            height=200,
            visible=False,
        )

        self._parse_status = ft.Text(
            "",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )

        file_section = SectionCard(
            title="Import File",
            icon=ft.Icons.UPLOAD_FILE,
            help_text=HELP_TEXTS["file_import"],
            content=ft.Column([
                ft.Row([
                    self._browse_btn,
                    self._file_path_text,
                ], spacing=12),
                self._parse_status,
                self._sites_table_container,
            ], spacing=12),
        )

        # =================================================================
        # Job Name Section
        # =================================================================
        self._job_name_field = ft.TextField(
            hint_text="e.g., California_Sites_Jan2024",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            hint_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
            width=300,
        )

        name_section = LabeledField(
            label="Batch Job Name",
            field=self._job_name_field,
            help_text="A unique name for this batch import job. Helps identify it in the job list.",
            required=True,
        )

        # =================================================================
        # Default Settings Section
        # =================================================================
        self._default_radius = ft.Slider(
            min=1,
            max=50,
            value=10,
            divisions=49,
            label="{value} km",
            active_color=Colors.PRIMARY,
            inactive_color=Colors.SURFACE_VARIANT,
            on_change=self._on_radius_change,
        )

        self._radius_label = ft.Text(
            "10 km",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )

        # Date pickers
        today = date.today()
        default_start = today - timedelta(days=30)
        self._start_date = default_start
        self._end_date = today

        self._date_start_label = ft.Text(default_start.strftime("%b %d, %Y"))
        self._date_start_btn = ft.OutlinedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.CALENDAR_MONTH, size=18),
                self._date_start_label,
            ], spacing=8, tight=True),
            on_click=self._open_start_picker,
        )

        self._date_end_label = ft.Text(today.strftime("%b %d, %Y"))
        self._date_end_btn = ft.OutlinedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.CALENDAR_MONTH, size=18),
                self._date_end_label,
            ], spacing=8, tight=True),
            on_click=self._open_end_picker,
        )

        self._start_picker = ft.DatePicker(
            first_date=date(2023, 8, 1),
            last_date=today,
            on_change=self._on_start_date_change,
        )

        self._end_picker = ft.DatePicker(
            first_date=date(2023, 8, 1),
            last_date=today,
            on_change=self._on_end_date_change,
        )

        # Day selector
        self._day_selector = DaySelector(value=[0, 1, 2, 3, 4], on_change=self._on_day_change)

        day_presets = ft.Row([
            ft.TextButton("Weekdays", on_click=lambda e: self._day_selector.select_weekdays()),
            ft.TextButton("Weekends", on_click=lambda e: self._day_selector.select_weekends()),
            ft.TextButton("All", on_click=lambda e: self._day_selector.select_all()),
        ], spacing=4)

        # Hour selection
        self._hour_start = ft.Dropdown(
            options=[ft.DropdownOption(str(h), f"{h:02d}:00 UTC") for h in range(24)],
            value="16",
            width=130,
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )

        self._hour_end = ft.Dropdown(
            options=[ft.DropdownOption(str(h), f"{h:02d}:00 UTC") for h in range(24)],
            value="20",
            width=130,
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )

        # Quality filters
        self._max_cloud = ft.Slider(
            min=0,
            max=1,
            value=0.3,
            divisions=20,
            label="{value}",
            active_color=Colors.PRIMARY,
            inactive_color=Colors.SURFACE_VARIANT,
            on_change=self._on_cloud_change,
        )

        self._cloud_label = ft.Text(
            "0.30 (30% cloud cover)",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )

        self._max_sza = ft.Slider(
            min=30,
            max=90,
            value=70,
            divisions=12,
            label="{value}",
            active_color=Colors.PRIMARY,
            inactive_color=Colors.SURFACE_VARIANT,
            on_change=self._on_sza_change,
        )

        self._sza_label = ft.Text(
            "70 degrees",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )

        # Batch size
        self._batch_size = ft.Dropdown(
            options=[
                ft.DropdownOption("1", "1 site at a time"),
                ft.DropdownOption("3", "3 sites in parallel"),
                ft.DropdownOption("5", "5 sites in parallel (Recommended)"),
                ft.DropdownOption("10", "10 sites in parallel"),
            ],
            value="5",
            width=220,
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )

        settings_section = SectionCard(
            title="Default Settings",
            icon=ft.Icons.SETTINGS,
            help_text="These settings apply to all sites unless overridden in the Excel file.",
            content=ft.Column([
                # Radius
                ft.Column([
                    ft.Row([
                        ft.Text("Default Radius", size=14, color=Colors.ON_SURFACE),
                        HelpTooltip(HELP_TEXTS["default_radius"]),
                    ]),
                    self._default_radius,
                    self._radius_label,
                ], spacing=4),

                ft.Divider(height=16, color=Colors.DIVIDER),

                # Date range
                ft.Row([
                    ft.Column([
                        ft.Text("Start Date", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                        self._date_start_btn,
                    ], spacing=6),
                    ft.Text("to", color=Colors.ON_SURFACE_VARIANT),
                    ft.Column([
                        ft.Text("End Date", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                        self._date_end_btn,
                    ], spacing=6),
                ], spacing=16, vertical_alignment=ft.CrossAxisAlignment.END),

                # Day filter
                ft.Column([
                    ft.Text("Days of Week", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                    ft.Row([
                        self._day_selector,
                        ft.Container(width=16),
                        day_presets,
                    ]),
                ], spacing=6),

                # Hour range
                ft.Row([
                    LabeledField("Hour Start", self._hour_start),
                    ft.Text("to", color=Colors.ON_SURFACE_VARIANT),
                    LabeledField("Hour End", self._hour_end),
                ], spacing=16, vertical_alignment=ft.CrossAxisAlignment.END),

                ft.Divider(height=16, color=Colors.DIVIDER),

                # Quality filters
                ft.Column([
                    ft.Text("Max Cloud Fraction", size=14, color=Colors.ON_SURFACE),
                    self._max_cloud,
                    self._cloud_label,
                ], spacing=4),

                ft.Column([
                    ft.Text("Max Solar Zenith Angle", size=14, color=Colors.ON_SURFACE),
                    self._max_sza,
                    self._sza_label,
                ], spacing=4),

                ft.Divider(height=16, color=Colors.DIVIDER),

                # Batch size
                LabeledField(
                    label="Parallel Processing",
                    field=self._batch_size,
                    help_text=HELP_TEXTS["batch_size"],
                ),
            ], spacing=12),
        )

        # =================================================================
        # Action Buttons
        # =================================================================
        self._start_btn = ft.FilledButton(
            content=ft.Row([
                ft.Icon(ft.Icons.PLAY_ARROW, size=20),
                ft.Text("Start Batch Import"),
            ], spacing=8, tight=True),
            on_click=self._on_start_click,
        )

        self._pause_btn = ft.OutlinedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.PAUSE, size=20),
                ft.Text("Pause"),
            ], spacing=8, tight=True),
            visible=False,
            on_click=self._on_pause_click,
        )

        self._cancel_btn = ft.OutlinedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.CANCEL, size=20),
                ft.Text("Cancel"),
            ], spacing=8, tight=True),
            visible=False,
            on_click=self._on_cancel_click,
        )

        # =================================================================
        # Progress Section
        # =================================================================
        self._progress_bar = ft.ProgressBar(
            value=0,
            color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
        )

        self._progress_text = ft.Text(
            "Ready",
            size=14,
            color=Colors.ON_SURFACE,
        )

        self._progress_detail = ft.Text(
            "",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )

        progress_section = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Icon(ft.Icons.TRENDING_UP, size=18, color=Colors.PRIMARY),
                    ft.Text("Progress", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                ], spacing=8),
                self._progress_bar,
                self._progress_text,
                self._progress_detail,
            ], spacing=8),
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.border.all(1, Colors.BORDER),
            padding=16,
        )

        # Activity log
        self._status_log = StatusLogPanel()

        # =================================================================
        # Resumable Jobs Section
        # =================================================================
        self._resumable_jobs_container = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Icon(ft.Icons.HISTORY, size=18, color=Colors.WARNING),
                    ft.Text("Resumable Jobs", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                ], spacing=8),
                ft.Text(
                    "These jobs were interrupted and can be resumed:",
                    size=13,
                    color=Colors.ON_SURFACE_VARIANT,
                ),
            ], spacing=8),
            bgcolor=Colors.WARNING_CONTAINER,
            border_radius=12,
            padding=16,
            visible=False,
        )

        # =================================================================
        # Page Layout
        # =================================================================
        left_column = ft.Column(
            controls=[
                # Header
                ft.Row([
                    ft.Icon(ft.Icons.UPLOAD_FILE, size=28, color=Colors.PRIMARY),
                    ft.Text("Batch Import", size=28, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
                    HelpTooltip(
                        "Import multiple sites from an Excel or CSV file.\n\n"
                        "Each site will get its own dataset with a bounding box\n"
                        "calculated from the site coordinates and radius.\n\n"
                        "Downloads run in parallel for fast processing of\n"
                        "hundreds or thousands of sites."
                    ),
                ], spacing=8),

                ft.Divider(height=20, color=Colors.DIVIDER),

                self._resumable_jobs_container,

                file_section,
                ft.Container(height=8),
                name_section,
                ft.Container(height=8),
                settings_section,
                ft.Container(height=16),

                # Action buttons
                ft.Row([
                    self._start_btn,
                    self._pause_btn,
                    self._cancel_btn,
                ], spacing=12),
            ],
            scroll=ft.ScrollMode.AUTO,
            spacing=0,
            expand=True,
        )

        right_column = ft.Column(
            controls=[
                progress_section,
                ft.Container(height=8),
                self._status_log,
            ],
            expand=True,
        )

        self.content = ft.Row(
            controls=[
                ft.Container(content=left_column, expand=2, padding=ft.padding.only(right=16)),
                ft.Container(content=right_column, expand=1),
            ],
            expand=True,
            spacing=0,
        )

        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL

    # =====================================================================
    # File Handling
    # =====================================================================

    async def _open_file_picker(self):
        """Open the file picker dialog and handle result."""
        files = await self._file_picker.pick_files(
            allowed_extensions=["xlsx", "xls", "csv"],
            dialog_title="Select sites file",
        )

        if not files:
            return

        file_path = Path(files[0].path)
        self._file_path_text.value = file_path.name
        self._file_path_text.italic = False

        # Parse the file
        self._parse_result = parse_import_file(file_path)

        # Update UI with results
        self._update_parse_results()
        self.update()

    def _update_parse_results(self):
        """Update the UI with parse results."""
        if not self._parse_result:
            return

        result = self._parse_result

        # Update status text
        if result.is_valid:
            status_color = Colors.SUCCESS
            status_text = f"Parsed {result.valid_count} valid sites"
            if result.invalid_sites:
                status_text += f" ({len(result.invalid_sites)} with errors)"
            if result.warnings:
                status_text += f", {len(result.warnings)} warnings"
        else:
            status_color = Colors.ERROR
            status_text = f"Parse failed: {', '.join(result.errors)}"

        self._parse_status.value = status_text
        self._parse_status.color = status_color

        # Update table
        rows = []
        for site in result.sites[:50]:  # Limit to 50 for performance


            if site.error:
                status_icon = ft.Icon(ft.Icons.ERROR, size=16, color=Colors.ERROR)
                status_text = site.error[:30]
            else:
                status_icon = ft.Icon(ft.Icons.CHECK_CIRCLE, size=16, color=Colors.SUCCESS)
                status_text = "OK"

            rows.append(ft.DataRow(
                cells=[
                    ft.DataCell(ft.Text(str(site.row_number), size=12, color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(site.site_name[:20], size=12, color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(f"{site.latitude:.4f}", size=12, color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(f"{site.longitude:.4f}", size=12, color=Colors.ON_SURFACE)),

                    ft.DataCell(ft.Row([status_icon, ft.Text(status_text, size=11, color=Colors.ON_SURFACE)], spacing=4)),
                ]
            ))

        self._sites_table.rows = rows
        self._sites_table_container.visible = len(rows) > 0

        # Auto-generate job name from filename
        if result.file_path and not self._job_name_field.value:
            base_name = Path(result.file_path).stem
            self._job_name_field.value = f"{base_name}_{date.today().strftime('%Y%m%d')}"

    # =====================================================================
    # Date Pickers
    # =====================================================================

    def _open_start_picker(self, e):
        self._start_picker.open = True
        self.page.update()

    def _open_end_picker(self, e):
        self._end_picker.open = True
        self.page.update()

    def _on_start_date_change(self, e):
        if self._start_picker.value:
            self._start_date = self._start_picker.value
            self._date_start_label.value = self._start_date.strftime("%b %d, %Y")
            self.update()

    def _on_end_date_change(self, e):
        if self._end_picker.value:
            self._end_date = self._end_picker.value
            self._date_end_label.value = self._end_date.strftime("%b %d, %Y")
            self.update()

    # =====================================================================
    # Slider/Filter Handlers
    # =====================================================================

    def _on_day_change(self, days: list[int]):
        pass  # Day selector handles its own state

    def _on_radius_change(self, e):
        self._radius_label.value = f"{int(e.control.value)} km"
        self.update()

    def _on_cloud_change(self, e):
        val = e.control.value
        self._cloud_label.value = f"{val:.2f} ({int(val*100)}% cloud cover)"
        self.update()

    def _on_sza_change(self, e):
        self._sza_label.value = f"{int(e.control.value)} degrees"
        self.update()

    # =====================================================================
    # Job Actions
    # =====================================================================

    def _on_start_click(self, e):
        """Start the batch import job."""
        asyncio.create_task(self._start_batch_job())

    def _on_pause_click(self, e):
        """Pause the current job."""
        if self._scheduler:
            asyncio.create_task(self._scheduler.pause_job())
            self._status_log.add_info("Pausing job...")

    def _on_cancel_click(self, e):
        """Cancel the current job."""
        if self._scheduler:
            asyncio.create_task(self._scheduler.cancel_job())
            self._status_log.add_warning("Cancelling job...")

    async def _start_batch_job(self):
        """Create and start the batch import job."""
        # Validate inputs
        if not self._parse_result or not self._parse_result.valid_sites:
            self._status_log.add_error("No valid sites to import. Please select a file first.")
            return

        job_name = self._job_name_field.value.strip()
        if not job_name:
            self._status_log.add_error("Please enter a job name.")
            return

        # Get settings
        default_radius = self._default_radius.value
        hour_start = int(self._hour_start.value)
        hour_end = int(self._hour_end.value)
        hour_filter = list(range(hour_start, hour_end + 1))
        day_filter = self._day_selector.value
        max_cloud = self._max_cloud.value
        max_sza = self._max_sza.value
        batch_size = int(self._batch_size.value)

        # Create batch job
        job = BatchJob(
            id=str(uuid.uuid4()),
            name=job_name,
            created_at=datetime.now(),
            status=BatchJobStatus.PENDING,
            source_file=self._parse_result.file_path,
            total_sites=len(self._parse_result.valid_sites),
            default_radius_km=default_radius,
            date_start=self._start_date,
            date_end=self._end_date,
            day_filter=day_filter,
            hour_filter=hour_filter,
            max_cloud=max_cloud,
            max_sza=max_sza,
            batch_size=batch_size,
        )

        job = self.db.create_batch_job(job)
        self._current_job = job

        # Create batch sites
        batch_sites = []
        for i, parsed_site in enumerate(self._parse_result.valid_sites):
            radius = default_radius
            bbox = bbox_from_center(parsed_site.latitude, parsed_site.longitude, radius)

            batch_site = BatchSite(
                batch_job_id=job.id,
                site_name=parsed_site.site_name,
                latitude=parsed_site.latitude,
                longitude=parsed_site.longitude,
                radius_km=radius,
                bbox_west=bbox.west,
                bbox_south=bbox.south,
                bbox_east=bbox.east,
                bbox_north=bbox.north,
                custom_date_start=date.fromisoformat(parsed_site.custom_date_start) if parsed_site.custom_date_start else None,
                custom_date_end=date.fromisoformat(parsed_site.custom_date_end) if parsed_site.custom_date_end else None,
                custom_max_cloud=parsed_site.custom_max_cloud,
                custom_max_sza=parsed_site.custom_max_sza,
                sequence_number=i,
            )
            batch_sites.append(batch_site)

        self.db.create_batch_sites(batch_sites)

        # Update UI
        self._is_processing = True
        self._start_btn.visible = False
        self._pause_btn.visible = True
        self._cancel_btn.visible = True
        self.update()

        self._status_log.add_info(f"Starting batch job: {job_name}")
        self._status_log.add_info(f"Processing {job.total_sites} sites with batch size {batch_size}")

        # Create scheduler and start
        api_key = self.config.rsig_api_key if self.config else ""
        self._scheduler = BatchScheduler(
            db=self.db,
            data_dir=self.data_dir,
            max_concurrent_sites=batch_size,
            api_key=api_key,
            on_progress=self._on_progress,
            on_site_complete=self._on_site_complete,
            on_job_complete=self._on_job_complete,
        )

        try:
            await self._scheduler.start_job(job.id)
        except Exception as e:
            self._status_log.add_error(f"Job failed: {e}")
        finally:
            self._is_processing = False
            self._start_btn.visible = True
            self._pause_btn.visible = False
            self._cancel_btn.visible = False
            self.update()

    def _on_progress(self, job: BatchJob, site: BatchSite, message: str):
        """Handle progress updates from scheduler."""
        self._progress_text.value = message

        progress = job.progress
        self._progress_bar.value = progress
        self._progress_detail.value = f"{job.completed_sites + job.failed_sites}/{job.total_sites} sites processed"

        self._status_log.add_progress(f"{site.site_name}: {message}")
        self.update()

    def _on_site_complete(self, site: BatchSite):
        """Handle site completion."""
        if site.status == BatchSiteStatus.COMPLETED:
            self._status_log.add_success(f"Completed: {site.site_name}")
        else:
            self._status_log.add_error(f"Failed: {site.site_name} - {site.error_message}")
        self.update()

    def _on_job_complete(self, job: BatchJob):
        """Handle job completion."""
        if job.status == BatchJobStatus.COMPLETED:
            self._status_log.add_success(f"Job completed! {job.completed_sites} sites processed, {job.failed_sites} failed.")
            self._progress_text.value = "Completed!"
        elif job.status == BatchJobStatus.PAUSED:
            self._status_log.add_warning("Job paused. You can resume it later.")
            self._progress_text.value = "Paused"
        else:
            self._status_log.add_error(f"Job ended with status: {job.status.value}")
            self._progress_text.value = f"Ended: {job.status.value}"

        self._progress_bar.value = job.progress
        self.update()

    def _check_resumable_jobs(self):
        """Check for jobs that can be resumed."""
        resumable = self.db.get_resumable_batch_jobs()

        if not resumable:
            self._resumable_jobs_container.visible = False
            return

        # Build resumable jobs UI
        job_buttons = []
        for job in resumable[:5]:  # Show up to 5
            progress_pct = int(job.progress * 100)
            btn = ft.OutlinedButton(
                content=ft.Row([
                    ft.Icon(ft.Icons.PLAY_ARROW, size=16),
                    ft.Text(f"{job.name} ({progress_pct}%)", size=13),
                ], spacing=4, tight=True),
                on_click=lambda e, j=job: asyncio.create_task(self._resume_job(j.id)),
            )
            job_buttons.append(btn)

        self._resumable_jobs_container.content.controls = [
            ft.Row([
                ft.Icon(ft.Icons.HISTORY, size=18, color=Colors.WARNING),
                ft.Text("Resumable Jobs", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
            ], spacing=8),
            ft.Text(
                "These jobs were interrupted and can be resumed:",
                size=13,
                color=Colors.ON_SURFACE_VARIANT,
            ),
            ft.Row(job_buttons, spacing=8, wrap=True),
        ]

        self._resumable_jobs_container.visible = True
        self.update()

    async def _resume_job(self, job_id: str):
        """Resume a paused/interrupted job."""
        job = self.db.get_batch_job(job_id)
        if not job:
            self._status_log.add_error("Job not found")
            return

        self._current_job = job
        self._job_name_field.value = job.name

        # Update UI
        self._is_processing = True
        self._start_btn.visible = False
        self._pause_btn.visible = True
        self._cancel_btn.visible = True
        self._resumable_jobs_container.visible = False
        self.update()

        self._status_log.add_info(f"Resuming job: {job.name}")

        # Create scheduler and start
        api_key = self.config.rsig_api_key if self.config else ""
        self._scheduler = BatchScheduler(
            db=self.db,
            data_dir=self.data_dir,
            max_concurrent_sites=job.batch_size,
            api_key=api_key,
            on_progress=self._on_progress,
            on_site_complete=self._on_site_complete,
            on_job_complete=self._on_job_complete,
        )

        try:
            await self._scheduler.start_job(job_id)
        except Exception as e:
            self._status_log.add_error(f"Resume failed: {e}")
        finally:
            self._is_processing = False
            self._start_btn.visible = True
            self._pause_btn.visible = False
            self._cancel_btn.visible = False
            self._check_resumable_jobs()
            self.update()


========================================
FILE: src/tempo_app/ui/pages/create.py
========================================
"""Create Dataset Page - Configure and download TEMPO data.

This page allows users to:
1. Select a geographic region (preset or custom)
2. Choose date range and time filters
3. Set quality filters
4. Download data with real-time progress updates
"""

import flet as ft
from datetime import date, datetime, timedelta
from pathlib import Path
from typing import Optional
import asyncio
import tempfile
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt

from ..theme import Colors, Spacing
from ..components.widgets import (
    HelpTooltip,
    LabeledField,
    SectionCard,
    StatusLogPanel,
    ProgressPanel,
    WorkerProgressPanel,
    DaySelector,
)
from ...storage.models import REGION_PRESETS, BoundingBox, Dataset, DatasetStatus, Granule
from ...storage.database import Database
from ...core.status import get_status_manager
from ...core.downloader import RSIGDownloader
from ...core.processor import DataProcessor
from ...core.config import ConfigManager

class UIStatusAdapter:
    """Adapts UI StatusLogPanel to core StatusManager interface."""
    def __init__(self, log_panel, progress_panel, worker_panel=None, total_granules: int = 0, num_workers: int = 4):
        self.log = log_panel
        self.progress = progress_panel
        self.worker_panel = worker_panel
        self.total = total_granules
        self.completed = 0
        self._worker_slot = 0  # Cycle through worker slots for display
        self._num_workers = num_workers
        
    def emit(self, event: str, message: str, value: Optional[float] = None):
        if event == "download":
            if value is not None:
                self.progress.update_progress(value, message)
                # Update worker panel overall progress  
                if self.worker_panel and self.total > 0:
                    self.completed = int(value * self.total)
                    self.worker_panel.update_overall(self.completed, self.total, f"Downloading...")
            
            # Update a worker slot with the current download message
            if self.worker_panel and message:
                # Extract meaningful part from message (remove emoji prefixes)
                clean_msg = message.lstrip("‚¨áÔ∏è‚úÖ‚ö†Ô∏è‚ùå ").strip()
                if clean_msg:
                    self.worker_panel.update_worker(self._worker_slot, clean_msg[:40], active=True)
                    self._worker_slot = (self._worker_slot + 1) % self._num_workers
            
            # Log progress events
            self.log.add_progress(message)
        elif event == "error":
            self.log.add_error(message)
        elif event == "info":
            self.log.add_info(message)
            if self.worker_panel:
                self.worker_panel.update_overall(self.completed, self.total, message)
        elif event == "ok":
            self.log.add_success(message)
            # Mark as complete
            if self.worker_panel:
                self.worker_panel.complete(self.completed, self.total)
        elif event == "warning":
            self.log.add_warning(message)


# =============================================================================
# Help Text Constants - Detailed explanations for every field
# =============================================================================

HELP_TEXTS = {
    "dataset_name": (
        "A unique, descriptive name for this dataset. This helps you identify it later.\n\n"
        "Examples:\n"
        "‚Ä¢ 'July_Weekdays_SoCal' - July weekday data for Southern California\n"
        "‚Ä¢ 'Houston_Summer_2024' - Summer 2024 data for Houston area\n\n"
        "Tip: Include the time period and region in the name for easy reference."
    ),
    
    "region_preset": (
        "Pre-defined geographic regions with optimized bounding boxes.\n\n"
        "Each preset includes:\n"
        "‚Ä¢ Bounding box coordinates (West, South, East, North)\n"
        "‚Ä¢ Region-appropriate road data for map overlays\n\n"
        "Select 'Custom Region' to enter your own coordinates."
    ),
    
    "bbox_west": (
        "Western boundary longitude in decimal degrees.\n\n"
        "‚Ä¢ Negative values = West of Prime Meridian (Americas)\n"
        "‚Ä¢ Positive values = East of Prime Meridian (Europe, Asia)\n\n"
        "Example: Los Angeles is at approximately -118.2¬∞\n"
        "Valid range: -180 to 180"
    ),
    
    "bbox_south": (
        "Southern boundary latitude in decimal degrees.\n\n"
        "‚Ä¢ Positive values = Northern Hemisphere\n"
        "‚Ä¢ Negative values = Southern Hemisphere\n\n"
        "Example: San Diego is at approximately 32.7¬∞\n"
        "Valid range: -90 to 90"
    ),
    
    "bbox_east": (
        "Eastern boundary longitude in decimal degrees.\n\n"
        "Must be greater than the Western boundary.\n\n"
        "Example: For Southern California, a typical value is -116.4¬∞"
    ),
    
    "bbox_north": (
        "Northern boundary latitude in decimal degrees.\n\n"
        "Must be greater than the Southern boundary.\n\n"
        "Example: For Southern California, a typical value is 35.7¬∞"
    ),
    
    "date_start": (
        "First date to include in the download (inclusive).\n\n"
        "TEMPO data availability:\n"
        "‚Ä¢ Data begins from August 2023\n"
        "‚Ä¢ Recent data may have a 1-2 day delay\n\n"
        "Format: Year-Month-Day (YYYY-MM-DD)"
    ),
    
    "date_end": (
        "Last date to include in the download (inclusive).\n\n"
        "Tip: Keep the date range reasonable (1-2 months) for faster downloads.\n"
        "You can always create additional datasets for other periods."
    ),
    
    "day_filter": (
        "Filter by day of week to analyze patterns.\n\n"
        "Common choices:\n"
        "‚Ä¢ Weekdays Only (Mon-Fri): Compare to weekends for traffic analysis\n"
        "‚Ä¢ Weekends Only (Sat-Sun): Lower traffic baseline\n"
        "‚Ä¢ All Days: Complete dataset\n\n"
        "This is useful for studying how pollution varies by human activity patterns."
    ),
    
    "hour_start": (
        "Starting hour for daily data (UTC timezone).\n\n"
        "‚ö†Ô∏è IMPORTANT: All hours are in UTC (Coordinated Universal Time), not local time!\n\n"
        "UTC to Local Time Conversions:\n"
        "‚Ä¢ UTC 16:00 = PST 8:00 AM (Pacific, winter)\n"
        "‚Ä¢ UTC 16:00 = PDT 9:00 AM (Pacific, summer)\n"
        "‚Ä¢ UTC 16:00 = EST 11:00 AM (Eastern, winter)\n"
        "‚Ä¢ UTC 16:00 = EDT 12:00 PM (Eastern, summer)\n\n"
        "TEMPO observes North America roughly 8 AM - 6 PM local time.\n"
        "Typical UTC range: 13:00 - 23:00 (covers continental US daylight hours)"
    ),
    
    "hour_end": (
        "Ending hour for daily data (UTC timezone).\n\n"
        "Each hour between start and end (inclusive) will be downloaded separately.\n\n"
        "Example: Hours 16-20 UTC = 5 hourly observations per day\n\n"
        "More hours = larger download but better temporal resolution."
    ),
    
    "max_cloud": (
        "Maximum cloud fraction allowed (0.0 to 1.0).\n\n"
        "‚Ä¢ 0.0 = Only completely clear pixels (very strict)\n"
        "‚Ä¢ 0.3 = Up to 30% cloud cover (recommended)\n"
        "‚Ä¢ 0.5 = Up to 50% cloud cover (more data, less quality)\n"
        "‚Ä¢ 1.0 = No filtering (includes fully cloudy pixels)\n\n"
        "Cloud-covered pixels have unreliable NO‚ÇÇ/HCHO measurements.\n"
        "Recommended: 0.2 - 0.4 for good balance of quality and coverage."
    ),
    
    "max_sza": (
        "Maximum Solar Zenith Angle in degrees.\n\n"
        "Solar Zenith Angle (SZA) is the angle between the sun and vertical:\n"
        "‚Ä¢ 0¬∞ = Sun directly overhead (noon at equator)\n"
        "‚Ä¢ 90¬∞ = Sun at horizon (sunrise/sunset)\n\n"
        "High SZA means longer light path through atmosphere = less accurate retrieval.\n\n"
        "Recommended values:\n"
        "‚Ä¢ 70¬∞ - Standard filter (excludes very early/late observations)\n"
        "‚Ä¢ 60¬∞ - Stricter filter (higher quality)\n"
        "‚Ä¢ 80¬∞ - Lenient filter (more data, lower quality)"
    ),
}


class CreatePage(ft.Container):
    """Create Dataset page with form inputs and download functionality."""
    
    def __init__(self, db: Database, config: ConfigManager = None):
        super().__init__()
        self.db = db
        self.config = config
        self.status = get_status_manager()
        self._is_downloading = False
        self._current_download_id: Optional[str] = None  # For global DownloadManager
        
        # Form state
        self._selected_preset: Optional[str] = None
        self._extend_mode = False  # True = extending existing dataset
        self._extend_dataset_id: Optional[str] = None  # ID of dataset being extended

        
        # Build the page
        self._build()

    def did_mount(self):
        """Called when the control is added to the page."""
        # Add date pickers to overlay safely
        if hasattr(self, "_start_picker") and self._start_picker not in self.page.overlay:
            self.page.overlay.append(self._start_picker)
        if hasattr(self, "_end_picker") and self._end_picker not in self.page.overlay:
            self.page.overlay.append(self._end_picker)
        # Load datasets async
        self.page.run_task(self._load_datasets_async)
        # Generate initial map preview
        self.page.run_task(self._update_map_preview_async)

    async def _load_datasets_async(self):
        """Load datasets without blocking UI."""
        import asyncio
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        self._apply_datasets(datasets)
        self.update()

    def _apply_datasets(self, datasets: list):
        """Apply datasets to selector (no DB call)."""
        options = []
        for ds in datasets:
            label = f"{ds.name} ({ds.date_start} to {ds.date_end})"
            options.append(ft.DropdownOption(key=ds.id, text=label))
        self._dataset_selector.options = options
        if options:
            self._dataset_selector.value = options[0].key
    
    def _build(self):
        """Build the page layout."""
        # =====================================================================
        # Mode Toggle - New vs Extend Existing
        # =====================================================================
        self._mode_radio = ft.RadioGroup(
            value="new",
            on_change=self._on_mode_change,
            content=ft.Row([
                ft.Radio(value="new", label="New Dataset", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
                ft.Radio(value="extend", label="Extend Existing", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
            ], spacing=16),
        )
        
        # Dataset selector for extend mode
        self._dataset_selector = ft.Dropdown(
            label="Select Dataset",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            width=300,
        )
        
        self._load_btn = ft.FilledTonalButton(
            content=ft.Row([
                ft.Icon(ft.Icons.DOWNLOAD, size=18),
                ft.Text("Load"),
            ], spacing=6, tight=True),
            on_click=self._on_load_dataset_click,
        )
        
        self._extend_container = ft.Container(
            content=ft.Row([
                self._dataset_selector,
                self._load_btn,
            ], spacing=12),
            visible=False,  # Hidden by default
            padding=ft.padding.only(top=8),
        )
        
        mode_section = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Text("Mode:", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                    self._mode_radio,
                ], spacing=16, vertical_alignment=ft.CrossAxisAlignment.CENTER),
                self._extend_container,
            ], spacing=4),
            padding=ft.padding.only(bottom=8),
        )
        
        # =====================================================================
        # Dataset Name Section
        # =====================================================================
        self._name_field = ft.TextField(
            hint_text="e.g., July_Weekdays_SoCal",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            hint_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
        )
        
        name_section = LabeledField(
            label="Dataset Name",
            field=self._name_field,
            help_text=HELP_TEXTS["dataset_name"],
            required=True,
        )
        
        # =====================================================================
        # Geographic Region Section - Simple Range Inputs
        # =====================================================================
        preset_options = [
            ft.DropdownOption(key="custom", text="Custom Region"),
        ]
        for name, (bbox, _) in REGION_PRESETS.items():
            preset_options.append(ft.DropdownOption(key=name, text=name))
        
        self._region_dropdown = ft.Dropdown(
            options=preset_options,
            value="Southern California",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            on_select=self._on_region_change,
            width=220,
        )
        
        # Store bbox values
        self._bbox = [-119.68, 32.23, -116.38, 35.73]  # west, south, east, north
        
        # Custom coordinate inputs (shown when "Custom" selected)
        self._west_field = ft.TextField(value="-119.68", width=90, label="West", dense=True,
            border_color=Colors.BORDER, bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE, size=13))
        self._east_field = ft.TextField(value="-116.38", width=90, label="East", dense=True,
            border_color=Colors.BORDER, bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE, size=13))
        self._south_field = ft.TextField(value="32.23", width=90, label="South", dense=True,
            border_color=Colors.BORDER, bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE, size=13))
        self._north_field = ft.TextField(value="35.73", width=90, label="North", dense=True,
            border_color=Colors.BORDER, bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE, size=13))
        
        self._custom_fields = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Text("Lon:", size=12, color=Colors.ON_SURFACE_VARIANT, width=30),
                    self._west_field, ft.Text("to", size=12, color=Colors.ON_SURFACE_VARIANT), self._east_field,
                ], spacing=8),
                ft.Row([
                    ft.Text("Lat:", size=12, color=Colors.ON_SURFACE_VARIANT, width=30),
                    self._south_field, ft.Text("to", size=12, color=Colors.ON_SURFACE_VARIANT), self._north_field,
                ], spacing=8),
            ], spacing=8),
            visible=False,
            padding=ft.padding.only(top=8),
        )
        
        # Coordinate display (shown when preset selected)
        self._coord_display = ft.Container(
            content=ft.Column([
                ft.Text(f"Longitude: {self._bbox[0]:.2f}¬∞ to {self._bbox[2]:.2f}¬∞", size=13, color=Colors.ON_SURFACE),
                ft.Text(f"Latitude: {self._bbox[1]:.2f}¬∞ to {self._bbox[3]:.2f}¬∞", size=13, color=Colors.ON_SURFACE),
            ], spacing=4),
            visible=True,
        )
        
        region_section = SectionCard(
            title="Geographic Region",
            icon=ft.Icons.MAP,
            help_text="Select a preset region or enter custom coordinates.",
            content=ft.Column([
                ft.Row([
                    ft.Text("Region:", size=14, color=Colors.ON_SURFACE),
                    self._region_dropdown,
                ], spacing=12),
                self._custom_fields,
                self._coord_display,
            ], spacing=6),
        )


        
        # =====================================================================
        # Temporal Selection Section
        # =====================================================================
        today = date.today()
        default_start = today - timedelta(days=30)
        
        self._start_date = default_start
        self._end_date = today
        
        # Date picker buttons (click to open calendar)
        self._date_start_label = ft.Text(default_start.strftime("%b %d, %Y"))
        self._date_start_btn = ft.OutlinedButton(
            content=ft.Row(
                [
                    ft.Icon(ft.Icons.CALENDAR_MONTH),
                    self._date_start_label,
                ],
                alignment=ft.MainAxisAlignment.CENTER,
                spacing=8,
            ),
            on_click=self._open_start_picker,
            style=ft.ButtonStyle(
                shape=ft.RoundedRectangleBorder(radius=8),
            ),
        )
        
        self._date_end_label = ft.Text(today.strftime("%b %d, %Y"))
        self._date_end_btn = ft.OutlinedButton(
             content=ft.Row(
                [
                    ft.Icon(ft.Icons.CALENDAR_MONTH),
                    self._date_end_label,
                ],
                alignment=ft.MainAxisAlignment.CENTER,
                spacing=8,
            ),
            on_click=self._open_end_picker,
            style=ft.ButtonStyle(
                shape=ft.RoundedRectangleBorder(radius=8),
            ),
        )
        
        # Date pickers
        self._start_picker = ft.DatePicker(
            first_date=date(2023, 8, 1),  # TEMPO data start
            last_date=today,
            on_change=self._on_start_date_change,
        )
        
        self._end_picker = ft.DatePicker(
            first_date=date(2023, 8, 1),
            last_date=today,
            on_change=self._on_end_date_change,
        )
        
        # Day selector - toggle chips
        self._day_selector = DaySelector(value=[0, 1, 2, 3, 4], on_change=self._on_day_change)
        
        # Quick preset buttons for day selection
        day_presets = ft.Row([
            ft.TextButton("Weekdays", on_click=lambda e: self._day_selector.select_weekdays()),
            ft.TextButton("Weekends", on_click=lambda e: self._day_selector.select_weekends()),
            ft.TextButton("All", on_click=lambda e: self._day_selector.select_all()),
        ], spacing=4)
        
        self._hour_start = ft.Dropdown(
            options=[ft.DropdownOption(str(h), f"{h:02d}:00 UTC") for h in range(24)],
            value="16",
            width=130,
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        self._hour_start.on_change = self._on_hour_change
        
        self._hour_end = ft.Dropdown(
            options=[ft.DropdownOption(str(h), f"{h:02d}:00 UTC") for h in range(24)],
            value="20",
            width=130,
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        self._hour_end.on_change = self._on_hour_change
        
        # UTC timezone warning
        utc_warning = ft.Container(
            content=ft.Row([
                ft.Icon(ft.Icons.INFO, size=16, color=Colors.WARNING),
                ft.Text(
                    "‚è∞ Hours are in UTC (Coordinated Universal Time), not local time! "
                    "UTC 16:00 = 8 AM PST / 11 AM EST",
                    size=12,
                    color=Colors.WARNING,
                    italic=True,
                ),
            ], spacing=8),
            bgcolor=Colors.WARNING_CONTAINER,
            border_radius=8,
            padding=10,
            margin=ft.margin.only(top=8),
        )
        
        temporal_section = SectionCard(
            title="Time Selection",
            icon=ft.Icons.CALENDAR_MONTH,
            help_text=(
                "Select the date range and hours to download.\n\n"
                "TEMPO satellite scans North America hourly during daylight hours, "
                "typically from ~13:00 to ~23:00 UTC (covering sunrise to sunset across the continent).\n\n"
                "Each unique combination of date + hour is called a 'granule'."
            ),
            content=ft.Column([
                # Date range row
                ft.Row([
                    ft.Column([
                        ft.Row([
                            ft.Text("Start Date", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                            HelpTooltip(HELP_TEXTS["date_start"]),
                        ]),
                        self._date_start_btn,
                    ], spacing=6),
                    ft.Text("to", color=Colors.ON_SURFACE_VARIANT),
                    ft.Column([
                        ft.Row([
                            ft.Text("End Date", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                            HelpTooltip(HELP_TEXTS["date_end"]),
                        ]),
                        self._date_end_btn,
                    ], spacing=6),
                ], spacing=16, vertical_alignment=ft.CrossAxisAlignment.END),
                
                # Day filter with toggle chips
                ft.Column([
                    ft.Row([
                        ft.Text("Days of Week", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                        HelpTooltip(HELP_TEXTS["day_filter"]),
                    ]),
                    ft.Row([
                        self._day_selector,
                        ft.Container(width=16),
                        day_presets,
                    ]),
                ], spacing=6),
                
                ft.Row([
                    LabeledField("Hour Start", self._hour_start, HELP_TEXTS["hour_start"]),
                    ft.Text("to", color=Colors.ON_SURFACE_VARIANT),
                    LabeledField("Hour End", self._hour_end, HELP_TEXTS["hour_end"]),
                ], spacing=16, vertical_alignment=ft.CrossAxisAlignment.END),
                
                utc_warning,
            ], spacing=12),
        )
        
        # =====================================================================
        # Quality Filters Section
        # =====================================================================
        self._max_cloud = ft.Slider(
            min=0,
            max=1,
            value=0.3,
            divisions=20,
            label="{value}",
            active_color=Colors.PRIMARY,
            inactive_color=Colors.SURFACE_VARIANT,
            on_change=self._on_cloud_change,
        )
        
        self._cloud_label = ft.Text(
            "0.30 (30% cloud cover)",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        self._max_sza = ft.Slider(
            min=30,
            max=90,
            value=70,
            divisions=12,
            label="{value}¬∞",
            active_color=Colors.PRIMARY,
            inactive_color=Colors.SURFACE_VARIANT,
            on_change=self._on_sza_change,
        )
        
        self._sza_label = ft.Text(
            "70¬∞ (excludes very early/late observations)",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        filter_section = SectionCard(
            title="Quality Filters",
            icon=ft.Icons.FILTER_ALT,
            help_text=(
                "Quality filters remove unreliable data points.\n\n"
                "Stricter filters (lower values) = higher quality but less data.\n"
                "Lenient filters (higher values) = more data but lower quality.\n\n"
                "The defaults are good starting points for most analyses."
            ),
            content=ft.Column([
                ft.Column([
                    ft.Row([
                        ft.Text("Max Cloud Fraction", size=14, color=Colors.ON_SURFACE),
                        HelpTooltip(HELP_TEXTS["max_cloud"]),
                    ]),
                    self._max_cloud,
                    self._cloud_label,
                ], spacing=4),
                
                ft.Container(height=8),
                
                ft.Column([
                    ft.Row([
                        ft.Text("Max Solar Zenith Angle", size=14, color=Colors.ON_SURFACE),
                        HelpTooltip(HELP_TEXTS["max_sza"]),
                    ]),
                    self._max_sza,
                    self._sza_label,
                ], spacing=4),
            ], spacing=8),
        )
        
        # =====================================================================
        # Download Button and Progress
        # =====================================================================
        self._download_btn = ft.FilledButton(
            content=ft.Row([
                ft.Icon(ft.Icons.CLOUD_DOWNLOAD, size=20),
                ft.Text("Download & Create Dataset"),
            ], spacing=8, tight=True),
            style=ft.ButtonStyle(
                padding=ft.padding.symmetric(horizontal=24, vertical=12),
            ),
            on_click=self._on_download_click,
        )
        
        self._cancel_btn = ft.OutlinedButton(
            content=ft.Row([
                ft.Icon(ft.Icons.CANCEL, size=20),
                ft.Text("Cancel"),
            ], spacing=8, tight=True),
            visible=False,
            on_click=self._on_cancel_click,
        )
        
        self._progress_panel = ProgressPanel()
        # Use configured number of workers (default 4 if config not available)
        num_workers = self.config.download_workers if self.config else 4
        self._worker_progress = WorkerProgressPanel(num_workers=num_workers)  # For right column
        self._status_log = StatusLogPanel()
        
        # =====================================================================
        # Dataset Preview / Summary
        # =====================================================================
        self._preview_text = ft.Text(
            "",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        preview_section = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Icon(ft.Icons.PREVIEW, size=18, color=Colors.PRIMARY),
                    ft.Text("Dataset Preview", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                    HelpTooltip(
                        "A summary of what will be downloaded based on your current settings.\n\n"
                        "Check this before clicking Download to ensure the dataset is reasonable in size."
                    ),
                ], spacing=8),
                self._preview_text,
            ], spacing=8),
            bgcolor=Colors.SURFACE,
            border_radius=12,
            border=ft.Border(
                left=ft.BorderSide(1, Colors.BORDER),
                top=ft.BorderSide(1, Colors.BORDER),
                right=ft.BorderSide(1, Colors.BORDER),
                bottom=ft.BorderSide(1, Colors.BORDER),
            ),
            padding=16,
        )
        

        
        # =====================================================================
        # Map Preview Section (Right Panel)
        # =====================================================================
        # Initialize map controls
        # Use 1x1 transparent pixel to avoid "valid src must be specified" error
        self._map_image_control = ft.Image(
            src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7",
            fit="contain", 
            gapless_playback=True
        )
        
        self._map_progress = ft.ProgressBar(width=None, visible=False, color=Colors.PRIMARY, height=2)
        
        self._map_preview = ft.Container(
            content=ft.Column([
                ft.Icon(ft.Icons.MAP_OUTLINED, size=64, color=Colors.ON_SURFACE_VARIANT),
                ft.Text("Region Preview", size=16, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                ft.Text("Select a region to see it on the map", size=12, color=Colors.ON_SURFACE_VARIANT),
            ], horizontal_alignment=ft.CrossAxisAlignment.CENTER, alignment=ft.MainAxisAlignment.CENTER),
            bgcolor=Colors.SURFACE_VARIANT,
            border_radius=12,
            alignment=ft.Alignment(0, 0),
            expand=True,
        )
        
        # Coordinate display for map preview
        self._map_coord_label = ft.Text(
            "W: -119.68¬∞  S: 32.23¬∞  E: -116.38¬∞  N: 35.73¬∞",
            size=12,
            color=Colors.ON_SURFACE_VARIANT,
            text_align=ft.TextAlign.CENTER,
        )
        
        map_preview_panel = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Text("Region Preview", size=16, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                    ft.Container(expand=True),
                    ft.IconButton(
                        icon=ft.Icons.SAVE_ALT,
                        tooltip="Save Image",
                        on_click=self._on_save_map_click,
                    ),
                    ft.IconButton(
                        icon=ft.Icons.REFRESH,
                        tooltip="Refresh map preview",
                        on_click=self._on_refresh_map_click,
                    ),
                ], spacing=8),
                ft.Container(height=8),

                self._map_progress,
                # Map Preview (Static)
                ft.Container(
                    content=self._map_image_control,
                    bgcolor=Colors.SURFACE_VARIANT,
                    border_radius=12,
                    alignment=ft.Alignment(0, 0),
                    clip_behavior=ft.ClipBehavior.HARD_EDGE,
                    expand=True,
                ),
                
                ft.Container(height=8),
                self._map_coord_label,
            ], expand=True),
            padding=16,
            expand=True,
        )
        
        # =====================================================================
        # Page Layout - Left (Form 40%) / Right (Map 60%)
        # =====================================================================
        
        # Back button for header
        back_btn = ft.TextButton(
            content=ft.Row([
                ft.Icon(ft.Icons.ARROW_BACK, size=18),
                ft.Text("Back to Library"),
            ], spacing=6, tight=True),
            on_click=self._on_back_click,
        )
        
        left_column = ft.Column(
            controls=[
                # Header with back button
                ft.Row([
                    back_btn,
                    ft.Container(width=16),
                    ft.Text("üìä New Dataset", size=24, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
                    HelpTooltip(
                        "Create a new dataset by downloading TEMPO satellite data.\n\n"
                        "TEMPO (Tropospheric Emissions: Monitoring of Pollution) is a NASA "
                        "satellite instrument that measures air quality over North America "
                        "every hour during daylight.\n\n"
                        "It provides:\n"
                        "‚Ä¢ NO‚ÇÇ (Nitrogen Dioxide) - from vehicles, power plants\n"
                        "‚Ä¢ HCHO (Formaldehyde) - from vegetation, fires, industry\n"
                        "‚Ä¢ FNR (HCHO/NO‚ÇÇ ratio) - indicates VOC vs NOx sensitivity"
                    ),
                ], spacing=8, vertical_alignment=ft.CrossAxisAlignment.CENTER),
                
                ft.Divider(height=20, color=Colors.DIVIDER),
                
                mode_section,
                name_section,
                ft.Container(height=8),
                region_section,
                ft.Container(height=8),
                temporal_section,
                ft.Container(height=8),
                filter_section,
                ft.Container(height=16),
                

                
                # Download controls
                ft.Row([
                    self._download_btn,
                    self._cancel_btn,
                ], spacing=12),
                
                self._progress_panel,
            ],
            scroll=ft.ScrollMode.AUTO,
            spacing=0,
            expand=True,
        )
        
        right_column = ft.Column(
            controls=[
                map_preview_panel,
                ft.Container(height=16),
                self._worker_progress,  # Download progress
                self._status_log,
            ],
            expand=True,
        )
        
        # Main content - 40/60 split
        self.content = ft.Row(
            controls=[
                ft.Container(content=left_column, expand=2, padding=ft.padding.only(right=16)),
                ft.VerticalDivider(width=1, color=Colors.DIVIDER),
                ft.Container(content=right_column, expand=3),
            ],
            expand=True,
            spacing=0,
        )
        
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL
        
        # Initial preview/estimation update
        self._update_preview()
        self._update_map_coord_label()
    
    def _get_shell(self):
        """Get the AppShell from the page."""
        if self.page and self.page.controls:
            return self.page.controls[0]
        return None
    
    def _get_download_manager(self):
        """Get the global DownloadManager from the shell."""
        shell = self._get_shell()
        if shell and hasattr(shell, 'download_manager'):
            return shell.download_manager
        return None
    
    def _on_back_click(self, e):
        """Navigate back to Library."""
        shell = self._get_shell()
        if shell and hasattr(shell, 'navigate_to'):
            shell.navigate_to("/library")
    
    def _on_refresh_map_click(self, e):
        """Manually refresh the map preview."""
        self._update_map_coord_label()
        if self.page:
            self.page.run_task(self._update_map_preview_async)
    
    def _update_map_coord_label(self):
        """Update the map coordinate label based on current bbox."""
        try:
            if hasattr(self, '_map_coord_label'):
                self._map_coord_label.value = f"W: {self._bbox[0]:.2f}¬∞  S: {self._bbox[1]:.2f}¬∞  E: {self._bbox[2]:.2f}¬∞  N: {self._bbox[3]:.2f}¬∞"
        except Exception:
            pass

    
    def _generate_map_preview(self, bbox=None, detailed=False):
        """Generate a lightweight map preview showing the bbox region using cartopy."""
        try:
            import cartopy.crs as ccrs
            import cartopy.feature as cfeature
            import cartopy.io.shapereader as shapereader
            import time
            
            # Use passed bbox or current state
            west, south, east, north = bbox if bbox else self._bbox
            
            # Create figure with cartopy projection - higher res for dynamic sizing
            fig = plt.figure(figsize=(10, 8), dpi=150)
            ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())
            
            # Set extent with some padding
            pad_lon = (east - west) * 0.15
            pad_lat = (north - south) * 0.15
            ax.set_extent([
                west - pad_lon, east + pad_lon,
                south - pad_lat, north + pad_lat
            ], crs=ccrs.PlateCarree())
            
            # Add geographic features
            ax.add_feature(cfeature.LAND, facecolor='#f8f9fa')
            ax.add_feature(cfeature.OCEAN, facecolor='#e3f2fd')
            ax.add_feature(cfeature.COASTLINE, linewidth=0.8, edgecolor='#37474f')
            ax.add_feature(cfeature.BORDERS, linewidth=1.0, edgecolor='#263238', linestyle='-')
            ax.add_feature(cfeature.STATES, linewidth=0.8, edgecolor='#546e7a', linestyle='-')
            
            # Add detailed highways (Natural Earth roads) if requested
            if detailed:
                try:
                    roads_10m = shapereader.natural_earth(
                        resolution='10m',
                        category='cultural',
                        name='roads'
                    )
                    reader = shapereader.Reader(roads_10m)
                    
                    # Filter and plot roads
                    for record in reader.records():
                        road_type = record.attributes.get('type', '')
                        
                        # Filter for map detail
                        # Interstate: Black, thick, top priority
                        # Major/Beltway: Dark Grey, medium
                        # Others: Light Grey, thin
                        
                        linewidth = 0.5
                        color = '#9e9e9e'
                        zorder = 3
                        
                        if road_type == 'Interstate':
                            linewidth = 1.2
                            color = '#000000' # Dark Black
                            zorder = 5
                        elif road_type in ['Major Highway', 'Beltway']:
                            linewidth = 1.0
                            color = '#424242' # Dark Grey
                            zorder = 4
                        else:
                            # Keep little roads but grey
                            linewidth = 0.6
                            color = '#757575' # Grey
                            zorder = 3
                            
                        # Check intersection with view bounds (optimization)
                        geom = record.geometry
                        bounds = geom.bounds
                        # Simple bounds check with padding
                        if (bounds[2] < west - pad_lon or bounds[0] > east + pad_lon or 
                            bounds[3] < south - pad_lat or bounds[1] > north + pad_lat):
                            continue
                            
                        ax.add_geometries(
                            [geom],
                            ccrs.PlateCarree(),
                            facecolor='none',
                            edgecolor=color,
                            linewidth=linewidth,
                            alpha=0.7,
                            zorder=zorder
                        )
                except Exception as e:
                    print(f"Road overlay warning: {e}")
                    
                except Exception as e:
                    print(f"Road overlay warning: {e}")
                    
                # Add top 5 major cities
                try:
                    cities_shp = shapereader.natural_earth(
                        resolution='10m',
                        category='cultural',
                        name='populated_places'
                    )
                    reader = shapereader.Reader(cities_shp)
                    
                    valid_cities = []
                    
                    for record in reader.records():
                        # Get attributes
                        name = record.attributes.get('NAME', '')
                        pop_max = record.attributes.get('POP_MAX', 0)
                        
                        # Use lower threshold to gather candidates, then sort
                        if pop_max < 50000:
                            continue
                            
                        geom = record.geometry
                        x = geom.x
                        y = geom.y
                        
                        # Check bounds (strict to view)
                        if (x < west - pad_lon or x > east + pad_lon or 
                            y < south - pad_lat or y > north + pad_lat):
                            continue
                            
                        valid_cities.append((pop_max, name, x, y))
                    
                    # Sort by population descending and take top 5
                    valid_cities.sort(key=lambda x: x[0], reverse=True)
                    top_cities = valid_cities[:5]
                    
                    for pop, name, x, y in top_cities:
                        # Plot marker
                        ax.plot(x, y, marker='o', color='black', markersize=4, 
                               transform=ccrs.PlateCarree(), zorder=10)
                        
                        # Use simple right alignment for all labels as requested
                        txt_x = x + 0.05
                        txt_y = y + 0.02
                        ha = 'left'
                        
                        # Plot label with halo
                        text = ax.text(txt_x, txt_y, name,
                                     fontsize=9, fontweight='bold', color='#212121',
                                     ha=ha, va='bottom',
                                     transform=ccrs.PlateCarree(), zorder=11)
                                     
                        import matplotlib.patheffects as path_effects
                        text.set_path_effects([
                            path_effects.withStroke(linewidth=2, foreground='white', alpha=0.8)
                        ])
                        
                except Exception as e:
                    print(f"City overlay warning: {e}")
            
            # Draw the bbox rectangle
            from matplotlib.patches import Rectangle
            rect = Rectangle(
                (west, south), east - west, north - south,
                linewidth=2, edgecolor='#6200EE', facecolor='#6200EE',
                alpha=0.2, transform=ccrs.PlateCarree(),
                zorder=6
            )
            ax.add_patch(rect)
            
            # Draw bbox outline
            ax.plot([west, east, east, west, west],
                    [south, south, north, north, south],
                    color='#6200EE', linewidth=2, transform=ccrs.PlateCarree(), zorder=6)
            
            # Add gridlines
            gl = ax.gridlines(draw_labels=True, linewidth=0.3, color='gray', alpha=0.5, zorder=7)
            gl.top_labels = False
            gl.right_labels = False
            gl.xlabel_style = {'size': 8}
            gl.ylabel_style = {'size': 8}
            
            plt.tight_layout()
            
            # Save to temp file with unique name to bust cache
            temp_dir = Path(tempfile.gettempdir()) / "tempo_app"
            temp_dir.mkdir(exist_ok=True)
            timestamp = int(time.time() * 1000)
            preview_path = temp_dir / f"region_preview_{timestamp}.png"
            fig.savefig(preview_path, dpi=100, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            plt.close(fig)
            
            return str(preview_path)
            
        except Exception as e:
            # Return error message as string
            import traceback
            traceback.print_exc()
            return f"Error: {e}"
    
    def _on_mode_change(self, e):
        """Handle mode toggle change."""
        selected = e.control.value if e.control.value else "new"
        self._extend_mode = (selected == "extend")
        
        # Show/hide extend container
        self._extend_container.visible = self._extend_mode
        
        # Update name field state
        if self._extend_mode:
            self._name_field.read_only = True
            self._name_field.hint_text = "Select a dataset to load"
            # Refresh dataset options in case new ones were created
            self.page.run_task(self._load_datasets_async)
        else:
            self._name_field.read_only = False
            self._name_field.hint_text = "e.g., July_Weekdays_SoCal"
            self._name_field.value = ""
            self._extend_dataset_id = None
        
        self.update()
    
    def _on_load_dataset_click(self, e):
        """Load selected dataset configuration into the form."""
        selected_id = self._dataset_selector.value
        if not selected_id:
            self._status_log.add_error("Please select a dataset to load")
            return
        
        dataset = self.db.get_dataset(selected_id)
        if not dataset:
            self._status_log.add_error("Dataset not found")
            return
        
        self._extend_dataset_id = dataset.id
        
        # Populate name field
        self._name_field.value = dataset.name
        
        # Populate bbox - find matching preset or set custom
        preset_found = False
        for preset_name, (preset_bbox, _) in REGION_PRESETS.items():
            if (abs(preset_bbox.west - dataset.bbox.west) < 0.01 and
                abs(preset_bbox.south - dataset.bbox.south) < 0.01 and
                abs(preset_bbox.east - dataset.bbox.east) < 0.01 and
                abs(preset_bbox.north - dataset.bbox.north) < 0.01):
                self._region_dropdown.value = preset_name
                preset_found = True
                break
        
        if not preset_found:
            self._region_dropdown.value = "custom"
            self._west_field.value = str(dataset.bbox.west)
            self._south_field.value = str(dataset.bbox.south)
            self._east_field.value = str(dataset.bbox.east)
            self._north_field.value = str(dataset.bbox.north)
            self._custom_fields.visible = True
            self._coord_display.visible = False
        else:
            self._bbox = [dataset.bbox.west, dataset.bbox.south, dataset.bbox.east, dataset.bbox.north]
            self._coord_display.content.controls[0].value = f"Longitude: {dataset.bbox.west:.2f}¬∞ to {dataset.bbox.east:.2f}¬∞"
            self._coord_display.content.controls[1].value = f"Latitude: {dataset.bbox.south:.2f}¬∞ to {dataset.bbox.north:.2f}¬∞"
            self._custom_fields.visible = False
            self._coord_display.visible = True
        
        # Populate dates
        self._start_date = dataset.date_start
        self._end_date = dataset.date_end
        self._date_start_label.value = dataset.date_start.strftime("%b %d, %Y")
        self._date_end_label.value = dataset.date_end.strftime("%b %d, %Y")
        
        # Populate day filter
        self._day_selector.value = dataset.day_filter
        
        # Populate hour range
        if dataset.hour_filter:
            self._hour_start.value = str(min(dataset.hour_filter))
            self._hour_end.value = str(max(dataset.hour_filter))
        
        # Populate quality filters
        self._max_cloud.value = dataset.max_cloud
        self._max_sza.value = dataset.max_sza
        # Update labels
        self._on_cloud_change(type('obj', (object,), {'control': self._max_cloud})())
        self._on_sza_change(type('obj', (object,), {'control': self._max_sza})())
        
        self._status_log.add_info(f"Loaded dataset: {dataset.name}")
        self._status_log.add_info(f"Tip: Change the date range to add new data")
        
        self._update_preview()
        self.update()
    
    def _on_region_change(self, e):
        """Handle region dropdown change."""
        value = e.control.value
        if value == "custom":
            # Show custom input fields
            self._custom_fields.visible = True
            self._coord_display.visible = False
        elif value in REGION_PRESETS:
            bbox, _ = REGION_PRESETS[value]
            self._bbox = [bbox.west, bbox.south, bbox.east, bbox.north]
            # Update display
            self._coord_display.content.controls[0].value = f"Longitude: {bbox.west:.2f}¬∞ to {bbox.east:.2f}¬∞"
            self._coord_display.content.controls[1].value = f"Latitude: {bbox.south:.2f}¬∞ to {bbox.north:.2f}¬∞"
            # Hide custom fields, show display
            self._custom_fields.visible = False
            self._coord_display.visible = True
        
        self._update_preview()
        self._update_map_coord_label()
        # Generate map preview asynchronously to avoid blocking UI
        if self.page:
            self.page.run_task(self._update_map_preview_async)
        self.update()
    
    
    def _on_save_map_click(self, e):
        """Save the current map preview to Downloads."""
        if not self._map_image_control.src:
             self._status_log.add_warning("No map to save")
             return
             
        try:
            import shutil
            import time
            
            src_path = Path(self._map_image_control.src)
            if not src_path.exists():
                self._status_log.add_error("Map file source not found")
                return
                
            # Get Downloads folder safely
            downloads_path = Path.home() / "Downloads"
            if not downloads_path.exists():
                 # Fallback to home
                 downloads_path = Path.home()
            
            timestamp = int(time.time())
            filename = f"TEMPO_Map_Preview_{timestamp}.png"
            dest_path = downloads_path / filename
            
            shutil.copy2(src_path, dest_path)
            
            self._status_log.add_success(f"Map saved to: {dest_path}")
            
        except Exception as e:
            self._status_log.add_error(f"Failed to save map: {e}")

    async def _update_map_preview_async(self):

        """Generate map preview asynchronously with two-stage loading."""
        # Capture current bbox to avoid race conditions
        current_bbox = list(self._bbox)
        
        # Show progress
        if hasattr(self, '_map_progress'):
            self._map_progress.visible = True
            self.update()
        
        # Stage 1: Generate base map immediately (fast)
        path_base = await asyncio.to_thread(
            self._generate_map_preview, 
            bbox=current_bbox, 
            detailed=False
        )
        
        # Update UI with base map - Use image control directly
        if path_base and not path_base.startswith("Error"):
             self._map_image_control.src = path_base
             self._map_image_control.update()
        
        # Stage 2: Generate detailed map with roads (slower)
        path_detailed = await asyncio.to_thread(
            self._generate_map_preview, 
            bbox=current_bbox, 
            detailed=True
        )
        
        # Update UI with detailed map
        if path_detailed and not path_detailed.startswith("Error"):
             self._map_image_control.src = path_detailed
             self._map_image_control.update()
        elif path_detailed:
             # Show error log
             print(f"Map Error: {path_detailed}")
             self._status_log.add_error(f"Map Error: {path_detailed}")
            
        # Hide progress
        if hasattr(self, '_map_progress'):
            self._map_progress.visible = False
            self.update()
    
    def _on_cloud_change(self, e):
        """Update cloud fraction label."""
        val = e.control.value
        percent = int(val * 100)
        if val <= 0.2:
            desc = "(strict - high quality)"
        elif val <= 0.4:
            desc = "(recommended balance)"
        elif val <= 0.6:
            desc = "(lenient - more data)"
        else:
            desc = "(very lenient - may include cloudy pixels)"
        self._cloud_label.value = f"{val:.2f} ({percent}% cloud cover) {desc}"
        self._update_preview()
        self.update()
    
    def _on_sza_change(self, e):
        """Update SZA label."""
        val = int(e.control.value)
        if val <= 60:
            desc = "(strict - midday observations only)"
        elif val <= 70:
            desc = "(excludes very early/late observations)"
        elif val <= 80:
            desc = "(includes early morning/late afternoon)"
        else:
            desc = "(includes near-horizon observations)"
        self._sza_label.value = f"{val}¬∞ {desc}"
        self._update_preview()
        self.update()
    
    def _open_start_picker(self, e):
        """Open start date picker."""
        self._start_picker.value = self._start_date
        self._start_picker.open = True
        self._start_picker.update()
    
    def _open_end_picker(self, e):
        """Open end date picker."""
        self._end_picker.value = self._end_date
        self._end_picker.open = True
        self._end_picker.update()
    
    def _on_start_date_change(self, e):
        """Handle start date selection."""
        if e.control.value:
            self._start_date = e.control.value
            self._date_start_label.value = self._start_date.strftime("%b %d, %Y")
            self._update_preview()

            self.update()
    
    def _on_end_date_change(self, e):
        """Handle end date selection."""
        if e.control.value:
            self._end_date = e.control.value
            self._date_end_label.value = self._end_date.strftime("%b %d, %Y")
            self._update_preview()

            self.update()
    
    def _on_day_change(self, days: list[int]):
        """Handle day selection change."""
        self._update_preview()

        if hasattr(self, 'update'):
            self.update()
    
    def _on_hour_change(self, e):
        """Handle hour dropdown change."""
        self._update_preview()

        self.update()

    def _update_preview(self):
        """Update the dataset preview text."""
        try:
            # Use stored dates
            start = self._start_date
            end = self._end_date
            
            # Get selected days from DaySelector
            days = self._day_selector.value
            day_names = ["M", "T", "W", "T", "F", "S", "S"]
            day_str = "".join(day_names[d] for d in days) if len(days) < 7 else "all days"
            if days == [0, 1, 2, 3, 4]:
                day_str = "weekdays"
            elif days == [5, 6]:
                day_str = "weekends"
            
            total_days = 0
            current = start
            while current <= end:
                if current.weekday() in days:
                    total_days += 1
                current += timedelta(days=1)
            
            # Count hours per day
            hour_start = int(self._hour_start.value)
            hour_end = int(self._hour_end.value)
            hours_per_day = max(0, hour_end - hour_start + 1)
            
            # Total granules
            total_granules = total_days * hours_per_day
            
            # Format preview - clean and organized
            self._preview_text.value = (
                f"Date Range: {start} to {end} ({(end - start).days + 1} days)\n"
                f"Day Filter: {day_str} ({total_days} matching days)\n"
                f"Hours: {hour_start:02d}:00 - {hour_end:02d}:00 UTC ({hours_per_day}/day)\n"
                f"Total Granules: {total_granules:,}"
            )
        except Exception as e:
            self._preview_text.value = f"‚ö†Ô∏è Error calculating preview: {e}"
    
    def _on_download_click(self, e):
        """Start the download process."""
        if self._is_downloading:
            return
        
        # Validate inputs
        name = self._name_field.value.strip()
        if not name:
            self._status_log.add_error("Dataset name is required!")
            return
        
        # Check for duplicate name (only in new mode)
        if not self._extend_mode:
            if self.db.get_dataset_by_name(name):
                self._status_log.add_error(f"A dataset named '{name}' already exists!")
                return
        else:
            # Extend mode: verify we have a dataset loaded
            if not self._extend_dataset_id:
                self._status_log.add_error("Please load a dataset first!")
                return
        
        # Start download
        self._is_downloading = True
        self._download_btn.disabled = True
        self._cancel_btn.visible = True
        self._progress_panel.show()
        
        # Register with global download manager
        import uuid
        self._current_download_id = str(uuid.uuid4())
        download_manager = self._get_download_manager()
        if download_manager:
            download_manager.add_download(self._current_download_id, name)
        
        if self._extend_mode:
            self._status_log.add_info(f"Extending dataset: {name}")
        else:
            self._status_log.add_info(f"Starting dataset creation: {name}")
        
        # Run download in background
        self.page.run_task(self._run_download)
        self.update()
    
    async def _run_download(self):
        """Run the download process asynchronously."""
        try:
            # Parse form values
            name = self._name_field.value.strip()
            
            # Get bbox - from custom fields if custom mode, else from stored values
            if self._region_dropdown.value == "custom":
                bbox = BoundingBox(
                    west=float(self._west_field.value),
                    south=float(self._south_field.value),
                    east=float(self._east_field.value),
                    north=float(self._north_field.value),
                )
            else:
                bbox = BoundingBox(
                    west=self._bbox[0],
                    south=self._bbox[1],
                    east=self._bbox[2],
                    north=self._bbox[3],
                )
            
            # Ensure dates are date objects (not datetime) for consistent comparison
            start_date = self._start_date if isinstance(self._start_date, date) and not isinstance(self._start_date, datetime) else self._start_date.date() if hasattr(self._start_date, 'date') else self._start_date
            end_date = self._end_date if isinstance(self._end_date, date) and not isinstance(self._end_date, datetime) else self._end_date.date() if hasattr(self._end_date, 'date') else self._end_date
            
            day_filter = self._day_selector.value
            
            hour_start = int(self._hour_start.value)
            hour_end = int(self._hour_end.value)
            hour_filter = list(range(hour_start, hour_end + 1))
            
            max_cloud = float(self._max_cloud.value)
            max_sza = float(self._max_sza.value)
            
            # Create or get existing dataset record
            if self._extend_mode and self._extend_dataset_id:
                # Extend mode: use existing dataset
                self._status_log.add_info(f"Loading existing dataset...")
                dataset = self.db.get_dataset(self._extend_dataset_id)
                if not dataset:
                    raise ValueError(f"Dataset {self._extend_dataset_id} not found")
                
                # Update date range to include new dates
                if start_date < dataset.date_start:
                    dataset.date_start = start_date
                if end_date > dataset.date_end:
                    dataset.date_end = end_date
                dataset.status = DatasetStatus.DOWNLOADING
                self.db.update_dataset(dataset)
                self._status_log.add_success(f"Extending dataset: {dataset.name}")
            else:
                # New mode: create new dataset
                dataset = Dataset(
                    id="",  # Will be generated
                    name=name,
                    created_at=datetime.now(),
                    bbox=bbox,
                    date_start=start_date,
                    date_end=end_date,
                    day_filter=day_filter,
                    hour_filter=hour_filter,
                    max_cloud=max_cloud,
                    max_sza=max_sza,
                    status=DatasetStatus.DOWNLOADING,
                )
                
                self._status_log.add_info(f"Creating dataset record...")
                dataset = self.db.create_dataset(dataset)
                self._status_log.add_success(f"Dataset ID: {dataset.id[:8]}...")
            
            # Generate granule list
            self._status_log.add_info("Calculating required granules...")
            granules = []
            current_date = start_date
            while current_date <= end_date:
                if current_date.weekday() in day_filter:
                    for hour in hour_filter:
                        granule = Granule(
                            dataset_id=dataset.id,
                            date=current_date,
                            hour=hour,
                            bbox_west=bbox.west,
                            bbox_south=bbox.south,
                            bbox_east=bbox.east,
                            bbox_north=bbox.north,
                            max_cloud=max_cloud,
                            max_sza=max_sza,
                        )
                        granules.append(granule)
                current_date += timedelta(days=1)
            
            self._status_log.add_success(f"Found {len(granules)} granules to download")
            
            # Save granule records
            self.db.create_granules_batch(granules)
            # Accumulate granule count if extending, otherwise set it
            current_count = dataset.granule_count or 0
            dataset.granule_count = current_count + len(granules)
            self.db.update_dataset(dataset)
            
            # Download all granules
            self._status_log.add_info(f"üì° Downloading {len(granules)} granules...")
            
            # Show worker progress panel
            self._worker_progress.show(total=len(granules))
            self.update()
            
            # Build date-hour list for downloader
            date_set = set()
            hour_set = set()
            for g in granules:
                date_set.add(g.date.isoformat())
                hour_set.add(g.hour)
            
            dates_list = sorted(date_set)
            hours_list = sorted(hour_set)
            
            # Create datasets folder with dataset name (sanitize for filesystem)
            safe_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in name)
            datasets_dir = self.db.db_path.parent / "datasets" / safe_name
            datasets_dir.mkdir(parents=True, exist_ok=True)
            
            # Initialize downloader with the named dataset directory and configured workers
            num_workers = self.config.download_workers if self.config else 4
            api_key = self.config.rsig_api_key if self.config else ""
            downloader = RSIGDownloader(datasets_dir, max_concurrent=num_workers, api_key=api_key)
            adapter = UIStatusAdapter(
                self._status_log, 
                self._progress_panel,
                worker_panel=self._worker_progress,
                total_granules=len(granules),
                num_workers=num_workers
            )
            
            # Single call to download all granules (parallel execution)
            new_files = await downloader.download_granules(
                dates=dates_list,
                hours=hours_list,
                bbox=[bbox.west, bbox.south, bbox.east, bbox.north],
                dataset_name=safe_name,
                max_cloud=max_cloud,
                max_sza=max_sza,
                status=adapter
            )
            
            # Update progress panel to complete
            self._worker_progress.complete(len(new_files), len(granules))
            
            if not self._is_downloading:
                dataset.status = DatasetStatus.PARTIAL
                self.db.update_dataset(dataset)
                return

            # Update dataset count - use total existing files
            # Gather ALL granule files (existing + new) for processing
            all_files = list(datasets_dir.glob("tempo_*.nc"))
            
            dataset.granules_downloaded = len(all_files)
            self.db.update_dataset(dataset)

            # Processing Step - FNR Calculation
            self._progress_panel.update_progress(0.9, "Processing data...")
            self._status_log.add_info("‚öôÔ∏è Processing hourly averages and FNR...")
            
            # Use ALL files for processing
            # all_files is already set above
            
            if not all_files:
                 self._status_log.add_error("No files found to process.")
                 dataset.status = DatasetStatus.ERROR
                 self.db.update_dataset(dataset)
                 return

            try:
                # Process Data
                ds_avg = await asyncio.to_thread(DataProcessor.process_dataset, all_files)
                
                if ds_avg is None:
                    self._status_log.add_error("Processing failed (no valid data returned).")
                    dataset.status = DatasetStatus.ERROR
                else:
                    # Save processed NetCDF file in the named datasets folder
                    try:
                        output_path = datasets_dir / f"{safe_name}_processed.nc"
                        await asyncio.to_thread(DataProcessor.save_processed, ds_avg, output_path)
                        
                        dataset.file_path = str(output_path)
                        # Calculate file size in MB
                        if output_path.exists():
                            dataset.file_size_mb = output_path.stat().st_size / (1024 * 1024)
                        dataset.granules_downloaded = len(granules)
                        dataset.status = DatasetStatus.COMPLETE
                        self._status_log.add_success("‚úì Processing complete!")
                        self._progress_panel.update_progress(1.0, "Complete!")
                    except Exception as e:
                         self._status_log.add_error(f"Save processed file failed: {e}")
                         dataset.status = DatasetStatus.ERROR
                    
            except Exception as e:
                self._status_log.add_error(f"Processing error: {e}")
                dataset.status = DatasetStatus.ERROR
                
            self.db.update_dataset(dataset)
            
            if dataset.status == DatasetStatus.COMPLETE:
                # Mark all granules as downloaded
                self.db.mark_granules_downloaded(dataset.id)
                self._status_log.add_success(f"üéâ Dataset '{name}' created successfully!")
                self._status_log.add_info(f"   ‚îî‚îÄ {len(granules)} granules processed")
                
                # Update global download manager
                download_manager = self._get_download_manager()
                if download_manager and self._current_download_id:
                    download_manager.complete(self._current_download_id)
                
                # Auto-navigate to Workspace after a short delay
                await asyncio.sleep(1.5)
                shell = self._get_shell()
                if shell and hasattr(shell, 'navigate_to'):
                    shell.navigate_to(f"/workspace/{dataset.id}")
            else:
                # Mark as error in download manager
                download_manager = self._get_download_manager()
                if download_manager and self._current_download_id:
                    download_manager.error(self._current_download_id)
            
        except Exception as e:
            self._status_log.add_error(f"Download failed: {e}")
            import traceback
            traceback.print_exc()
            # Mark as error in download manager
            download_manager = self._get_download_manager()
            if download_manager and self._current_download_id:
                download_manager.error(self._current_download_id)
        
        finally:
            self._is_downloading = False
            self._download_btn.disabled = False
            self._cancel_btn.visible = False
            self._current_download_id = None
            self.update()
    
    def _on_cancel_click(self, e):
        """Cancel the download."""
        self._is_downloading = False
        self._status_log.add_warning("Cancelling download...")
        
        # Cancel in download manager
        download_manager = self._get_download_manager()
        if download_manager and self._current_download_id:
            download_manager.cancel(self._current_download_id)
        
        self.update()


========================================
FILE: src/tempo_app/ui/pages/export.py
========================================
"""Export Page - Dedicated data export interface."""

import flet as ft
from pathlib import Path
import asyncio
import pandas as pd
import xarray as xr
import numpy as np
from typing import Optional
import logging

from ..theme import Colors, Spacing, Typography
from ...storage.database import Database
from ...storage.models import Dataset, Site
from ...core.exporter import DataExporter
from ..components.widgets import StatusLogPanel

class ExportPage(ft.Container):
    """Page for exporting dataset data to Excel files."""

    def __init__(self, db: Database, data_dir: Path, dataset_id: str = None):
        super().__init__()
        self.db = db
        self.data_dir = data_dir
        self.dataset_id = dataset_id # Can be passed from navigation
        self.exporter = DataExporter(data_dir)
        
        # State
        self._dataset: Optional[Dataset] = None
        self._sites: list[Site] = []
        
        self.expand = True
        self.padding = 0
        self.bgcolor = Colors.BACKGROUND
        
        self._build()

    def did_mount(self):
        """Load data when mounted."""
        self.page.run_task(self._load_initial_data)

    async def _load_initial_data(self):
        """Load datasets and initial state."""
        # Load all datasets for dropdown
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        
        options = []
        for ds in datasets:
            # Only show completed datasets? Or all? User req: "Complete"
            # Assuming all for now, maybe filter by status if we had it.
            options.append(ft.DropdownOption(key=ds.id, text=ds.name))
        self._dataset_dropdown.options = options

        # Pre-select if ID provided, or defaults
        if self.dataset_id:
            # Validate it exists
            if any(d.id == self.dataset_id for d in datasets):
                self._dataset_dropdown.value = self.dataset_id
                await self._load_selected_dataset()
            else:
                self.dataset_id = None
        
        if not self.dataset_id and options:
            # Optional: Select first by default or leave empty
            # self._dataset_dropdown.value = options[0].key
            # await self._load_selected_dataset()
            pass
            
        self.update()

    async def _load_selected_dataset(self):
        """Load the full dataset object and update UI."""
        ds_id = self._dataset_dropdown.value
        logging.info(f"ExportPage: Loading dataset ID {ds_id}")
        self._status_log.add_info(f"Loading dataset info for ID: {ds_id}")
        self.update()
        
        if not ds_id:
            logging.warning("ExportPage: No dataset ID selected")
            return

        self._dataset = await asyncio.to_thread(self.db.get_dataset, ds_id)
        if self._dataset:
            # Update info text
            self._dataset_info.value = f"Selected: {self._dataset.name}\n" \
                                       f"Range: {self._dataset.date_start} to {self._dataset.date_end}"
            
            # Load sites
            logging.info(f"ExportPage: Loading sites in bbox {self._dataset.bbox}")
            self._sites = await asyncio.to_thread(
                self.db.get_sites_in_bbox, self._dataset.bbox
            )
            logging.info(f"ExportPage: Found {len(self._sites)} sites")
            
            # Update Preview
            await self._update_preview()
            
        else:
            logging.error(f"ExportPage: Dataset {ds_id} returned None from DB")
            self._status_log.add_error(f"Failed to load dataset {ds_id}")
            
        self.update()

    def _is_valid_value(self, val) -> bool:
        """Check if a value is valid (not NaN and not a fill value)."""
        if pd.isna(val):
            return False
        # Filter out common fill values (large negative numbers)
        # Many datasets use values like -9.99e36 or -999 as fill values
        if val < -1e30:  # Filter out extremely large negative values
            return False
        if val < -900:  # Filter out common fill values like -999
            return False
        # Also filter extremely large positive values (likely fill values)
        if val > 1e30:
            return False
        # NO2/HCHO values should reasonably be in range 1e13 to 1e18 molecules/cm2
        # Values outside this range are suspect
        if abs(val) > 1e20:
            return False
        return True

    async def _update_preview(self):
        """Update the data preview table based on current config."""
        if not self._dataset:
            self._preview_table.rows = []
            return

        logging.info("ExportPage: Updating preview...")
        self._status_log.add_info("Updating preview...")
        self._progress.visible = True
        self.update()
        
        try:
            # Find processed file
            if self._dataset.file_path and Path(self._dataset.file_path).exists():
                processed_path = Path(self._dataset.file_path)
            else:
                # Fallback path logic
                safe_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in self._dataset.name)
                processed_path = self.data_dir / "datasets" / safe_name / f"{safe_name}_processed.nc"

            logging.info(f"ExportPage: Looking for processed file at {processed_path}")

            if not processed_path.exists():
                logging.error(f"ExportPage: Processed file not found at {processed_path}")
                self._status_log.add_error(f"Data file not found at {processed_path}")
                self._progress.visible = False
                self.update()
                return

            # Open with xarray (lazy)
            ds = await asyncio.to_thread(xr.open_dataset, processed_path)
            
            # Depending on format, show different columns
            fmt = self._format_radio.value
            utc_offset = float(self._utc_offset.value)
            
            # Get spatial aggregation settings
            spatial_mode = self._spatial_mode.value  # "points" or "radius"
            try:
                spatial_value = float(self._spatial_value.value)
            except ValueError:
                spatial_value = 9.0
            
            preview_rows = []
            columns = []
            
            # Get first site info for cell selection
            site_lat = self._sites[0].latitude if self._sites else None
            site_lon = self._sites[0].longitude if self._sites else None
            site_code = self._sites[0].code if self._sites else "N/A"
            
            # Determine time dimension
            time_dim = None
            time_values = None
            if 'TIME' in ds.dims:
                time_dim = 'TIME'
                time_values = pd.to_datetime(ds.TIME.values)
            elif 'TSTEP' in ds.dims:
                time_dim = 'TSTEP'
                time_values = pd.to_datetime(ds.TSTEP.values)
            elif 'HOUR' in ds.dims:
                time_dim = 'HOUR'
                time_values = ds.HOUR.values
            
            # Get coordinate arrays
            lats = ds['LAT'].values if 'LAT' in ds.coords else None
            lons = ds['LON'].values if 'LON' in ds.coords else None
            
            # Find cells based on spatial settings
            selected_cells = []  # List of (row, col) tuples
            
            if lats is not None and lons is not None and site_lat is not None and site_lon is not None:
                # Calculate distances from site to all grid cells
                from math import radians, sin, cos, sqrt, atan2
                
                def haversine(lat1, lon1, lat2, lon2):
                    R = 6371.0  # Earth radius in km
                    lat1_rad, lon1_rad = radians(lat1), radians(lon1)
                    lat2_rad, lon2_rad = radians(lat2), radians(lon2)
                    dlat = lat2_rad - lat1_rad
                    dlon = lon2_rad - lon1_rad
                    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2
                    c = 2 * atan2(sqrt(a), sqrt(1 - a))
                    return R * c
                
                # Calculate distances for all cells
                cell_distances = []
                for i in range(lats.shape[0]):
                    for j in range(lats.shape[1]):
                        dist = haversine(site_lat, site_lon, lats[i, j], lons[i, j])
                        cell_distances.append((i, j, dist))
                
                # Sort by distance
                cell_distances.sort(key=lambda x: x[2])
                
                if spatial_mode == "points":
                    # Take N nearest cells
                    n_points = int(spatial_value)
                    selected_cells = [(r, c) for r, c, _ in cell_distances[:n_points]]
                else:  # radius
                    # Take all cells within distance
                    selected_cells = [(r, c) for r, c, d in cell_distances if d <= spatial_value]
                    if not selected_cells:
                        # If no cells within radius, take at least 1
                        selected_cells = [(cell_distances[0][0], cell_distances[0][1])]
            else:
                # Fallback to center cell
                if lats is not None:
                    row_idx = lats.shape[0] // 2
                    col_idx = lats.shape[1] // 2
                else:
                    row_idx, col_idx = 0, 0
                selected_cells = [(row_idx, col_idx)]
            
            logging.info(f"ExportPage: Using {len(selected_cells)} cells for preview (mode={spatial_mode}, value={spatial_value})")
            
            # Limit to first 10 timesteps
            num_preview = min(10, len(time_values) if time_values is not None else 0)

            
            if fmt == "hourly":
                n_cells = len(selected_cells)
                use_fill = self._use_fill.value
                
                # Build columns to match export format: UTC_Time, Local_Time, Cell1_NO2, Cell1_HCHO, ...
                columns = ["UTC_Time", f"Local_Time (UTC{utc_offset:+.1f})"]
                for i in range(min(n_cells, 3)):  # Limit preview to first 3 cells
                    columns.extend([f"Cell{i+1}_NO2", f"Cell{i+1}_HCHO"])
                if n_cells > 3:
                    columns.append(f"... +{n_cells - 3} more")
                
                for t_idx in range(num_preview):
                    # Get time
                    if time_values is not None:
                        t = time_values[t_idx]
                        if isinstance(t, (pd.Timestamp, np.datetime64)):
                            t = pd.Timestamp(t)
                            local_t = t + pd.Timedelta(hours=utc_offset)
                            utc_str = t.strftime("%Y-%m-%d %H:%M")
                            local_str = local_t.strftime("%Y-%m-%d %H:%M")
                        else:
                            utc_str = f"{int(t):02d}:00 UTC"
                            local_h = (int(t) + int(utc_offset)) % 24
                            local_str = f"{local_h:02d}:00 Local"
                    else:
                        utc_str = f"T{t_idx}"
                        local_str = f"T{t_idx}"
                    
                    cells = [
                        ft.DataCell(ft.Text(utc_str, color=Colors.ON_SURFACE)),
                        ft.DataCell(ft.Text(local_str, color=Colors.ON_SURFACE)),
                    ]
                    
                    # Add per-cell values (first 3 cells)
                    for cell_idx, (row_idx, col_idx) in enumerate(selected_cells[:3]):
                        no2_str = "-999"
                        hcho_str = "-999"
                        
                        if 'NO2_TropVCD' in ds:
                            try:
                                val = ds['NO2_TropVCD'].isel(**{time_dim: t_idx}, ROW=row_idx, COL=col_idx).values.item()
                                if self._is_valid_value(val):
                                    no2_str = f"{val:.2e}"
                            except Exception:
                                pass
                        
                        if 'HCHO_TotVCD' in ds:
                            try:
                                val = ds['HCHO_TotVCD'].isel(**{time_dim: t_idx}, ROW=row_idx, COL=col_idx).values.item()
                                if self._is_valid_value(val):
                                    hcho_str = f"{val:.2e}"
                            except Exception:
                                pass
                        
                        cells.extend([
                            ft.DataCell(ft.Text(no2_str, color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(hcho_str, color=Colors.ON_SURFACE)),
                        ])
                    
                    if n_cells > 3:
                        cells.append(ft.DataCell(ft.Text("...", color=Colors.ON_SURFACE_VARIANT)))
                    
                    preview_rows.append(ft.DataRow(cells=cells))
            
            elif fmt == "daily":
                min_hours = int(self._min_hours_slider.value)
                use_fill = self._use_fill.value
                n_cells = len(selected_cells)
                spatial_mode = self._spatial_mode.value
                
                # Simpler column names for radius mode (include km in name)
                if spatial_mode == "radius":
                    try:
                        radius_km = float(self._spatial_value.value)
                    except ValueError:
                        radius_km = 10.0
                    radius_str = f"{int(radius_km)}km" if radius_km == int(radius_km) else f"{radius_km}km"
                    
                    columns = [
                        "Date", "Site",
                        f"TEMPO_NoFill_NO2_{radius_str}", f"TEMPO_NoFill_NO2_Cnt",
                        f"TEMPO_NoFill_HCHO_{radius_str}", f"TEMPO_NoFill_HCHO_Cnt"
                    ]
                    if use_fill:
                        columns.extend([
                            f"TEMPO_Fill_NO2_{radius_str}", f"TEMPO_Fill_NO2_Cnt",
                            f"TEMPO_Fill_HCHO_{radius_str}", f"TEMPO_Fill_HCHO_Cnt"
                        ])
                else:
                    # N-points mode: include cell count in column name
                    columns = [
                        "Date", "Site",
                        f"NO2_NoFill_{n_cells}_Avg", f"NO2_NoFill_{n_cells}_Cnt",
                        f"HCHO_NoFill_{n_cells}_Avg", f"HCHO_NoFill_{n_cells}_Cnt"
                    ]
                    if use_fill:
                        columns.extend([
                            f"NO2_Fill_{n_cells}_Avg", f"NO2_Fill_{n_cells}_Cnt",
                            f"HCHO_Fill_{n_cells}_Avg", f"HCHO_Fill_{n_cells}_Cnt"
                        ])
                
                # Group by date and compute daily averages
                if time_values is not None and len(time_values) > 0:
                    # Collect all values by date
                    daily_data = {}  # date -> {'no2_vals': [], 'hcho_vals': []}
                    
                    for t_idx in range(len(time_values)):
                        t = time_values[t_idx]
                        if isinstance(t, (pd.Timestamp, np.datetime64)):
                            t = pd.Timestamp(t)
                            local_t = t + pd.Timedelta(hours=utc_offset)
                            date_str = local_t.strftime("%Y-%m-%d")
                        else:
                            date_str = "Unknown"
                        
                        if date_str not in daily_data:
                            daily_data[date_str] = {'no2_vals': [], 'hcho_vals': [], 'hours': 0}
                        
                        daily_data[date_str]['hours'] += 1
                        
                        # Collect values from all selected cells for this timestep
                        for row_idx, col_idx in selected_cells:
                            if 'NO2_TropVCD' in ds:
                                try:
                                    val = ds['NO2_TropVCD'].isel(**{time_dim: t_idx}, ROW=row_idx, COL=col_idx).values.item()
                                    if self._is_valid_value(val):
                                        daily_data[date_str]['no2_vals'].append(val)
                                except Exception:
                                    pass
                            if 'HCHO_TotVCD' in ds:
                                try:
                                    val = ds['HCHO_TotVCD'].isel(**{time_dim: t_idx}, ROW=row_idx, COL=col_idx).values.item()
                                    if self._is_valid_value(val):
                                        daily_data[date_str]['hcho_vals'].append(val)
                                except Exception:
                                    pass
                    
                    # Build rows matching export format
                    sorted_dates = sorted(daily_data.keys())[:10]  # First 10 days
                    
                    for date_str in sorted_dates:
                        data = daily_data[date_str]
                        hours = data['hours']
                        
                        # NoFill: only if hours >= min_hours
                        if hours >= min_hours:
                            no2_avg = np.nanmean(data['no2_vals']) if data['no2_vals'] else -999
                            hcho_avg = np.nanmean(data['hcho_vals']) if data['hcho_vals'] else -999
                            no2_cnt = len(data['no2_vals'])
                            hcho_cnt = len(data['hcho_vals'])
                        else:
                            no2_avg = -999  # MISSING_VALUE indicator
                            hcho_avg = -999
                            no2_cnt = 0
                            hcho_cnt = 0
                        
                        # Format values
                        no2_avg_str = f"{no2_avg:.2e}" if no2_avg != -999 else "-999"
                        hcho_avg_str = f"{hcho_avg:.2e}" if hcho_avg != -999 else "-999"
                        
                        cells = [
                            ft.DataCell(ft.Text(date_str, color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(site_code, color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(no2_avg_str, color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(str(no2_cnt), color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(hcho_avg_str, color=Colors.ON_SURFACE)),
                            ft.DataCell(ft.Text(str(hcho_cnt), color=Colors.ON_SURFACE)),
                        ]
                        
                        if use_fill:
                            # Fill: always compute (even if < min_hours)
                            # For preview, show that Fill will have >= NoFill counts
                            # Real export uses monthly-hourly means which will fill more gaps
                            no2_fill_avg = np.nanmean(data['no2_vals']) if data['no2_vals'] else -999
                            hcho_fill_avg = np.nanmean(data['hcho_vals']) if data['hcho_vals'] else -999
                            
                            # In actual export, fill counts will be >= nofill counts
                            # Show with + indicator to indicate fill will add values
                            no2_fill_cnt = len(data['no2_vals'])
                            hcho_fill_cnt = len(data['hcho_vals'])
                            
                            no2_fill_str = f"{no2_fill_avg:.2e}" if no2_fill_avg != -999 else "-999"
                            hcho_fill_str = f"{hcho_fill_avg:.2e}" if hcho_fill_avg != -999 else "-999"
                            
                            # Show counts with + to indicate fill will have more
                            no2_cnt_str = f"{no2_fill_cnt}+" if no2_fill_cnt > 0 else "0"
                            hcho_cnt_str = f"{hcho_fill_cnt}+" if hcho_fill_cnt > 0 else "0"
                            
                            cells.extend([
                                ft.DataCell(ft.Text(no2_fill_str, color=Colors.ON_SURFACE)),
                                ft.DataCell(ft.Text(no2_cnt_str, color=Colors.ON_SURFACE)),
                                ft.DataCell(ft.Text(hcho_fill_str, color=Colors.ON_SURFACE)),
                                ft.DataCell(ft.Text(hcho_cnt_str, color=Colors.ON_SURFACE)),
                            ])
                        
                        preview_rows.append(ft.DataRow(cells=cells))

            # Update DataTable columns and rows
            logging.info(f"ExportPage: Setting {len(columns)} columns and {len(preview_rows)} rows for format '{fmt}'")
            self._preview_table.columns = [ft.DataColumn(ft.Text(c, color=Colors.ON_SURFACE)) for c in columns]
            self._preview_table.rows = preview_rows
            self._preview_table.update()  # Explicitly update the table
            
            ds.close()
            self._status_log.add_info(f"Preview updated for {fmt} - showing {len(preview_rows)} rows")
            logging.info(f"ExportPage: Preview updated successfully for {fmt}")

        except Exception as e:
            import traceback
            logging.error(f"ExportPage: Preview error: {e}")
            logging.error(traceback.format_exc())
            self._status_log.add_error(f"Preview error: {e}")
        
        self._progress.visible = False
        self.update()

    def _on_dataset_change(self, e):
        """Handle dataset dropdown change."""
        print(f"DEBUG: _on_dataset_change triggered. Value: {self._dataset_dropdown.value}")
        self._status_log.add_info("Selection changed...")
        self.update()
        self.page.run_task(self._load_selected_dataset)

    def _on_param_change(self, e):
        """Handle parameter changes to update preview."""
        logging.info(f"ExportPage: _on_param_change triggered, format={self._format_radio.value}")
        print(f"DEBUG: _on_param_change triggered, format={self._format_radio.value}")
        self.page.run_task(self._update_preview)

    def _on_min_hours_change(self, e):
        """Handle min hours slider change."""
        val = int(self._min_hours_slider.value)
        self._min_hours_label.value = f"Min Hours for Daily (NOFILL): {val}"
        self._min_hours_label.update()
        self.page.run_task(self._update_preview)

    def _on_export_click(self, e):
        """Run the export process."""
        self.page.run_task(self._export_async)

    async def _export_async(self):
        """Async implementation of export process."""
        if not self._dataset:
            self._status_log.add_error("Please select a dataset first.")
            return

        self._export_btn.disabled = True
        self._progress.visible = True
        self.update()
        
        try:
            # Prepare args
            ds_name = self._dataset.name
            fmt = self._format_radio.value
            utc_off = float(self._utc_offset.value)
            
            # Map radio selection to internal format strings
            format_map = {
                "hourly": "hourly",
                "daily": "daily",
            }
            export_fmt = format_map.get(fmt, "hourly")
            
            # Spatial mode
            mode = self._spatial_mode.value
            val = float(self._spatial_value.value)
            
            num_points = int(val) if mode == "points" else None
            distance = val if mode == "radius" else None
            
            self._status_log.add_info(f"Starting export: {ds_name} ({fmt})...")
            
            # Find file
            if self._dataset.file_path and Path(self._dataset.file_path).exists():
                processed_path = Path(self._dataset.file_path)
            else:
                 # Fallback path logic
                safe_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in self._dataset.name)
                processed_path = self.data_dir / "datasets" / safe_name / f"{safe_name}_processed.nc"
                
            if not processed_path.exists():
                raise FileNotFoundError(f"Processed file not found: {processed_path}")

            # Run in thread
            ds = await asyncio.to_thread(xr.open_dataset, processed_path)
            
            # Get sites dict
            sites_dict = {s.code: s.to_tuple() for s in self._sites} if self._sites else None

            metadata = {
                "dataset_id": self._dataset.id,
                "exported_by": "Tempo Analyzer",
                "spatial_mode": mode,
                "spatial_value": val
            }

            files = await asyncio.to_thread(
                self.exporter.export_dataset,
                dataset=ds,
                dataset_name=ds_name,
                export_format=export_fmt,
                utc_offset=utc_off,
                num_points=num_points,
                distance_km=distance,
                sites=sites_dict,
                metadata=metadata
            )
            
            ds.close()
            
            if files:
                self._status_log.add_success(f"Export successful! Created {len(files)} files.")
                self._status_log.add_success(f"Location: {self.exporter.output_dir}")
                # Ideally show a button to open folder
            else:
                self._status_log.add_warning("No files created.")

        except Exception as ex:
             import traceback
             traceback.print_exc()
             self._status_log.add_error(f"Export failed: {ex}")
        
        self._export_btn.disabled = False
        self._progress.visible = False
        self.update()


    def _build(self):
        """Build the UI layout."""
        
        # 1. Configuration Panel (Left)
        # -----------------------------
        
        # Dataset Selection
        self._dataset_dropdown = ft.Dropdown(
            label="Select Dataset",
            text_size=14,
            width=300,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
            on_select=self._on_dataset_change,
        )
        self._dataset_info = ft.Text(
            "No dataset selected", 
            size=12, 
            color=Colors.ON_SURFACE_VARIANT
        )
        
        # Format Selection
        self._format_radio = ft.RadioGroup(
            content=ft.Column([
                ft.Radio(value="hourly", label="Hourly (Per Site)", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
                ft.Radio(value="daily", label="Daily (Aggregated)", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
            ]),
            value="hourly",
            on_change=self._on_param_change
        )
        
        # Parameters
        self._utc_offset = ft.TextField(
            label="Time Zone Offset (UTC)",
            value="-6.0", 
            width=100, 
            keyboard_type=ft.KeyboardType.NUMBER,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
            on_change=self._on_param_change # Update preview on change?
        )
        
        self._spatial_mode = ft.RadioGroup(
             content=ft.Row([
                ft.Radio(value="points", label="N-Points", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
                ft.Radio(value="radius", label="Radius (km)", label_style=ft.TextStyle(color=Colors.ON_SURFACE)),
            ]),
            value="points",
            on_change=self._on_param_change
        )
        
        self._spatial_value = ft.TextField(
            label="Count / Dist",
            value="9",
            width=100,
            keyboard_type=ft.KeyboardType.NUMBER,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
            on_change=self._on_param_change
        )
        
        # Min hours slider for daily NOFILL
        self._min_hours_label = ft.Text(
            "Min Hours for Daily (NOFILL): 4",
            size=12,
            color=Colors.ON_SURFACE
        )
        self._min_hours_slider = ft.Slider(
            min=1,
            max=12,
            value=4,
            divisions=11,
            label="{value}",
            on_change=self._on_min_hours_change,
            width=200,
        )
        
        # Gap filling checkbox
        self._use_fill = ft.Checkbox(
            label="Apply Monthly Mean Fill",
            value=False,
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
            on_change=self._on_param_change
        )

        self._export_btn = ft.FilledButton(
            "Export Excel File",
            icon=ft.Icons.DOWNLOAD,
            width=300,
            on_click=self._on_export_click
        )

        config_panel = ft.Container(
            content=ft.Column([
                ft.Text("1. Source", weight=ft.FontWeight.BOLD, size=16, color=Colors.ON_SURFACE),
                self._dataset_dropdown,
                self._dataset_info,
                ft.Divider(),
                
                ft.Text("2. Format", weight=ft.FontWeight.BOLD, size=16, color=Colors.ON_SURFACE),
                self._format_radio,
                ft.Divider(),
                
                ft.Text("3. Configuration", weight=ft.FontWeight.BOLD, size=16, color=Colors.ON_SURFACE),
                self._utc_offset,
                ft.Text("Spatial Aggregation:", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                self._spatial_mode,
                self._spatial_value,
                ft.Container(height=5),
                ft.Text("Daily Settings:", size=14, weight=ft.FontWeight.W_500, color=Colors.ON_SURFACE),
                self._min_hours_label,
                self._min_hours_slider,
                self._use_fill,
                
                ft.Divider(),
                ft.Container(height=10),
                self._export_btn,
                
            ], spacing=10, scroll=ft.ScrollMode.AUTO),
            width=350,
            padding=20,
            border=ft.border.only(right=ft.BorderSide(1, Colors.DIVIDER)),
            bgcolor=Colors.SURFACE
        )


        # 2. Preview Panel (Right)
        # ------------------------
        
        self._preview_table = ft.DataTable(
            columns=[ft.DataColumn(ft.Text("Select a dataset", color=Colors.ON_SURFACE))],
            rows=[],
            border=ft.border.all(1, Colors.BORDER),
            vertical_lines=ft.border.BorderSide(1, Colors.BORDER),
            horizontal_lines=ft.border.BorderSide(1, Colors.BORDER),
            heading_row_color=Colors.SURFACE_VARIANT,
        )
        
        self._status_log = StatusLogPanel()
        self._progress = ft.ProgressBar(visible=False, color=Colors.PRIMARY)

        preview_panel = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Text("Data Preview", weight=ft.FontWeight.BOLD, size=18, color=Colors.ON_SURFACE),
                    ft.Container(expand=True),
                    ft.TextButton("Refresh Preview", icon=ft.Icons.REFRESH, on_click=self._on_param_change)
                ]),
                ft.Container(
                    content=ft.Row(
                        [self._preview_table],
                        scroll=ft.ScrollMode.AUTO,  # Horizontal scroll
                    ),
                    expand=True,
                    border=ft.border.all(1, Colors.BORDER),
                    border_radius=8,
                    bgcolor=Colors.SURFACE,
                ),
                ft.Container(height=10),
                ft.Text("Export Log", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
                self._progress,
                ft.Container(
                    content=self._status_log,
                    height=150,
                    border=ft.border.all(1, Colors.BORDER),
                    border_radius=8,
                )
            ], expand=True, spacing=10),
            padding=20,
            expand=True
        )

        # Header with Back button
        header = ft.Container(
            content=ft.Row([
                ft.IconButton(ft.Icons.ARROW_BACK, on_click=lambda e: self.page.go("/workspace")),
                ft.Text("EXPORT DATASET", size=20, weight=ft.FontWeight.BOLD)
            ]),
            padding=10,
            bgcolor=Colors.SURFACE,
            border=ft.border.only(bottom=ft.BorderSide(1, Colors.DIVIDER))
        )

        self.content = ft.Column([
            # header, # Nav bar handles nav? Or should I keep a back button to workspace? 
            # Requirements said "[< Back to Workspace]". I'll include it.
            # But "page.go" might not work if shell uses _on_route_change directly. 
            # I'll rely on shell navigation if possible, but the button is visual.
            # I'll enable the button to navigate to /workspace
            
            ft.Row([
                config_panel,
                preview_panel
            ], expand=True, spacing=0)
        ], expand=True)



========================================
FILE: src/tempo_app/ui/pages/inspect.py
========================================
"""Data Inspector page - detailed view of datasets and their data."""

import flet as ft
from datetime import datetime
from pathlib import Path
from typing import Optional
import asyncio

try:
    import xarray as xr
    import np
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import io
    import base64
except ImportError:
    xr = None
    np = None
    plt = None

from ..theme import Colors, Spacing
from ..components.widgets import SectionCard
from ...storage.database import Database
from ...storage.models import Dataset, DatasetStatus


class InspectPage(ft.Container):
    """Page for inspecting dataset details and statistics."""
    
    def __init__(self, db: Database):
        super().__init__()
        self.db = db
        self.selected_dataset: Optional[Dataset] = None
        self._build()
    
    def did_mount(self):
        """Called when control is added to page - load data async."""
        self.page.run_task(self._load_initial_data_async)

    async def _load_initial_data_async(self):
        """Load initial page data without blocking UI."""
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        self._apply_datasets(datasets)
        # Load stats for initially selected dataset
        if self.selected_dataset:
            await self._load_data_stats()
        self.update()

    def _apply_datasets(self, datasets: list):
        """Apply datasets to dropdown (no DB call)."""
        self._dataset_dropdown.options = [
            ft.DropdownOption(dataset.id, dataset.name) for dataset in datasets
        ]
        if datasets:
            self._dataset_dropdown.value = datasets[0].id
            self.selected_dataset = datasets[0]
            self._update_info_display()
    
    def _build(self):
        """Build the inspect page."""
        # Header
        header = ft.Row([
            ft.Icon(ft.Icons.ANALYTICS, size=28, color=Colors.PRIMARY),
            ft.Text("Data Inspector", size=24, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
        ], spacing=12)
        
        # Dataset Selector
        self._dataset_dropdown = ft.Dropdown(
            label="Select Dataset",
            hint_text="Choose a dataset to inspect",
            width=400,
            options=[],
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        self._dataset_dropdown.on_change = self._on_dataset_change
        
        self._refresh_btn = ft.IconButton(
            icon=ft.Icons.REFRESH,
            tooltip="Refresh datasets",
            on_click=self._on_refresh,
        )
        
        selector_row = ft.Row([
            self._dataset_dropdown,
            self._refresh_btn,
        ], spacing=8)
        
        # Dataset Info Card
        self._info_content = ft.Column([
            ft.Text("Select a dataset to view details", color=Colors.ON_SURFACE_VARIANT, italic=True)
        ], spacing=8)
        
        info_card = SectionCard(
            title="Dataset Configuration",
            icon=ft.Icons.INFO,
            content=self._info_content,
        )
        
        # Data Statistics Card
        self._stats_content = ft.Column([
            ft.Text("No data loaded", color=Colors.ON_SURFACE_VARIANT, italic=True)
        ], spacing=8)
        
        stats_card = SectionCard(
            title="Data Statistics",
            icon=ft.Icons.BAR_CHART,
            content=self._stats_content,
        )
        
        # Loading indicator
        self._loading = ft.ProgressRing(visible=False)
        
        self.content = ft.Column([
            header,
            ft.Container(height=16),
            selector_row,
            ft.Container(height=16),
            self._loading,
            info_card,
            ft.Container(height=16),
            stats_card,
        ], scroll=ft.ScrollMode.AUTO)
        
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL

    def _on_refresh(self, e):
        """Refresh the dataset list."""
        self.page.run_task(self._refresh_async)

    async def _refresh_async(self):
        """Async refresh handler."""
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        self._apply_datasets(datasets)
        if self.selected_dataset:
            await self._load_data_stats()
        self.update()

    def _on_dataset_change(self, e):
        """Handle dataset selection change."""
        dataset_id = e.control.value
        if dataset_id:
            self.page.run_task(self._change_dataset_async, dataset_id)

    async def _change_dataset_async(self, dataset_id: str):
        """Async dataset change handler."""
        self.selected_dataset = await asyncio.to_thread(self.db.get_dataset, dataset_id)
        self._update_info_display()
        await self._load_data_stats()
        self.update()
    
    def _update_info_display(self):
        """Update the dataset info display."""
        if not self.selected_dataset:
            return
        
        ds = self.selected_dataset
        
        # Status indicator
        if ds.status == DatasetStatus.COMPLETE:
            status_text = "‚úì Complete"
            status_color = Colors.SUCCESS
        elif ds.status == DatasetStatus.PARTIAL:
            status_text = "‚ö†Ô∏è Partial"
            status_color = Colors.WARNING
        elif ds.status == DatasetStatus.ERROR:
            status_text = "‚úó Error"
            status_color = Colors.ERROR
        else:
            status_text = "Pending"
            status_color = Colors.ON_SURFACE_VARIANT
        
        self._info_content.controls = [
            self._info_row("Name", ds.name),
            self._info_row("Status", status_text, status_color),
            ft.Divider(height=1, color=Colors.BORDER),
            self._info_row("Date Range", f"{ds.date_start} to {ds.date_end}"),
            self._info_row("Day Filter", ds.day_filter_str()),
            self._info_row("Hour Filter", ds.hour_filter_str()),
            ft.Divider(height=1, color=Colors.BORDER),
            self._info_row("Region", f"{ds.bbox.west:.2f}¬∞ to {ds.bbox.east:.2f}¬∞ W, {ds.bbox.south:.2f}¬∞ to {ds.bbox.north:.2f}¬∞ N"),
            self._info_row("Cloud Filter", f"‚â§ {ds.max_cloud:.0%}"),
            self._info_row("SZA Filter", f"‚â§ {ds.max_sza:.0f}¬∞"),
            ft.Divider(height=1, color=Colors.BORDER),
            self._info_row("File Size", f"{ds.file_size_mb:.1f} MB"),
            self._info_row("Granules", f"{ds.granules_downloaded} / {ds.granule_count}"),
        ]
        
        if ds.file_path:
            self._info_content.controls.append(
                self._info_row("File Path", ds.file_path, size=10)
            )
    
    def _info_row(self, label: str, value: str, color=None, size: int = 13) -> ft.Row:
        """Create an info row."""
        return ft.Row([
            ft.Text(label + ":", size=size, color=Colors.ON_SURFACE_VARIANT, width=100),
            ft.Text(value, size=size, color=color or Colors.ON_SURFACE, expand=True),
        ], spacing=8)
    
    async def _load_data_stats(self):
        """Load and display statistics from the actual data file."""
        if not self.selected_dataset or not self.selected_dataset.file_path:
            self._stats_content.controls = [
                ft.Text("No processed data file available", color=Colors.ON_SURFACE_VARIANT, italic=True)
            ]
            self.update()
            return
        
        if xr is None:
            self._stats_content.controls = [
                ft.Text("xarray not available", color=Colors.ERROR)
            ]
            self.update()
            return
        
        self._loading.visible = True
        self.update()
        
        try:
            # Load stats in thread
            stats = await asyncio.to_thread(self._compute_stats, self.selected_dataset.file_path)
            
            if stats:
                self._build_stats_display(stats)
            else:
                self._stats_content.controls = [
                    ft.Text("Could not load data statistics", color=Colors.WARNING)
                ]
        except Exception as e:
            self._stats_content.controls = [
                ft.Text(f"Error loading stats: {e}", color=Colors.ERROR)
            ]
        
        self._loading.visible = False
        self.update()
    
    def _build_stats_display(self, stats: dict):
        """Build the statistics display with charts."""
        controls = []
        
        # Basic info section
        controls.extend([
            self._info_row("Grid Size", stats.get("grid_size", "N/A")),
            self._info_row("Available Hours", stats.get("hours", "N/A")),
            ft.Container(height=8),
        ])
        
        # Hourly Averages Chart
        hourly_data = stats.get("hourly_data", {})
        if hourly_data:
            controls.extend([
                ft.Divider(height=1, color=Colors.BORDER),
                ft.Container(height=8),
                ft.Text("üìä Hourly Averages & Trends", size=16, weight=ft.FontWeight.W_600, color=Colors.PRIMARY),
                ft.Container(height=8),
                self._build_hourly_trend_chart(hourly_data),
                ft.Container(height=16),
                self._build_hourly_chart(hourly_data),
                ft.Container(height=16),
            ])
        
        # Variable Statistics Cards
        controls.append(ft.Divider(height=1, color=Colors.BORDER))
        controls.append(ft.Container(height=8))
        controls.append(ft.Text("üìà Variable Statistics", size=16, weight=ft.FontWeight.W_600, color=Colors.PRIMARY))
        controls.append(ft.Container(height=8))
        
        # Create stats cards for each variable
        var_cards = []
        
        # NO2 card
        if stats.get("no2_mean") is not None:
            var_cards.append(self._build_variable_card(
                "NO‚ÇÇ Tropospheric VCD",
                stats.get("no2_min"),
                stats.get("no2_max"),
                stats.get("no2_mean"),
                stats.get("no2_std"),
                stats.get("no2_valid_pct"),
                ft.Colors.BLUE_800,
            ))

        # HCHO card
        if stats.get("hcho_mean") is not None:
            var_cards.append(self._build_variable_card(
                "HCHO Total VCD",
                stats.get("hcho_min"),
                stats.get("hcho_max"),
                stats.get("hcho_mean"),
                stats.get("hcho_std"),
                stats.get("hcho_valid_pct"),
                ft.Colors.GREEN_800,
            ))

        # FNR card
        if stats.get("fnr_mean") is not None:
            var_cards.append(self._build_variable_card(
                "FNR (HCHO/NO‚ÇÇ)",
                stats.get("fnr_min"),
                stats.get("fnr_max"),
                stats.get("fnr_mean"),
                stats.get("fnr_std"),
                stats.get("fnr_valid_pct"),
                ft.Colors.ORANGE_800,
            ))
        
        if var_cards:
            controls.append(ft.Row(var_cards, wrap=True, spacing=16, run_spacing=16))
        

        
        self._stats_content.controls = controls
    
    def _build_variable_card(self, title: str, min_val, max_val, mean_val, std_val, valid_pct, color) -> ft.Container:
        """Build a statistics card for a variable."""
        def fmt(v):
            if v is None:
                return "N/A"
            if abs(v) < 0.001 or abs(v) > 1000:
                return f"{v:.2e}"
            return f"{v:.4f}"
        
        return ft.Container(
            content=ft.Column([
                ft.Text(title, size=16, weight=ft.FontWeight.BOLD, color=color),
                ft.Container(height=8),
                ft.Row([
                    ft.Column([
                        ft.Text("Min", size=12, color=Colors.ON_SURFACE_VARIANT),
                        ft.Text(fmt(min_val), size=14, font_family="monospace", color=Colors.ON_SURFACE),
                    ], spacing=4, horizontal_alignment=ft.CrossAxisAlignment.CENTER),
                    ft.Column([
                        ft.Text("Mean", size=12, color=Colors.ON_SURFACE_VARIANT),
                        ft.Text(fmt(mean_val), size=15, font_family="monospace", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
                    ], spacing=4, horizontal_alignment=ft.CrossAxisAlignment.CENTER),
                    ft.Column([
                        ft.Text("Max", size=12, color=Colors.ON_SURFACE_VARIANT),
                        ft.Text(fmt(max_val), size=14, font_family="monospace", color=Colors.ON_SURFACE),
                    ], spacing=4, horizontal_alignment=ft.CrossAxisAlignment.CENTER),
                ], spacing=20, alignment=ft.MainAxisAlignment.SPACE_AROUND),
                ft.Container(height=8),
                ft.Row([
                    ft.Text(f"œÉ = {fmt(std_val)}", size=12, color=Colors.ON_SURFACE_VARIANT),
                    ft.Text(f"{valid_pct:.1f}% valid" if valid_pct else "", size=12, color=Colors.ON_SURFACE_VARIANT),
                ], spacing=16, alignment=ft.MainAxisAlignment.CENTER),
            ], spacing=0, horizontal_alignment=ft.CrossAxisAlignment.CENTER),
            bgcolor=Colors.SURFACE_VARIANT,
            padding=16,
            border_radius=8,
            width=300,
        )
    
    def _build_hourly_chart(self, hourly_data: dict) -> ft.Container:
        """Build a table showing hourly averages for each variable."""
        hours = sorted(hourly_data.keys())
        if not hours:
            return ft.Text("No hourly data available", color=Colors.ON_SURFACE_VARIANT, italic=True)

        # Build table rows
        rows = [
            # Header row
            ft.DataRow(cells=[
                ft.DataCell(ft.Text("Hour", size=12, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
                ft.DataCell(ft.Text("NO‚ÇÇ (molec/cm¬≤)", size=12, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
                ft.DataCell(ft.Text("HCHO (molec/cm¬≤)", size=12, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
            ])
        ]

        # Data rows
        for hour in hours:
            data = hourly_data[hour]
            no2_str = f"{data['no2']:.2e}" if data.get("no2") is not None else "N/A"
            hcho_str = f"{data['hcho']:.2e}" if data.get("hcho") is not None else "N/A"

            rows.append(ft.DataRow(cells=[
                ft.DataCell(ft.Text(f"{hour:02d}:00 UTC", size=11, color=Colors.ON_SURFACE)),
                ft.DataCell(ft.Text(no2_str, size=11, font_family="monospace", color=ft.Colors.BLUE_800)),
                ft.DataCell(ft.Text(hcho_str, size=11, font_family="monospace", color=ft.Colors.GREEN_800)),
            ]))

        table = ft.DataTable(
            columns=[
                ft.DataColumn(ft.Text("")),
                ft.DataColumn(ft.Text("")),
                ft.DataColumn(ft.Text("")),
            ],
            rows=rows,
            border=ft.border.all(1, Colors.BORDER),
            border_radius=8,
            horizontal_lines=ft.BorderSide(1, Colors.BORDER),
        )

        return ft.Container(
            content=table,
            bgcolor=Colors.SURFACE,
            padding=12,
            border_radius=8,
            border=ft.border.all(1, Colors.BORDER),
        )
    
    def _build_hourly_trend_chart(self, hourly_data: dict) -> ft.Container:
        """Build a line chart showing hourly trends for NO2 and HCHO using Matplotlib."""
        if not hourly_data or plt is None:
            return ft.Container(content=ft.Text("No data or matplotlib missing"))

        hours = sorted(hourly_data.keys())
        if not hours:
            return ft.Container(content=ft.Text("No hourly data"))

        no2_vals = []
        hcho_vals = []
        valid_hours = []

        for h in hours:
            data = hourly_data[h]
            if data.get("no2") is not None and data.get("hcho") is not None:
                valid_hours.append(h)
                no2_vals.append(data["no2"])
                hcho_vals.append(data["hcho"])

        if not valid_hours:
            return ft.Container(content=ft.Text("No valid data for chart"))

        # Create plot
        fig, ax1 = plt.subplots(figsize=(10, 5))
        
        # Determine colors from theme if possible, or use standard
        color_no2 = '#1565C0'  # Blue 800
        color_hcho = '#2E7D32' # Green 800
        
        # Plot NO2
        ax1.set_xlabel('Hour (UTC)')
        ax1.set_ylabel('NO‚ÇÇ (molec/cm¬≤)', color=color_no2, fontweight='bold')
        line1, = ax1.plot(valid_hours, no2_vals, color=color_no2, marker='o', linewidth=2, label='NO‚ÇÇ')
        ax1.tick_params(axis='y', labelcolor=color_no2)
        ax1.grid(True, linestyle='--', alpha=0.3)
        
        # Plot HCHO on same axis for now (or dual axis if ranges differ substantially)
        # Using dual axis for better visibility since magnitudes might differ
        ax2 = ax1.twinx()
        ax2.set_ylabel('HCHO (molec/cm¬≤)', color=color_hcho, fontweight='bold')
        line2, = ax2.plot(valid_hours, hcho_vals, color=color_hcho, marker='s', linewidth=2, label='HCHO')
        ax2.tick_params(axis='y', labelcolor=color_hcho)

        # Title and Layout
        plt.title('Hourly Average Trends', fontweight='bold')
        
        # Legend
        lines = [line1, line2]
        labels = [l.get_label() for l in lines]
        ax1.legend(lines, labels, loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)
        
        plt.tight_layout()

        # Save to buffer
        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        plt.close(fig)
        buf.seek(0)
        img_b64 = base64.b64encode(buf.read()).decode()

        return ft.Container(
            content=ft.Image(
                src_base64=img_b64,
                width=800,
                height=400,
                fit=ft.ImageFit.CONTAIN,
            ),
            bgcolor=Colors.SURFACE,
            padding=10,
            border_radius=12,
            border=ft.border.all(1, Colors.BORDER),
            alignment=ft.Alignment(0, 0),
        )
    
    def _stats_row(self, label: str, value) -> ft.Row:
        """Create a stats row with formatted value."""
        if value is None:
            formatted = "N/A"
        elif isinstance(value, float):
            if abs(value) < 0.001 or abs(value) > 1000:
                formatted = f"{value:.2e}"
            else:
                formatted = f"{value:.4f}"
        else:
            formatted = str(value)
        
        return ft.Row([
            ft.Text(f"  {label}:", size=12, color=Colors.ON_SURFACE_VARIANT, width=60),
            ft.Text(formatted, size=12, color=Colors.ON_SURFACE, font_family="monospace"),
        ], spacing=8)
    
    def _compute_stats(self, file_path: str) -> dict:
        """Compute statistics from a NetCDF file."""
        try:
            with xr.open_dataset(file_path) as ds:
                stats = {}
                
                # Metadata
                stats["dims"] = dict(ds.sizes)
                stats["coords"] = list(ds.coords)
                stats["data_vars"] = list(ds.data_vars)
                
                # Grid dimensions
                if 'ROW' in ds.sizes and 'COL' in ds.sizes:
                    stats["grid_size"] = f"{ds.sizes['ROW']} √ó {ds.sizes['COL']}"
                else:
                    stats["grid_size"] = "N/A"
                
                # Available hours (handle case-insensitive)
                hours = []
                hour_dim = None
                for dim_name in ['hour', 'HOUR', 'Hour']:
                    if dim_name in ds.dims:
                        hour_dim = dim_name
                        break

                if hour_dim:
                    hours = sorted(ds[hour_dim].values.tolist())
                    stats["hours"] = ", ".join(f"{h:02d}:00" for h in hours)
                elif 'TSTEP' in ds.dims:
                    stats["hours"] = f"{len(ds['TSTEP'])} time steps"
                else:
                    stats["hours"] = "N/A"
                
                # Compute hourly data for charts
                hourly_data = {}
                coverage_by_hour = {}
                
                for var_name, stat_prefix in [('NO2_TropVCD', 'no2'), ('HCHO_TotVCD', 'hcho'), ('FNR', 'fnr')]:
                    if var_name in ds:
                        data = ds[var_name].values
                        # Filter valid values
                        valid_mask = np.isfinite(data) & (data > -1e30) & (data < 1e30)
                        valid = data[valid_mask]
                        
                        if len(valid) > 0:
                            stats[f"{stat_prefix}_min"] = float(np.min(valid))
                            stats[f"{stat_prefix}_max"] = float(np.max(valid))
                            stats[f"{stat_prefix}_mean"] = float(np.mean(valid))
                            stats[f"{stat_prefix}_std"] = float(np.std(valid))
                            stats[f"{stat_prefix}_valid_pct"] = 100.0 * len(valid) / data.size
                        
                        # Hourly averages
                        if hour_dim and hours:
                            for h in hours:
                                if h not in hourly_data:
                                    hourly_data[h] = {}

                                # Get data for this hour
                                try:
                                    hour_slice = ds[var_name].sel({hour_dim: h}).values
                                    hour_valid = hour_slice[np.isfinite(hour_slice) & (hour_slice > -1e30) & (hour_slice < 1e30)]
                                    if len(hour_valid) > 0:
                                        hourly_data[h][stat_prefix] = float(np.mean(hour_valid))


                                except Exception:
                                    pass
                
                stats["hourly_data"] = hourly_data
                stats["coverage_by_hour"] = coverage_by_hour
                
                return stats
        except Exception as e:
            print(f"Error computing stats: {e}")
            return None


========================================
FILE: src/tempo_app/ui/pages/library.py
========================================
"""Data Library page - View and manage downloaded datasets.

The new home page of the app. Shows datasets in a grid view with search,
filter, and sort capabilities. Includes FAB for quick access to create new datasets.
"""

import flet as ft
import asyncio
from datetime import datetime
from typing import Optional, Callable
from enum import Enum

from ..theme import Colors, Spacing, Sizing
from ..components.widgets import SectionCard, StatusLogPanel
from ...storage.database import Database
from ...storage.models import Dataset, DatasetStatus, BatchJob


class FilterOption(Enum):
    ALL = "All"
    COMPLETE = "Complete"
    PARTIAL = "Partial"
    DOWNLOADING = "Downloading"


class SortOption(Enum):
    RECENT = "Recent"
    NAME_AZ = "Name A-Z"
    NAME_ZA = "Name Z-A"
    SIZE = "Size"


class DatasetCard(ft.Container):
    """Grid card for displaying a dataset with hover actions."""
    
    def __init__(
        self, 
        dataset: Dataset,
        on_click: Callable[[Dataset], None] = None,
        on_delete: Callable[[Dataset], None] = None,
        on_duplicate: Callable[[Dataset], None] = None,
    ):
        super().__init__()
        self.dataset = dataset
        self._on_click = on_click
        self._on_delete = on_delete
        self._on_duplicate = on_duplicate
        self._build()
    
    def _build(self):
        """Build the card UI."""
        ds = self.dataset
        
        # Status indicator
        if ds.status == DatasetStatus.COMPLETE:
            status_icon = ft.Icons.CHECK_CIRCLE
            status_color = Colors.SUCCESS
            status_text = "Complete"
        elif ds.status == DatasetStatus.PARTIAL:
            status_icon = ft.Icons.WARNING
            status_color = Colors.WARNING
            status_text = "Partial"
        else:
            status_icon = ft.Icons.DOWNLOADING
            status_color = Colors.INFO
            status_text = "Downloading"
        
        # Thumbnail placeholder (could be last map image)
        thumbnail = ft.Container(
            content=ft.Icon(ft.Icons.MAP, size=48, color=Colors.PRIMARY),
            height=100,
            bgcolor=Colors.PRIMARY_CONTAINER,
            border_radius=ft.border_radius.only(top_left=8, top_right=8),
            alignment=ft.Alignment(0, 0),
        )
        
        # Quick action buttons (visible on hover via opacity)
        self._actions_row = ft.Row([
            ft.IconButton(
                icon=ft.Icons.COPY,
                icon_size=18,
                icon_color=Colors.ON_SURFACE_VARIANT,
                tooltip="Duplicate config",
                on_click=lambda e: self._on_duplicate(ds) if self._on_duplicate else None,
            ),
            ft.IconButton(
                icon=ft.Icons.DELETE_OUTLINE,
                icon_size=18,
                icon_color=Colors.ERROR,
                tooltip="Delete",
                on_click=lambda e: self._on_delete(ds) if self._on_delete else None,
            ),
        ], spacing=0, alignment=ft.MainAxisAlignment.END)
        
        # Card content
        card_content = ft.Column([
            thumbnail,
            ft.Container(
                content=ft.Column([
                    # Title row
                    ft.Row([
                        ft.Text(
                            ds.name,
                            size=14,
                            weight=ft.FontWeight.W_600,
                            color=Colors.ON_SURFACE,
                            overflow=ft.TextOverflow.ELLIPSIS,
                            max_lines=1,
                            expand=True,
                        ),
                        ft.Icon(status_icon, size=16, color=status_color),
                    ]),
                    # Date range
                    ft.Text(
                        f"{ds.date_start} ‚Üí {ds.date_end}",
                        size=11,
                        color=Colors.ON_SURFACE_VARIANT,
                    ),
                    # Size and status
                    ft.Row([
                        ft.Text(
                            f"{ds.file_size_mb:.1f} MB",
                            size=11,
                            color=Colors.ON_SURFACE_VARIANT,
                        ),
                        ft.Container(expand=True),
                        self._actions_row,
                    ], spacing=0),
                ], spacing=4),
                padding=ft.padding.only(left=12, right=8, top=8, bottom=8),
            ),
        ], spacing=0)
        
        self.content = card_content
        self.width = 220
        self.bgcolor = Colors.SURFACE
        self.border_radius = 8
        self.border = ft.border.all(1, Colors.BORDER)
        self.on_click = lambda e: self._on_click(ds) if self._on_click else None
        self.on_hover = self._handle_hover
        
        # Shadow on hover
        self.shadow = None
        self.animate = ft.Animation(150, ft.AnimationCurve.EASE_OUT)
    
    def _handle_hover(self, e):
        """Handle hover state."""
        if e.data == "true":
            self.shadow = ft.BoxShadow(
                spread_radius=0,
                blur_radius=8,
                color=Colors.CARD_SHADOW,
                offset=ft.Offset(0, 2),
            )
            self.border = ft.border.all(1, Colors.PRIMARY)
        else:
            self.shadow = None
            self.border = ft.border.all(1, Colors.BORDER)
        self.update()


class BatchFolderCard(ft.Container):
    """Card representing a batch of datasets (folder)."""

    def __init__(self, batch_name: str, batch_id: str, count: int, on_click, on_delete=None):
        super().__init__()
        self.batch_name = batch_name
        self.batch_id = batch_id
        self.count = count
        self._on_click = on_click
        self._on_delete = on_delete
        self._build()

    def _build(self):
        # Delete button
        delete_btn = ft.IconButton(
            icon=ft.Icons.DELETE_OUTLINE,
            icon_size=18,
            icon_color=Colors.ERROR,
            tooltip="Delete batch",
            on_click=lambda e: self._on_delete(self.batch_id, self.batch_name) if self._on_delete else None,
        )

        self.content = ft.Column([
            ft.Container(
                content=ft.Icon(ft.Icons.FOLDER, size=48, color=Colors.PRIMARY),
                height=100,
                bgcolor=Colors.PRIMARY_CONTAINER,
                border_radius=ft.border_radius.only(top_left=8, top_right=8),
                alignment=ft.Alignment(0, 0),
            ),
            ft.Container(
                content=ft.Column([
                    ft.Row([
                        ft.Text(
                            self.batch_name,
                            size=14,
                            weight=ft.FontWeight.W_600,
                            color=Colors.ON_SURFACE,
                            overflow=ft.TextOverflow.ELLIPSIS,
                            expand=True,
                        ),
                        delete_btn,
                    ]),
                    ft.Text(
                        f"{self.count} datasets",
                        size=12,
                        color=Colors.ON_SURFACE_VARIANT,
                    ),
                ], spacing=4),
                padding=ft.padding.only(left=12, right=4, top=8, bottom=8),
            ),
        ], spacing=0)

        self.width = 220
        self.bgcolor = Colors.SURFACE
        self.border_radius = 8
        self.border = ft.border.all(1, Colors.BORDER)
        self.on_click = lambda e: self._on_click(self.batch_id)
        self.on_hover = self._handle_hover

    def _handle_hover(self, e):
        if e.data == "true":
            self.border = ft.border.all(1, Colors.PRIMARY)
            self.shadow = ft.BoxShadow(blur_radius=8, color=Colors.CARD_SHADOW, offset=ft.Offset(0, 2))
        else:
            self.border = ft.border.all(1, Colors.BORDER)
            self.shadow = None
        self.update()


class LibraryPage(ft.Container):
    """Page for browsing and managing stored datasets.
    
    Features:
    - Grid view of dataset cards
    - Search by name
    - Filter by status
    - Sort by date/name/size
    - Active downloads section
    - FAB for creating new datasets
    """
    
    def __init__(self, db: Database, on_navigate: Callable[[str], None] = None):
        super().__init__()
        self.db = db
        self._on_navigate = on_navigate
        self._all_datasets: list[Dataset] = []
        self._filtered_datasets: list[Dataset] = []
        self._batch_jobs: dict[str, BatchJob] = {}  # Map batch_id -> BatchJob
        self._current_filter = FilterOption.ALL
        self._current_sort = SortOption.RECENT
        self._current_folder: Optional[str] = None  # batch_job_id or None
        self._folder_name: str = ""
        self._search_query = ""
        self._build()
    
    def did_mount(self):
        """Called when control is added to page - load data async."""
        # Set the FAB on the page for proper positioning
        if hasattr(self, '_fab'):
            self.page.floating_action_button = self._fab
            self.page.update()
        self.page.run_task(self._load_data_async)
    
    def will_unmount(self):
        """Called when control is removed from page."""
        # Remove the FAB when leaving this page
        if self.page and self.page.floating_action_button == self._fab:
            self.page.floating_action_button = None

    async def _load_data_async(self):
        """Load all page data without blocking UI."""
        stats = await asyncio.to_thread(self.db.get_storage_stats)
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        batch_jobs = await asyncio.to_thread(self.db.get_all_batch_jobs)

        self._all_datasets = datasets
        self._batch_jobs = {job.id: job for job in batch_jobs}
        self._apply_filters()
        self._apply_storage_stats(stats)
        self._render_datasets()
        self.update()
    
    def _build(self):
        """Build the library page."""
        # Header with storage info
        self._storage_text = ft.Text("0 datasets ‚Ä¢ 0 MB", size=13, color=Colors.ON_SURFACE_VARIANT)
        
        header = ft.Container(
            content=ft.Row([
                ft.Row([
                    ft.Icon(ft.Icons.FOLDER_OPEN, size=28, color=Colors.PRIMARY),
                    ft.Text("Library", size=24, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
                ], spacing=12),
                ft.Container(expand=True),
                self._storage_text,
                ft.IconButton(
                    icon=ft.Icons.REFRESH,
                    icon_color=Colors.ON_SURFACE_VARIANT,
                    tooltip="Refresh",
                    on_click=self._on_refresh,
                ),
            ]),
            padding=ft.padding.only(bottom=Spacing.MD),
        )
        
        # Search and filter bar
        self._search_field = ft.TextField(
            hint_text="Search datasets...",
            prefix_icon=ft.Icons.SEARCH,
            border_radius=8,
            height=40,
            text_size=14,
            content_padding=ft.padding.only(left=8, right=8, bottom=8),
            on_change=self._on_search_change,
            expand=True,
        )
        
        self._filter_dropdown = ft.Dropdown(
            value=FilterOption.ALL.value,
            options=[ft.DropdownOption(opt.value) for opt in FilterOption],
            width=130,
            height=40,
            text_size=13,
            content_padding=ft.padding.symmetric(horizontal=12),
        )
        self._filter_dropdown.on_change = self._on_filter_change
        
        self._sort_dropdown = ft.Dropdown(
            value=SortOption.RECENT.value,
            options=[ft.DropdownOption(opt.value) for opt in SortOption],
            width=120,
            height=40,
            text_size=13,
            content_padding=ft.padding.symmetric(horizontal=12),
        )
        self._sort_dropdown.on_change = self._on_sort_change
        
        filter_row = ft.Row([
            self._search_field,
            ft.Container(width=8),
            ft.Text("Filter:", size=13, color=Colors.ON_SURFACE_VARIANT),
            self._filter_dropdown,
            ft.Container(width=8),
            ft.Text("Sort:", size=13, color=Colors.ON_SURFACE_VARIANT),
            self._sort_dropdown,
        ], vertical_alignment=ft.CrossAxisAlignment.CENTER)
        
        # Active downloads section (hidden when empty)
        self._downloads_section = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Icon(ft.Icons.DOWNLOADING, size=20, color=Colors.INFO),
                    ft.Text("Active Downloads", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                ], spacing=8),
                ft.Container(height=8),
                # Downloads will be added here
                ft.Column([], spacing=8),
            ]),
            bgcolor=Colors.INFO + "10",  # Very light blue tint
            border_radius=8,
            padding=Spacing.MD,
            visible=False,  # Hidden by default
        )
        
        # Dataset grid
        self._dataset_grid = ft.Row(
            wrap=True,
            spacing=16,
            run_spacing=16,
        )
        
        # Empty state
        self._empty_state = ft.Container(
            content=ft.Column([
                ft.Icon(ft.Icons.FOLDER_OPEN, size=64, color=Colors.ON_SURFACE_VARIANT),
                ft.Container(height=16),
                ft.Text(
                    "No datasets yet",
                    size=18,
                    weight=ft.FontWeight.W_500,
                    color=Colors.ON_SURFACE,
                ),
                ft.Text(
                    "Hit that + button to get started!",
                    size=14,
                    color=Colors.ON_SURFACE_VARIANT,
                ),
            ], horizontal_alignment=ft.CrossAxisAlignment.CENTER),
            alignment=ft.Alignment(0, 0),
            expand=True,
            visible=True,
        )
        
        # Grid container
        self._grid_container = ft.Container(
            content=ft.Column([
                self._dataset_grid,
            ], scroll=ft.ScrollMode.AUTO, expand=True),
            expand=True,
            visible=False,  # Start hidden, will be shown if datasets exist
        )
        
        # FAB button - will be added to page in did_mount
        self._fab = ft.FloatingActionButton(
            icon=ft.Icons.ADD,
            bgcolor=Colors.PRIMARY,
            foreground_color=Colors.ON_PRIMARY,
            tooltip="Create new dataset",
            on_click=self._on_fab_click,
        )
        
        # Content area with either empty state or grid
        self._content_stack = ft.Stack([
            self._empty_state,
            self._grid_container,
        ], expand=True)
        
        # Main layout
        main_content = ft.Column([
            header,
            filter_row,
            ft.Container(height=Spacing.MD),
            self._downloads_section,
            ft.Container(
                content=self._content_stack,
                expand=True,
            ),
        ], expand=True)
        
        self.content = main_content
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL
    
    def _apply_storage_stats(self, stats: dict):
        """Apply storage stats to UI."""
        try:
            total_mb = stats.get("total_size_mb", 0)
            dataset_count = stats.get("dataset_count", 0)
            self._storage_text.value = f"{dataset_count} dataset{'s' if dataset_count != 1 else ''} ‚Ä¢ {total_mb:.1f} MB"
        except Exception as e:
            self._storage_text.value = f"Error: {e}"
    
    def _apply_filters(self):
        """Apply current filter and sort to datasets."""
        # First, filter by current folder (batch)
        if self._current_folder:
            # Inside a folder: show only datasets in this batch
            current_level = [d for d in self._all_datasets if d.batch_job_id == self._current_folder]
        else:
            # Root level: show independent datasets (batch_job_id is None)
            # We will handle batch folders separately in _render_datasets
            current_level = [d for d in self._all_datasets if d.batch_job_id is None]

        filtered = current_level
        
        if self._current_filter == FilterOption.COMPLETE:
            filtered = [d for d in filtered if d.status == DatasetStatus.COMPLETE]
        elif self._current_filter == FilterOption.PARTIAL:
            filtered = [d for d in filtered if d.status == DatasetStatus.PARTIAL]
        elif self._current_filter == FilterOption.DOWNLOADING:
            filtered = [d for d in filtered if d.status == DatasetStatus.DOWNLOADING]
        
        # Search
        if self._search_query:
            query = self._search_query.lower()
            filtered = [d for d in filtered if query in d.name.lower()]
        
        # Sort
        if self._current_sort == SortOption.RECENT:
            filtered = sorted(filtered, key=lambda d: d.created_at or datetime.min, reverse=True)
        elif self._current_sort == SortOption.NAME_AZ:
            filtered = sorted(filtered, key=lambda d: d.name.lower())
        elif self._current_sort == SortOption.NAME_ZA:
            filtered = sorted(filtered, key=lambda d: d.name.lower(), reverse=True)
        elif self._current_sort == SortOption.SIZE:
            filtered = sorted(filtered, key=lambda d: d.file_size_mb, reverse=True)
        
        self._filtered_datasets = filtered
    
    def _render_datasets(self):
        """Render the dataset grid."""
        self._dataset_grid.controls.clear()
        items_to_show = []

        # If in a folder, show a "Back" button
        if self._current_folder:
            back_btn = ft.Container(
                content=ft.Row([
                    ft.Icon(ft.Icons.ARROW_BACK, size=16, color=Colors.PRIMARY),
                    ft.Text(f"Back to Library", color=Colors.PRIMARY),
                ], spacing=8),
                on_click=self._exit_folder,
                padding=10,
                border_radius=8,
                ink=True,
            )
            self._dataset_grid.controls.append(ft.Container(content=back_btn, width=220))

        # Handle search mode - search across all datasets ignoring hierarchy
        if self._search_query:
            query = self._search_query.lower()
            filtered = [d for d in self._all_datasets if query in d.name.lower()]

            # Apply status filter if not ALL
            if self._current_filter == FilterOption.COMPLETE:
                filtered = [d for d in filtered if d.status == DatasetStatus.COMPLETE]
            elif self._current_filter == FilterOption.PARTIAL:
                filtered = [d for d in filtered if d.status == DatasetStatus.PARTIAL]
            elif self._current_filter == FilterOption.DOWNLOADING:
                filtered = [d for d in filtered if d.status == DatasetStatus.DOWNLOADING]

            # Apply sort
            filtered = self._sort_datasets(filtered)

            for ds in filtered:
                items_to_show.append(
                    DatasetCard(
                        dataset=ds,
                        on_click=self._on_dataset_click,
                        on_delete=self._on_delete_click,
                        on_duplicate=self._on_duplicate_click,
                    )
                )
        elif self._current_folder:
            # Inside a batch folder - show datasets from this batch
            for ds in self._filtered_datasets:
                items_to_show.append(
                    DatasetCard(
                        dataset=ds,
                        on_click=self._on_dataset_click,
                        on_delete=self._on_delete_click,
                        on_duplicate=self._on_duplicate_click,
                    )
                )
        else:
            # Root level - show folders and independent datasets
            # First, collect batch folders
            if self._current_filter == FilterOption.ALL:
                batch_groups = {}
                for d in self._all_datasets:
                    if d.batch_job_id:
                        if d.batch_job_id not in batch_groups:
                            batch_groups[d.batch_job_id] = []
                        batch_groups[d.batch_job_id].append(d)

                # Create folder cards
                for bid, dlist in batch_groups.items():
                    # Get batch job name if available
                    batch_job = self._batch_jobs.get(bid)
                    folder_name = batch_job.name if batch_job else "Batch Import"
                    items_to_show.append(BatchFolderCard(folder_name, bid, len(dlist), self._enter_folder, self._on_batch_delete_click))

            # Add independent datasets (not in any batch)
            for ds in self._filtered_datasets:
                items_to_show.append(
                    DatasetCard(
                        dataset=ds,
                        on_click=self._on_dataset_click,
                        on_delete=self._on_delete_click,
                        on_duplicate=self._on_duplicate_click,
                    )
                )

        # Show empty state or grid
        if not items_to_show and not self._current_folder:
            self._empty_state.visible = True
            self._grid_container.visible = False
        else:
            self._empty_state.visible = False
            self._grid_container.visible = True
            for item in items_to_show:
                self._dataset_grid.controls.append(item)

    def _sort_datasets(self, datasets: list[Dataset]) -> list[Dataset]:
        """Sort datasets according to current sort option."""
        if self._current_sort == SortOption.RECENT:
            return sorted(datasets, key=lambda d: d.created_at or datetime.min, reverse=True)
        elif self._current_sort == SortOption.NAME_AZ:
            return sorted(datasets, key=lambda d: d.name.lower())
        elif self._current_sort == SortOption.NAME_ZA:
            return sorted(datasets, key=lambda d: d.name.lower(), reverse=True)
        elif self._current_sort == SortOption.SIZE:
            return sorted(datasets, key=lambda d: d.file_size_mb, reverse=True)
        return datasets

    def _enter_folder(self, batch_id: str):
        self._current_folder = batch_id
        # Get the batch name
        batch_job = self._batch_jobs.get(batch_id)
        self._folder_name = batch_job.name if batch_job else "Batch Import"
        self._apply_filters()
        self._render_datasets()
        self.update()

    def _exit_folder(self, e):
        self._current_folder = None
        self._apply_filters()
        self._render_datasets()
        self.update()

    
    def _on_search_change(self, e):
        """Handle search input change."""
        self._search_query = e.control.value or ""
        self._apply_filters()
        self._render_datasets()
        self.update()
    
    def _on_filter_change(self, e):
        """Handle filter dropdown change."""
        value = e.control.value
        self._current_filter = next((f for f in FilterOption if f.value == value), FilterOption.ALL)
        self._apply_filters()
        self._render_datasets()
        self.update()
    
    def _on_sort_change(self, e):
        """Handle sort dropdown change."""
        value = e.control.value
        self._current_sort = next((s for s in SortOption if s.value == value), SortOption.RECENT)
        self._apply_filters()
        self._render_datasets()
        self.update()
    
    def _on_refresh(self, e):
        """Refresh the dataset list."""
        self.page.run_task(self._load_data_async)
    
    def _on_fab_click(self, e):
        """Handle FAB click - navigate to new dataset page."""
        if self.page:
            # Navigate using shell's navigate function
            shell = self.page.controls[0] if self.page.controls else None
            if shell and hasattr(shell, 'navigate_to'):
                shell.navigate_to("/new")
    
    def _on_dataset_click(self, dataset: Dataset):
        """Handle dataset card click - navigate to workspace."""
        if self.page:
            shell = self.page.controls[0] if self.page.controls else None
            if shell and hasattr(shell, 'navigate_to'):
                shell.navigate_to(f"/workspace/{dataset.id}")
    
    def _on_delete_click(self, dataset: Dataset):
        """Handle delete button click."""
        # Show confirmation dialog
        def close_dialog(e):
            dlg.open = False
            self.page.update()
        
        def confirm_delete(e):
            dlg.open = False
            self.page.run_task(self._delete_dataset_async, dataset)
        
        dlg = ft.AlertDialog(
            modal=True,
            title=ft.Text("Delete Dataset?", color=Colors.ON_SURFACE),
            content=ft.Text(f"Are you sure you want to delete '{dataset.name}'?\nThis cannot be undone.", color=Colors.ON_SURFACE),
            actions=[
                ft.TextButton("Cancel", on_click=close_dialog),
                ft.TextButton("Delete", on_click=confirm_delete, style=ft.ButtonStyle(color=Colors.ERROR)),
            ],
            actions_alignment=ft.MainAxisAlignment.END,
        )
        
        self.page.overlay.append(dlg)
        dlg.open = True
        self.page.update()
    
    async def _delete_dataset_async(self, dataset: Dataset):
        """Delete a dataset asynchronously."""
        try:
            await asyncio.to_thread(self.db.delete_dataset, dataset.id)
            # Refresh
            await self._load_data_async()
        except Exception as e:
            print(f"Error deleting dataset: {e}")

    def _on_batch_delete_click(self, batch_id: str, batch_name: str):
        """Handle batch folder delete button click."""
        def close_dialog(e):
            dlg.open = False
            self.page.update()

        def confirm_delete(e):
            dlg.open = False
            self.page.run_task(self._delete_batch_async, batch_id)

        dlg = ft.AlertDialog(
            modal=True,
            title=ft.Text("Delete Entire Batch?", color=Colors.ON_SURFACE),
            content=ft.Text(
                f"Are you sure you want to delete the batch '{batch_name}' and ALL its datasets?\n\nThis will delete all files and cannot be undone.",
                color=Colors.ON_SURFACE
            ),
            actions=[
                ft.TextButton("Cancel", on_click=close_dialog),
                ft.TextButton("Delete All", on_click=confirm_delete, style=ft.ButtonStyle(color=Colors.ERROR)),
            ],
            actions_alignment=ft.MainAxisAlignment.END,
        )

        self.page.overlay.append(dlg)
        dlg.open = True
        self.page.update()

    async def _delete_batch_async(self, batch_id: str):
        """Delete an entire batch job and all associated data."""
        try:
            await asyncio.to_thread(self.db.delete_batch_job_full, batch_id)
            # Refresh
            await self._load_data_async()
        except Exception as e:
            print(f"Error deleting batch: {e}")

    def _on_duplicate_click(self, dataset: Dataset):
        """Handle duplicate button click - navigate to new dataset with prefilled config."""
        if self.page:
            shell = self.page.controls[0] if self.page.controls else None
            if shell and hasattr(shell, 'navigate_to'):
                # TODO: Pass dataset config as query params
                shell.navigate_to("/new")


========================================
FILE: src/tempo_app/ui/pages/plot.py
========================================
"""Plot Page - Visualize TEMPO data maps.

This page allows users to:
1. Select a dataset to visualize
2. Choose variable (NO2, HCHO, FNR)
3. Select hour and view/animate maps
"""

import flet as ft
from pathlib import Path
from typing import Optional
import asyncio

from ..theme import Colors, Spacing
from ..components.widgets import LabeledField, SectionCard, StatusLogPanel
from ...storage.database import Database
from ...core.plotter import MapPlotter


class PlotPage(ft.Container):
    """Page for visualizing TEMPO data as maps."""
    
    def __init__(self, db: Database, data_dir: Path):
        super().__init__()
        self.db = db
        self.data_dir = data_dir
        self.plotter = MapPlotter(data_dir)
        
        self._current_dataset = None
        self._current_hour = 12
        
        self._build()

    def did_mount(self):
        """Called when control is added to page - load data async."""
        self.page.run_task(self._load_datasets_async)

    async def _load_datasets_async(self):
        """Load datasets without blocking UI."""
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        self._apply_datasets(datasets)
        self.update()

    def _build(self):
        """Build the page layout."""
        # Dataset selector
        self._dataset_dropdown = ft.Dropdown(
            label="Select Dataset",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            width=300,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        self._dataset_dropdown.on_change = self._on_dataset_change
        
        # Variable selector
        self._variable_dropdown = ft.Dropdown(
            label="Variable",
            options=[
                ft.DropdownOption(key="FNR", text="FNR (HCHO/NO‚ÇÇ Ratio)"),
                ft.DropdownOption(key="NO2", text="NO‚ÇÇ Tropospheric VCD"),
                ft.DropdownOption(key="HCHO", text="HCHO Total VCD"),
            ],
            value="FNR",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            width=200,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        # Road detail selector
        self._road_dropdown = ft.Dropdown(
            label="Road Detail",
            options=[
                ft.DropdownOption(key="primary", text="Interstates Only"),
                ft.DropdownOption(key="major", text="Major Roads"),
                ft.DropdownOption(key="all", text="All Roads"),
            ],
            value="primary",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            width=150,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        # Hour slider
        self._hour_slider = ft.Slider(
            min=0,
            max=23,
            value=12,
            divisions=23,
            label="{value}:00 UTC",
            expand=True,
        )
        self._hour_slider.on_change = self._on_hour_change
        
        self._hour_text = ft.Text(
            "12:00 UTC",
            size=16,
            weight=ft.FontWeight.W_500,
            color=Colors.ON_SURFACE,
        )
        
        # Generate button
        self._generate_btn = ft.FilledButton(
            content=ft.Row([
                ft.Icon(ft.Icons.MAP, size=20),
                ft.Text("Generate Map"),
            ], spacing=8, tight=True),
            on_click=self._on_generate_click,
        )
        
        # Customization controls
        self._font_slider = ft.Slider(min=8, max=24, divisions=16, value=10, label="{value}")
        self._border_slider = ft.Slider(min=0.5, max=5.0, divisions=45, value=1.5, label="{value}")
        self._road_slider = ft.Slider(min=0.5, max=3.0, divisions=25, value=1.0, label="{value}")
        self._cmap_dropdown = ft.Dropdown(
            options=[
                ft.DropdownOption("Default"),
                ft.DropdownOption("viridis"),
                ft.DropdownOption("plasma"),
                ft.DropdownOption("inferno"),
                ft.DropdownOption("magma"),
                ft.DropdownOption("cividis"),
                ft.DropdownOption("coolwarm"),
                ft.DropdownOption("bwr"),
                ft.DropdownOption("seismic"),
                ft.DropdownOption("jet"),
            ],
            value="Default",
            label="Colormap",
            width=200,
            text_size=14,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        self._vmin_input = ft.TextField(
            label="Min Value",
            width=90,
            text_size=14,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            keyboard_type=ft.KeyboardType.NUMBER,
            border_color=Colors.BORDER,
        )
        
        self._vmax_input = ft.TextField(
            label="Max Value",
            width=90,
            text_size=14,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            keyboard_type=ft.KeyboardType.NUMBER,
            border_color=Colors.BORDER,
        )

        # Image display
        self._map_image = ft.Image(
            src="",
            width=700,
            height=600,
            fit="contain",
            visible=False,
        )
        
        self._placeholder = ft.Container(
            content=ft.Column([
                ft.Icon(ft.Icons.MAP_OUTLINED, size=64, color=Colors.ON_SURFACE_VARIANT),
                ft.Text(
                    "Select a dataset and click 'Generate Map'",
                    size=16,
                    color=Colors.ON_SURFACE_VARIANT,
                ),
            ], horizontal_alignment=ft.CrossAxisAlignment.CENTER, spacing=16),
            alignment=ft.Alignment(0, 0),
            width=700,
            height=600,
            bgcolor=Colors.SURFACE_VARIANT,
            border_radius=12,
        )
        
        # Status Message Area
        self._status_icon = ft.Icon(ft.Icons.INFO_OUTLINE, color=Colors.PRIMARY)
        self._status_text = ft.Text(
            "Ready to generate map",
            size=16,
            color=Colors.ON_SURFACE,
            weight=ft.FontWeight.W_500,
            expand=True,
        )
        
        self._status_container = ft.Container(
            content=ft.Column([
                ft.Row([
                    self._status_icon,
                    self._status_text,
                ], spacing=12),
                ft.ProgressBar(
                    visible=False,
                    color=Colors.PRIMARY,
                    bgcolor=Colors.BACKGROUND,
                    height=4,
                ),
            ], spacing=0),  # Zero spacing so bar is tight against bottom or separate? Let's use internal padding or spacing.
            padding=16,
            border_radius=8,
            bgcolor=Colors.SURFACE_VARIANT,
            visible=True,
        )
        # Store ref to progress bar for easy access
        self._progress_bar = self._status_container.content.controls[1]
        
        # Layout
        controls_row = ft.Row([
            self._dataset_dropdown,
            self._variable_dropdown,
            self._road_dropdown,
            self._generate_btn,
        ], spacing=16, wrap=True)
        
        
        hour_row = ft.Row([
            ft.Text("Hour:", color=Colors.ON_SURFACE),
            self._hour_slider,
            self._hour_text,
        ], spacing=12, vertical_alignment=ft.CrossAxisAlignment.CENTER)
        
        left_col = ft.Column([
            # Header
            ft.Text("üó∫Ô∏è Map Visualization", size=28, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
            ft.Divider(height=20, color=Colors.DIVIDER),
            
            # Controls
            controls_row,
            ft.Container(height=16),
            hour_row,
            ft.Container(height=16),
            
            # Status Message (Top)
            self._status_container,
            ft.Container(height=16),

            # Map display
            ft.Stack([
                self._placeholder,
                ft.GestureDetector(
                    mouse_cursor=ft.MouseCursor.ZOOM_IN,
                    on_tap=self._on_map_click,
                    content=ft.Container(
                        content=self._map_image,
                        bgcolor="transparent",
                    ),
                ),
            ]),
            
        ], scroll=ft.ScrollMode.AUTO, expand=True)
        


        # Right Column - Customization
        right_col = ft.Column([
            ft.Text("Map Style", size=16, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
            
            SectionCard("Appearance", ft.Column([
                ft.Text("Font Size", color=Colors.ON_SURFACE),
                self._font_slider,
                
                ft.Text("Border Boldness", color=Colors.ON_SURFACE),
                self._border_slider,
                
                ft.Text("Road Width Scale", color=Colors.ON_SURFACE),
                self._road_slider,
                
                self._cmap_dropdown,
                
                ft.Text("Data Scale (Empty = Auto)", color=Colors.ON_SURFACE),
                ft.Row([
                    self._vmin_input,
                    self._vmax_input,
                ], spacing=10),
            ], spacing=20)),
            
        ], width=300, scroll=ft.ScrollMode.AUTO)
        
        # Main layout
        self.content = ft.Row([
            left_col,
            ft.VerticalDivider(width=1),
            right_col
        ], expand=True)
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL

    def _show_message(self, message: str, is_error: bool = False, is_success: bool = False):
        """Show a status message with appropriate styling."""
        self._status_text.value = message
        
        if is_error:
            self._status_container.bgcolor = Colors.ERROR_CONTAINER
            self._status_text.color = Colors.ON_ERROR_CONTAINER
            self._status_icon.name = ft.Icons.ERROR_OUTLINE
            self._status_icon.color = Colors.ERROR
        elif is_success:
            self._status_container.bgcolor = Colors.PRIMARY_CONTAINER
            self._status_text.color = Colors.ON_PRIMARY_CONTAINER
            self._status_icon.name = ft.Icons.CHECK_CIRCLE_OUTLINE
            self._status_icon.color = Colors.PRIMARY
        else:
            self._status_container.bgcolor = Colors.SURFACE_VARIANT
            self._status_text.color = Colors.ON_SURFACE_VARIANT
            self._status_icon.name = ft.Icons.INFO_OUTLINE
            self._status_icon.color = Colors.PRIMARY
            
        self.update()
    
    def _apply_datasets(self, datasets: list):
        """Apply datasets to dropdown (no DB call)."""
        options = []
        for ds in datasets:
            options.append(ft.DropdownOption(key=ds.id, text=ds.name))
        self._dataset_dropdown.options = options
        if options:
            self._dataset_dropdown.value = options[0].key

    def _on_dataset_change(self, e):
        """Handle dataset selection change."""
        self._current_dataset = None
        self._map_image.visible = False
        self._placeholder.visible = True
        self.update()
    
    def _on_hour_change(self, e):
        """Handle hour slider change."""
        self._current_hour = int(e.control.value)
        self._hour_text.value = f"{self._current_hour:02d}:00 UTC"
        self.update()
    
    def _on_generate_click(self, e):
        """Generate the map."""
        self.page.run_task(self._generate_map)
    
    async def _generate_map(self):
        """Generate map asynchronously."""
        import xarray as xr
        
        dataset_id = self._dataset_dropdown.value
        if not dataset_id:
            self._show_message("‚ö†Ô∏è Please select a dataset", is_error=True)
            return
        
        dataset = self.db.get_dataset(dataset_id)
        if not dataset:
            self._show_message("‚ö†Ô∏è Dataset not found", is_error=True)
            return
        
        # Find processed file - use file_path from database if available (handles batch imports)
        if dataset.file_path and Path(dataset.file_path).exists():
            processed_path = Path(dataset.file_path)
        else:
            # Fallback to constructed path for backwards compatibility
            safe_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in dataset.name)
            processed_path = self.data_dir / "datasets" / safe_name / f"{safe_name}_processed.nc"

        if not processed_path.exists():
            self._show_message("‚ö†Ô∏è Processed data not found. Download or process the dataset first.", is_error=True)
            return
        
        self._show_message("üé® Generating map...")
        self._progress_bar.visible = True
        self.update()
        
        try:
            # Load dataset
            ds = xr.open_dataset(processed_path)
            
            # Capture available hours for error reporting
            available_hours = []
            if 'HOUR' in ds.coords:
                available_hours = sorted(ds.HOUR.values.tolist())
            elif 'hour' in ds.coords:
                available_hours = sorted(ds.hour.values.tolist())
            
            variable = self._variable_dropdown.value
            hour = self._current_hour
            road_detail = self._road_dropdown.value
            bbox = [dataset.bbox.west, dataset.bbox.south, dataset.bbox.east, dataset.bbox.north]
            
            # Style args
            style_args = {
                'font_size': int(self._font_slider.value),
                'border_width': float(self._border_slider.value),
                'road_scale': float(self._road_slider.value),
            }
            if self._cmap_dropdown.value != "Default":
                style_args['colormap'] = self._cmap_dropdown.value
            
            # Parse Min/Max
            if self._vmin_input.value:
                try:
                    style_args['vmin'] = float(self._vmin_input.value)
                except ValueError:
                    self._show_message("‚ö†Ô∏è Invalid Min Value", is_error=True)
                    self._progress_bar.visible = False
                    return
            
            if self._vmax_input.value:
                try:
                    style_args['vmax'] = float(self._vmax_input.value)
                except ValueError:
                    self._show_message("‚ö†Ô∏è Invalid Max Value", is_error=True)
                    self._progress_bar.visible = False
                    return

            # Get sites from DB
            sites = self.db.get_sites_as_dict(dataset.bbox)

            # Generate map
            result = await asyncio.to_thread(
                self.plotter.generate_map,
                ds,
                hour,
                variable,
                dataset.name,
                bbox=bbox,
                road_detail=road_detail,
                sites=sites,
                **style_args
            )
            
            ds.close()
            self._progress_bar.visible = False
            
            if result:
                self._map_image.src = result
                self._map_image.visible = True
                self._placeholder.visible = False
                self._map_image.update()
                self._placeholder.update()
                self._show_message(f"‚úÖ Generated {variable} map for {hour:02d}:00 UTC", is_success=True)
            else:
                hours_str = ", ".join(f"{h:02d}:00" for h in available_hours) if available_hours else "None found"
                self._show_message(
                    f"‚ö†Ô∏è No data for hour {hour:02d}:00.\nAvailable hours: {hours_str}", 
                    is_error=True
                )
            
        except Exception as e:
            self._progress_bar.visible = False
            self._show_message(f"‚ùå Error: {e}", is_error=True)
    
    def _on_map_click(self, e):
        """Open map in full-screen gallery."""
        if not self._map_image.src:
            return
            
        # Create a dialog with the image in an InteractiveViewer
        img = ft.Image(
            src=self._map_image.src,
            fit="contain",
        )
        
        viewer = ft.InteractiveViewer(
            min_scale=1,
            max_scale=5,
            content=img,
        )
        
        dlg = ft.AlertDialog(
            content=ft.Container(
                content=viewer,
                width=1000,
                height=800,
            ),
            inset_padding=10,
        )
        
        self.page.dialog = dlg
        dlg.open = True
        self.page.update()


========================================
FILE: src/tempo_app/ui/pages/settings.py
========================================
import flet as ft
from pathlib import Path
from typing import Callable

from ..theme import Colors, Spacing
from ..components.widgets import SectionCard
from ...core.config import ConfigManager

class SettingsPage(ft.Container):
    """Application settings page."""
    
    def _on_save_dir(self, e):
        """Save the manually entered directory."""
        path_str = self._dir_input.value.strip()
        if not path_str:
            self.page.snack_bar = ft.SnackBar(ft.Text("Path cannot be empty"))
            self.page.snack_bar.open = True
            self.page.update()
            return

        # Basic cleanup
        if path_str.startswith('"') and path_str.endswith('"'):
            path_str = path_str[1:-1]
            
        try:
            p = Path(path_str)
            # Create if doesn't exist? Or just warn?
            # Let's try to verify if it's a valid path format
            if not p.exists():
                # Ask user if they want to create it? Or just assume yes?
                # For simplicity, just save it. The app will try to create it on restart.
                pass
                
            self.config.set("data_dir", str(p))
            
            if self.on_restart_request:
                self.on_restart_request("Data directory changed. Please restart the app.")
            else:
                self.page.snack_bar = ft.SnackBar(ft.Text("Path saved. Restart required."))
                self.page.snack_bar.open = True
                self.page.update()
                
        except Exception as e:
            self.page.snack_bar = ft.SnackBar(ft.Text(f"Invalid path: {e}"))
            self.page.snack_bar.open = True
            self.page.update()

    def __init__(self, config: ConfigManager, on_restart_request: Callable = None):
        super().__init__()
        self.config = config
        self.on_restart_request = on_restart_request
        self._build()
        
    async def _open_picker(self, e):
        """Open directory picker asynchronously."""
        # Note: In Flet async mode, get_directory_path is a coroutine
        # FilePicker removed by user request
        pass
        
    def _build(self):
        # Header
        header = ft.Column([
            ft.Text("‚öôÔ∏è Settings", size=28, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
            ft.Divider(height=20, color=Colors.DIVIDER),
        ])
        
        # Data Directory Section
        # Data Directory Section
        self._dir_input = ft.TextField(
            value=self.config.data_dir or str(Path.cwd() / "data"),
            label="Path",
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            border_color=Colors.BORDER,
            expand=True,
            text_size=14,
        )
        
        data_section = SectionCard("Data Storage", ft.Column([
            ft.Text("Location where datasets and plots are stored.", color=Colors.ON_SURFACE_VARIANT),
            ft.Row([
                ft.Icon(ft.Icons.FOLDER, color=Colors.PRIMARY),
                self._dir_input,
                ft.IconButton(
                    icon=ft.Icons.SAVE, 
                    icon_color=Colors.PRIMARY,
                    tooltip="Save Path",
                    on_click=self._on_save_dir
                )
            ], alignment=ft.MainAxisAlignment.SPACE_BETWEEN),
            ft.Text("‚ö†Ô∏è Changing this requires an app restart.", size=12, color=Colors.ERROR),
        ], spacing=10))
        
        # Appearance Section
        self._font_slider = ft.Slider(
            min=0.8, max=1.5, divisions=7, value=self.config.font_scale,
            label="{value}x",
            on_change_end=self._on_font_scale_change
        )
        
        appearance_section = SectionCard("Appearance", ft.Column([
            ft.Text("Application Font Size (Scale)", color=Colors.ON_SURFACE),
            ft.Row([
                ft.Text("Small", size=12, color=Colors.ON_SURFACE),
                ft.Container(content=self._font_slider, expand=True),
                ft.Text("Large", size=12, color=Colors.ON_SURFACE),
            ]),
            ft.Text("Adjusts the text size across the entire application (accessibility).", size=12, color=Colors.ON_SURFACE_VARIANT),
        ], spacing=10))
        
        # Download Workers Section
        self._workers_slider = ft.Slider(
            min=1, max=8, divisions=7, value=self.config.download_workers,
            label="{value}",
            on_change_end=self._on_workers_change
        )
        
        download_section = SectionCard("Downloads", ft.Column([
            ft.Text("Parallel Download Workers", color=Colors.ON_SURFACE),
            ft.Row([
                ft.Text("1", size=12, color=Colors.ON_SURFACE),
                ft.Container(content=self._workers_slider, expand=True),
                ft.Text("8", size=12, color=Colors.ON_SURFACE),
            ]),
            ft.Text(
                "Number of simultaneous downloads. Higher = faster but uses more bandwidth. "
                "Reduce if you experience network issues.", 
                size=12, color=Colors.ON_SURFACE_VARIANT
            ),
        ], spacing=10))
        
        # API Key Section
        self._api_key_input = ft.TextField(
            value=self.config.rsig_api_key,
            label="RSIG API Key",
            password=True,
            can_reveal_password=True,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            border_color=Colors.BORDER,
            expand=True,
            text_size=14,
            hint_text="Enter your NASA RSIG API key (optional)",
        )
        
        api_section = SectionCard("API Configuration", ft.Column([
            ft.Text("NASA RSIG API Key", color=Colors.ON_SURFACE),
            ft.Row([
                ft.Icon(ft.Icons.KEY, color=Colors.PRIMARY),
                self._api_key_input,
                ft.IconButton(
                    icon=ft.Icons.SAVE, 
                    icon_color=Colors.PRIMARY,
                    tooltip="Save API Key",
                    on_click=self._on_save_api_key
                )
            ], alignment=ft.MainAxisAlignment.SPACE_BETWEEN),
            ft.Text(
                "An API key is recommended for reliable data downloads. "
                "Leave empty for anonymous access (may have limits).", 
                size=12, color=Colors.ON_SURFACE_VARIANT
            ),
        ], spacing=10))
        
        # Content
        self.content = ft.Column([
            header,
            data_section,
            ft.Container(height=10),
            appearance_section,
            ft.Container(height=10),
            download_section,
            ft.Container(height=10),
            api_section,
            ft.Container(height=20),
        ], scroll=ft.ScrollMode.AUTO)
        
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL

    def _on_font_scale_change(self, e):
        new_scale = float(e.control.value)
        self.config.set("font_scale", new_scale)
        
        if self.on_restart_request:
            self.on_restart_request("Font scale changed. Please restart to apply fully.")
    
    def _on_workers_change(self, e):
        """Handle download workers slider change."""
        new_workers = int(e.control.value)
        self.config.set("download_workers", new_workers)
        
        # Show confirmation (no restart needed for this setting)
        if self.page:
            self.page.snack_bar = ft.SnackBar(
                ft.Text(f"Download workers set to {new_workers}"),
                duration=2000
            )
            self.page.snack_bar.open = True
            self.page.update()

    def _on_save_api_key(self, e):
        """Save the API key."""
        api_key = self._api_key_input.value.strip()
        self.config.set("rsig_api_key", api_key)
        
        if self.page:
            msg = "API key saved!" if api_key else "API key cleared (using anonymous access)"
            self.page.snack_bar = ft.SnackBar(
                ft.Text(msg),
                duration=2000
            )
            self.page.snack_bar.open = True
            self.page.update()


========================================
FILE: src/tempo_app/ui/pages/sites.py
========================================
"""Sites Management Page - Add, view, and delete monitoring sites."""

import flet as ft
from datetime import datetime
from typing import Optional

from ..theme import Colors, Spacing
from ..components.widgets import SectionCard
from ...storage.database import Database
from ...storage.models import Site


class SitesPage(ft.Container):
    """Page for managing monitoring sites that appear on maps."""
    
    def __init__(self, db: Database):
        super().__init__()
        self.db = db
        self._build()
    
    def _build(self):
        """Build the sites management page."""
        # Header
        header = ft.Row([
            ft.Icon(ft.Icons.PLACE, size=28, color=Colors.PRIMARY),
            ft.Text("Site Management", size=24, weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE),
        ], spacing=12)
        
        description = ft.Text(
            "Manage monitoring sites that are marked on map visualizations. "
            "Sites are shown as star markers with their code labels.",
            size=13,
            color=Colors.ON_SURFACE_VARIANT,
        )
        
        # === Add Site Form ===
        self._code_field = ft.TextField(
            label="Site Code",
            hint_text="e.g., BV, LC",
            width=100,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            max_length=10,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        self._name_field = ft.TextField(
            label="Site Name (optional)",
            hint_text="e.g., Bountiful, UT",
            width=200,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        self._lat_field = ft.TextField(
            label="Latitude",
            hint_text="e.g., 40.903",
            width=120,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            keyboard_type=ft.KeyboardType.NUMBER,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        self._lon_field = ft.TextField(
            label="Longitude",
            hint_text="e.g., -111.884",
            width=120,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            keyboard_type=ft.KeyboardType.NUMBER,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
        )
        
        self._add_btn = ft.ElevatedButton(
            "Add Site",
            icon=ft.Icons.ADD_LOCATION,
            bgcolor=Colors.PRIMARY,
            color=Colors.ON_PRIMARY,
            on_click=self._on_add_site,
        )
        
        self._status_text = ft.Text("", size=12, color=Colors.ON_SURFACE_VARIANT)
        
        add_form = ft.Container(
            content=ft.Column([
                ft.Text("Add New Site", size=16, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                ft.Container(height=8),
                ft.Row([
                    self._code_field,
                    self._name_field,
                    self._lat_field,
                    self._lon_field,
                    self._add_btn,
                ], spacing=12, wrap=True),
                self._status_text,
            ]),
            bgcolor=Colors.SURFACE,
            padding=16,
            border_radius=8,
            border=ft.border.all(1, Colors.BORDER),
        )
        
        # === Import Defaults Button ===
        self._import_btn = ft.OutlinedButton(
            "Import Default Sites",
            icon=ft.Icons.DOWNLOAD,
            on_click=self._on_import_defaults,
        )
        
        # === Sites List ===
        self._sites_table = ft.DataTable(
            columns=[
                ft.DataColumn(ft.Text("Code", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
                ft.DataColumn(ft.Text("Name", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
                ft.DataColumn(ft.Text("Latitude", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE), numeric=True),
                ft.DataColumn(ft.Text("Longitude", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE), numeric=True),
                ft.DataColumn(ft.Text("Actions", weight=ft.FontWeight.BOLD, color=Colors.ON_SURFACE)),
            ],
            rows=[],
            border=ft.border.all(1, Colors.BORDER),
            border_radius=8,
            heading_row_color=Colors.SURFACE_VARIANT,
            data_row_max_height=48,
        )
        
        sites_card = ft.Container(
            content=ft.Column([
                ft.Row([
                    ft.Text("Registered Sites", size=16, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
                    ft.Container(expand=True),
                    self._import_btn,
                    ft.IconButton(
                        icon=ft.Icons.REFRESH,
                        tooltip="Refresh list",
                        on_click=self._on_refresh,
                    ),
                ]),
                ft.Container(height=8),
                self._sites_table,
            ]),
            bgcolor=Colors.SURFACE,
            padding=16,
            border_radius=8,
            border=ft.border.all(1, Colors.BORDER),
        )
        
        # === Main Layout ===
        self.content = ft.Column([
            header,
            description,
            ft.Container(height=16),
            add_form,
            ft.Container(height=16),
            sites_card,
        ], scroll=ft.ScrollMode.AUTO)
        
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL
        
        # Load initial data
        self._load_sites()
    
    def _load_sites(self):
        """Load sites from database into table."""
        sites = self.db.get_all_sites()
        
        rows = []
        for site in sites:
            rows.append(ft.DataRow(
                cells=[
                    ft.DataCell(ft.Text(site.code, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(site.name or "-", color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(f"{site.latitude:.4f}", color=Colors.ON_SURFACE)),
                    ft.DataCell(ft.Text(f"{site.longitude:.4f}", color=Colors.ON_SURFACE)),
                    ft.DataCell(
                        ft.IconButton(
                            icon=ft.Icons.DELETE_OUTLINE,
                            icon_color=Colors.ERROR,
                            tooltip="Delete site",
                            data=site.id,  # Store ID for deletion
                            on_click=self._on_delete_site,
                        )
                    ),
                ]
            ))
        
        self._sites_table.rows = rows
        
        if not sites:
            self._status_text.value = "No sites registered. Add sites above or import defaults."
            self._status_text.color = Colors.ON_SURFACE_VARIANT
        else:
            self._status_text.value = f"{len(sites)} site(s) registered"
            self._status_text.color = Colors.ON_SURFACE_VARIANT
    
    def _on_add_site(self, e):
        """Handle adding a new site."""
        code = self._code_field.value.strip().upper()
        name = self._name_field.value.strip()
        lat_str = self._lat_field.value.strip()
        lon_str = self._lon_field.value.strip()
        
        # Validation
        if not code:
            self._show_status("Error: Site code is required", error=True)
            return
        
        try:
            lat = float(lat_str)
            lon = float(lon_str)
        except ValueError:
            self._show_status("Error: Invalid latitude or longitude", error=True)
            return
        
        if not (-90 <= lat <= 90):
            self._show_status("Error: Latitude must be between -90 and 90", error=True)
            return
        
        if not (-180 <= lon <= 180):
            self._show_status("Error: Longitude must be between -180 and 180", error=True)
            return
        
        # Create site
        try:
            site = Site(
                code=code,
                name=name,
                latitude=lat,
                longitude=lon,
                created_at=datetime.now(),
            )
            self.db.create_site(site)
            
            # Clear form
            self._code_field.value = ""
            self._name_field.value = ""
            self._lat_field.value = ""
            self._lon_field.value = ""
            
            self._show_status(f"‚úì Site '{code}' added successfully")
            self._load_sites()
            self.update()
            
        except Exception as ex:
            if "UNIQUE constraint" in str(ex):
                self._show_status(f"Error: Site code '{code}' already exists", error=True)
            else:
                self._show_status(f"Error: {ex}", error=True)
    
    def _on_delete_site(self, e):
        """Handle deleting a site."""
        site_id = e.control.data
        if site_id:
            self.db.delete_site(site_id)
            self._show_status("Site deleted")
            self._load_sites()
            self.update()
    
    def _on_import_defaults(self, e):
        """Import default hardcoded sites."""
        added = self.db.seed_default_sites()
        if added > 0:
            self._show_status(f"‚úì Imported {added} default site(s)")
        else:
            self._show_status("No new sites to import (all defaults already exist)")
        self._load_sites()
        self.update()
    
    def _on_refresh(self, e):
        """Refresh the sites list."""
        self._load_sites()
        self.update()
    
    def _show_status(self, message: str, error: bool = False):
        """Show a status message."""
        self._status_text.value = message
        self._status_text.color = Colors.ERROR if error else Colors.SUCCESS


========================================
FILE: src/tempo_app/ui/pages/workspace.py
========================================
"""Workspace Page - Unified view for a dataset with Map, Export, and Sites.

This page provides a single-page layout combining:
- Left: Map preview with controls
- Right Sidebar: Sites list + Export panel
"""

import flet as ft
from pathlib import Path
from typing import Optional
import asyncio
import xarray as xr

from ..theme import Colors, Spacing
from ..components.widgets import SectionCard
from ...storage.database import Database
from ...storage.models import Dataset, Site
from ...core.plotter import MapPlotter
from ...core.exporter import DataExporter


class WorkspacePage(ft.Container):
    """Unified workspace for a dataset - single page layout."""

    def __init__(self, db: Database, data_dir: Path, dataset_id: str = None):
        super().__init__()
        self.db = db
        self.data_dir = data_dir
        self.dataset_id = dataset_id
        self.plotter = MapPlotter(data_dir)
        self.exporter = DataExporter(data_dir)

        self._dataset: Optional[Dataset] = None
        self._sites: list[Site] = []
        self._current_hour = 12

        self._build()

    def did_mount(self):
        """Called when control is added to page - load data async."""
        import logging
        logging.info(f"WorkspacePage.did_mount called, dataset_id={self.dataset_id}")
        self.page.run_task(self._load_datasets_async)

    async def _load_datasets_async(self):
        """Load all datasets into dropdown."""
        import logging
        logging.info("Loading all datasets...")
        
        # Load all datasets for dropdown
        datasets = await asyncio.to_thread(self.db.get_all_datasets)
        logging.info(f"Found {len(datasets)} datasets")
        
        # Populate dropdown
        options = []
        for ds in datasets:
            options.append(ft.DropdownOption(key=ds.id, text=ds.name))
        self._dataset_dropdown.options = options
        
        # Select initial dataset
        if self.dataset_id:
            self._dataset_dropdown.value = self.dataset_id
        elif options:
            self._dataset_dropdown.value = options[0].key
        
        # Load selected dataset data
        await self._load_selected_dataset()
        self.update()

    async def _load_selected_dataset(self):
        """Load the currently selected dataset's data."""
        import logging
        dataset_id = self._dataset_dropdown.value if hasattr(self, '_dataset_dropdown') and self._dataset_dropdown.value else self.dataset_id
        
        if not dataset_id:
            self._dataset_title.value = "No datasets available"
            self._dataset_title.value = "No datasets available"
            logging.warning("No dataset selected")
            return
            
        self._dataset = await asyncio.to_thread(self.db.get_dataset, dataset_id)
        logging.info(f"Loaded dataset: {self._dataset}")
        
        if self._dataset:
            self._dataset_title.value = f"üìä {self._dataset.name}"
            self._sites = await asyncio.to_thread(
                self.db.get_sites_in_bbox, self._dataset.bbox
            )
            logging.info(f"Found {len(self._sites)} sites in bbox")
            self._update_sites_list()
            logging.info(f"Loaded: {self._dataset.name}")
            logging.info(f"Found {len(self._sites)} sites in bounds")
            
            # Load available hours from dataset file
            await self._load_available_hours()
        else:
            self._dataset_title.value = "Dataset not found"
            logging.error(f"Dataset not found: {dataset_id}")

    async def _load_available_hours(self):
        """Load available hours from the dataset file and update slider."""
        import logging
        import pandas as pd
        try:
            if not self._dataset or not self._dataset.file_path:
                return
            
            processed_path = Path(self._dataset.file_path)
            if not processed_path.exists():
                return
                
            ds = await asyncio.to_thread(xr.open_dataset, processed_path)
            
            available_hours = []
            num_timesteps = 0
            
            # Check for TIME (new format) or TSTEP (old format) datetime dimensions
            if 'TIME' in ds.dims:
                timestamps = pd.to_datetime(ds.TIME.values)
                available_hours = sorted(set(timestamps.hour.tolist()))
                num_timesteps = len(timestamps)
                logging.info(f"Dataset has {num_timesteps} timesteps ({timestamps[0].date()} to {timestamps[-1].date()})")
            elif 'TSTEP' in ds.dims:
                timestamps = pd.to_datetime(ds.TSTEP.values)
                available_hours = sorted(set(timestamps.hour.tolist()))
                num_timesteps = len(timestamps)
                logging.info(f"Dataset has {num_timesteps} timesteps ({timestamps[0].date()} to {timestamps[-1].date()})")
            # Fallback to HOUR dimension (old aggregated format)
            elif 'HOUR' in ds.coords:
                available_hours = sorted(ds.HOUR.values.tolist())
            elif 'hour' in ds.coords:
                available_hours = sorted(ds.hour.values.tolist())
            
            ds.close()
            
            if available_hours:
                self._available_hours = available_hours
                min_hour = int(min(available_hours))
                max_hour = int(max(available_hours))
                
                # Update slider range and value
                self._hour_slider.min = min_hour
                self._hour_slider.max = max_hour
                self._hour_slider.divisions = max(1, max_hour - min_hour)
                self._hour_slider.value = min_hour
                self._current_hour = min_hour
                self._hour_text.value = f"Hour: {min_hour} UTC"
                
                hours_str = ", ".join(f"{h}" for h in available_hours)
                logging.info(f"Available hours: {hours_str}")
                logging.info(f"Set hour slider: min={min_hour}, max={max_hour}")
        except Exception as e:
            import logging
            logging.error(f"Failed to load available hours: {e}")

    def _on_dataset_change(self, e):
        """Handle dataset selection change."""
        self.page.run_task(self._on_dataset_change_async)

    async def _on_dataset_change_async(self):
        """Load newly selected dataset."""
        await self._load_selected_dataset()
        self.update()


    def _build(self):
        """Build the unified workspace layout."""
        # === HEADER with Dataset Selector ===
        self._dataset_dropdown = ft.Dropdown(
            label="Dataset",
            border_color=Colors.BORDER,
            focused_border_color=Colors.PRIMARY,
            bgcolor=Colors.SURFACE_VARIANT,
            width=300,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
        )
        self._dataset_dropdown.on_change = self._on_dataset_change

        self._dataset_title = ft.Text(
            "Select a dataset",
            size=16,
            color=Colors.ON_SURFACE_VARIANT,
        )

        header = ft.Container(
            content=ft.Row([
                ft.IconButton(
                    icon=ft.Icons.ARROW_BACK,
                    icon_color=Colors.ON_SURFACE,
                    tooltip="Back to Library",
                    on_click=self._on_back_click,
                ),
                self._dataset_dropdown,
                ft.Container(width=16),
                self._dataset_title,
            ], spacing=8, vertical_alignment=ft.CrossAxisAlignment.CENTER),
            padding=ft.padding.only(bottom=Spacing.SM),
        )


        # === LEFT COLUMN: Map Generation ===
        left_column = self._build_map_section()

        # === RIGHT SIDEBAR: Sites + Export ===
        right_sidebar = self._build_sidebar()

        # === MAIN LAYOUT ===
        main_content = ft.Row([
            ft.Container(content=left_column, expand=True),
            ft.Container(
                content=right_sidebar,
                width=300,
                padding=ft.padding.only(left=Spacing.MD),
            ),
        ], expand=True, spacing=0)

        self.content = ft.Column([
            header,
            main_content,
        ], expand=True)
        self.expand = True
        self.padding = Spacing.PAGE_HORIZONTAL

    def _build_map_section(self):
        """Build the map preview and controls section (left column)."""
        # Variable selector - keys must match plotter expectations: 'NO2', 'HCHO', 'FNR'
        self._variable_dropdown = ft.Dropdown(
            label="Variable",
            value="FNR",
            options=[
                ft.DropdownOption(key="FNR", text="FNR (HCHO/NO2)"),
                ft.DropdownOption(key="NO2", text="NO2 Tropospheric VCD"),
                ft.DropdownOption(key="HCHO", text="HCHO Total VCD"),
            ],
            width=200,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            dense=True,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
        )

        # Road options
        self._road_dropdown = ft.Dropdown(
            label="Roads",
            value="primary",
            options=[
                ft.DropdownOption(key="primary", text="Interstates Only"),
                ft.DropdownOption(key="major", text="Major Roads"),
                ft.DropdownOption(key="all", text="All Roads"),
            ],
            width=140,
            border_color=Colors.BORDER,
            bgcolor=Colors.SURFACE_VARIANT,
            dense=True,
            text_style=ft.TextStyle(color=Colors.ON_SURFACE),
            label_style=ft.TextStyle(color=Colors.ON_SURFACE_VARIANT),
        )

        self._show_sites_checkbox = ft.Checkbox(
            label="Show Sites",
            value=True,
            label_style=ft.TextStyle(color=Colors.ON_SURFACE),
            on_change=self._on_show_sites_change,
        )

        # Hour slider
        self._hour_slider = ft.Slider(
            min=0, max=23, divisions=23, value=12,
            label="{value}",
            on_change=self._on_hour_change,
            expand=True,
        )
        self._hour_text = ft.Text("Hour: 12 UTC", size=13, width=90, color=Colors.ON_SURFACE)

        # Generate button
        self._generate_btn = ft.FilledButton(
            content=ft.Row([
                ft.Icon(ft.Icons.MAP, size=18),
                ft.Text("Generate Map", color=Colors.ON_PRIMARY),
            ], spacing=6, tight=True),
            on_click=self._on_generate_click,
        )

        # Map image display
        self._map_image = ft.Image(
            src="",
            visible=False,
        )

        self._map_placeholder = ft.Container(
            content=ft.Column([
                ft.Icon(ft.Icons.MAP, size=64, color=Colors.ON_SURFACE_VARIANT),
                ft.Text("Select options and click Generate Map", 
                        color=Colors.ON_SURFACE_VARIANT),
            ], horizontal_alignment=ft.CrossAxisAlignment.CENTER, spacing=16),
            alignment=ft.Alignment(0, 0),
            expand=True,
            bgcolor=Colors.SURFACE_VARIANT,
            border_radius=8,
        )

        self._progress_bar = ft.ProgressBar(visible=False, color=Colors.PRIMARY)

        # Status log (shared for map and export)


        # Controls row
        controls_row = ft.Row([
            self._variable_dropdown,
            self._road_dropdown,
            self._show_sites_checkbox,
            ft.Container(expand=True),
            self._generate_btn,
        ], vertical_alignment=ft.CrossAxisAlignment.CENTER, spacing=8)

        # Hour row
        hour_row = ft.Row([
            self._hour_text,
            self._hour_slider,
        ], vertical_alignment=ft.CrossAxisAlignment.CENTER)

        # Map container
        map_container = ft.Container(
            content=ft.Stack(
                [
                    self._map_placeholder,
                    self._map_image,
                ],
                fit=ft.StackFit.EXPAND,
            ),
            expand=True,
            border=ft.border.all(1, Colors.BORDER),
            border_radius=8,
            clip_behavior=ft.ClipBehavior.HARD_EDGE,
        )

        return ft.Column([
            controls_row,
            ft.Container(height=4),
            hour_row,
            self._progress_bar,
            ft.Container(height=4),
            map_container,
        ], expand=True, spacing=0)

    def _build_sidebar(self):
        """Build the right sidebar with Sites and Export panels."""
        # === SITES SECTION ===
        self._sites_list = ft.ListView(spacing=4, expand=True)
        self._sites_count = ft.Text(
            "0 sites",
            size=12,
            color=Colors.ON_SURFACE_VARIANT,
        )

        sites_header = ft.Row([
            ft.Icon(ft.Icons.LOCATION_ON, size=18, color=Colors.PRIMARY),
            ft.Text("Sites", size=14, weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE),
            ft.Container(expand=True),
            self._sites_count,
        ])

        manage_sites_btn = ft.TextButton(
            content=ft.Row([
                ft.Icon(ft.Icons.SETTINGS, size=16, color=Colors.PRIMARY),
                ft.Text("Manage Sites...", color=Colors.PRIMARY),
            ], spacing=4, tight=True),
            on_click=self._on_manage_sites,
        )

        sites_section = ft.Container(
            content=ft.Column([
                sites_header,
                ft.Container(
                    content=self._sites_list,
                    expand=True,
                    border=ft.border.all(1, Colors.BORDER),
                    border_radius=4,
                    padding=4,
                ),
                manage_sites_btn,
            ], spacing=6),
            expand=True,
            padding=Spacing.SM,
            bgcolor=Colors.SURFACE,
            border_radius=8,
            border=ft.border.all(1, Colors.BORDER),
        )

        # === EXPORT BUTTON ===
        self._export_nav_btn = ft.FilledButton(
            content=ft.Row([
                ft.Icon(ft.Icons.FILE_DOWNLOAD, size=18),
                ft.Text("Export Data...", color=Colors.ON_PRIMARY),
            ], spacing=6, tight=True),
            on_click=self._on_export_nav_click,
            style=ft.ButtonStyle(
                padding=ft.padding.all(20),
            )
        )

        return ft.Column([
            sites_section,
            ft.Container(height=16),
            ft.Container(
                content=self._export_nav_btn,
                alignment=ft.Alignment(0, 0)
            )
        ], expand=True)

    def _update_sites_list(self):
        """Update the sites list display."""
        self._sites_list.controls.clear()
        self._sites_count.value = f"{len(self._sites)} sites"

        for i, site in enumerate(self._sites):
            # Fallback name if code is missing/empty
            site_name = site.code if site.code else f"Site #{i+1}"
            
            self._sites_list.controls.append(
                ft.Container(
                    content=ft.Row([
                        ft.Container(
                            content=ft.Icon(ft.Icons.PLACE, size=16, color=Colors.PRIMARY),
                            padding=8,
                            bgcolor=Colors.SURFACE,
                            border_radius=8,
                        ),
                        ft.Column([
                            ft.Text(site_name, weight=ft.FontWeight.W_600, size=14, color=Colors.ON_SURFACE_VARIANT),
                            ft.Text(
                                f"{site.latitude:.4f}, {site.longitude:.4f}",
                                size=11,
                                color=Colors.ON_SURFACE_VARIANT,
                                opacity=0.7,
                            ),
                        ], spacing=2, expand=True),
                    ], spacing=12),
                    padding=ft.padding.all(8),
                    bgcolor=Colors.SURFACE_VARIANT,
                    border_radius=8,
                    border=ft.border.all(1, Colors.BORDER),
                )
            )

    # === EVENT HANDLERS ===

    def _on_back_click(self, e):
        """Navigate back to library."""
        if self.page:
            shell = self.page.controls[0] if self.page.controls else None
            if shell and hasattr(shell, 'navigate_to'):
                shell.navigate_to("/library")

    def _on_manage_sites(self, e):
        """Navigate to sites management page."""
        if self.page:
            shell = self.page.controls[0] if self.page.controls else None
            if shell and hasattr(shell, 'navigate_to'):
                shell.navigate_to("/sites")

    def _on_hour_change(self, e):
        """Handle hour slider change."""
        hour = int(e.control.value)
        self._current_hour = hour
        self._hour_text.value = f"Hour: {hour} UTC"
        self.update()

    def _on_show_sites_change(self, e):
        """Handle show sites checkbox change - regenerate map."""
        if self._dataset and self._map_image.visible:
            # Only regenerate if we have a map already displayed
            self.page.run_task(self._generate_map_async)

    def _on_generate_click(self, e):
        """Generate the map."""
        if not self._dataset:
            logging.error("No dataset loaded")
            return
        self.page.run_task(self._generate_map_async)

    async def _generate_map_async(self):
        """Generate map asynchronously."""
        import logging
        self._progress_bar.visible = True
        logging.info(f"Generating map for hour {self._current_hour}...")
        self.update()

        try:
            # Find processed file
            if self._dataset.file_path and Path(self._dataset.file_path).exists():
                processed_path = Path(self._dataset.file_path)
            else:
                safe_name = "".join(c if c.isalnum() or c in "._- " else "_" for c in self._dataset.name)
                processed_path = self.data_dir / "datasets" / safe_name / f"{safe_name}_processed.nc"

            logging.info(f"Looking for processed file: {processed_path}")
            
            if not processed_path.exists():
                logging.error("Processed data not found")
                self._progress_bar.visible = False
                self.update()
                return

            ds = await asyncio.to_thread(xr.open_dataset, processed_path)
            logging.info(f"Opened dataset with dims: {list(ds.dims)}")

            # Get sites if checkbox checked, otherwise pass empty dict to hide sites
            if self._show_sites_checkbox.value:
                sites = {s.code: s.to_tuple() for s in self._sites}
            else:
                sites = {}  # Empty dict = no sites shown

            variable = self._variable_dropdown.value
            road_detail = self._road_dropdown.value
            hour = self._current_hour
            
            logging.info(f"Calling plotter.generate_map(variable={variable}, hour={hour})")

            plot_path = await asyncio.to_thread(
                self.plotter.generate_map,
                ds,
                hour,
                variable,
                self._dataset.name,
                self._dataset.bbox.to_list(),
                road_detail,
                sites,
            )

            ds.close()
            
            logging.info(f"Plotter returned: {plot_path}")
            logging.info(f"Plotter returned: {plot_path}")

            if plot_path:
                if Path(plot_path).exists():
                    self._map_image.src = plot_path
                    self._map_image.visible = True
                    self._map_placeholder.visible = False
                    logging.info(f"Map generated: {variable} at {hour}:00 UTC")
                    logging.info(f"Map image set to: {plot_path}")
                else:
                    logging.error(f"Plot path doesn't exist: {plot_path}")
            else:
                logging.error(f"No map returned for hour {hour}")

        except Exception as ex:
            import traceback
            logging.error(f"Error: {ex}")
            logging.error(f"Map generation error: {ex}")
            traceback.print_exc()
        finally:
            self._progress_bar.visible = False
            self.update()

    async def _on_export_nav_click(self, e):
        """Navigate to export page with current dataset."""
        if not self._dataset:
            logging.warning("Select a dataset first to export.")
            return
            
        if self.page:
            shell = self.page.controls[0]
            if shell and hasattr(shell, 'navigate_to'):
                 # Pass dataset_id in route
                shell.navigate_to(f"/export/{self._dataset.id}")



========================================
FILE: src/tempo_app/ui/pages/__init__.py
========================================
"""UI Pages - Individual screen implementations."""


========================================
FILE: src/tempo_app/ui/shell.py
========================================
"""Application shell with top navigation bar and download manager."""

import flet as ft
from typing import Callable, Optional
from dataclasses import dataclass, field
from datetime import datetime

from .theme import Colors, Spacing, Sizing


@dataclass
class DownloadItem:
    """Represents an active download."""
    id: str
    name: str
    progress: float = 0.0  # 0.0 to 1.0
    status: str = "downloading"  # downloading, complete, error, cancelled
    started_at: datetime = field(default_factory=datetime.now)


class DownloadManager:
    """Global download state manager."""
    
    def __init__(self, on_change: Callable = None):
        self._downloads: dict[str, DownloadItem] = {}
        self._on_change = on_change
    
    def add_download(self, id: str, name: str) -> DownloadItem:
        """Add a new download."""
        item = DownloadItem(id=id, name=name)
        self._downloads[id] = item
        self._notify()
        return item
    
    def update_progress(self, id: str, progress: float):
        """Update download progress (0.0 to 1.0)."""
        if id in self._downloads:
            self._downloads[id].progress = min(1.0, max(0.0, progress))
            self._notify()
    
    def complete(self, id: str):
        """Mark download as complete."""
        if id in self._downloads:
            self._downloads[id].status = "complete"
            self._downloads[id].progress = 1.0
            self._notify()
    
    def error(self, id: str):
        """Mark download as errored."""
        if id in self._downloads:
            self._downloads[id].status = "error"
            self._notify()
    
    def cancel(self, id: str):
        """Cancel a download."""
        if id in self._downloads:
            self._downloads[id].status = "cancelled"
            self._notify()
    
    def remove(self, id: str):
        """Remove a download from the list."""
        if id in self._downloads:
            del self._downloads[id]
            self._notify()
    
    def get_active(self) -> list[DownloadItem]:
        """Get list of active (non-complete) downloads."""
        return [d for d in self._downloads.values() if d.status == "downloading"]
    
    def get_all(self) -> list[DownloadItem]:
        """Get all downloads."""
        return list(self._downloads.values())
    
    @property
    def active_count(self) -> int:
        """Number of active downloads."""
        return len(self.get_active())
    
    def _notify(self):
        """Notify listeners of state change."""
        if self._on_change:
            self._on_change()


class NavigationItem:
    """Navigation tab item configuration."""
    
    def __init__(
        self,
        icon: str,
        selected_icon: str,
        label: str,
        route: str,
    ):
        self.icon = icon
        self.selected_icon = selected_icon
        self.label = label
        self.route = route


# Navigation items - organized by user journey frequency
NAV_ITEMS = [
    NavigationItem(
        icon=ft.Icons.FOLDER_OUTLINED,
        selected_icon=ft.Icons.FOLDER,
        label="Library",
        route="/library",
    ),
    NavigationItem(
        icon=ft.Icons.ADD_CHART_OUTLINED,
        selected_icon=ft.Icons.ADD_CHART,
        label="New Dataset",
        route="/new",
    ),
    NavigationItem(
        icon=ft.Icons.UPLOAD_FILE_OUTLINED,
        selected_icon=ft.Icons.UPLOAD_FILE,
        label="Batch Import",
        route="/batch",
    ),
    NavigationItem(
        icon=ft.Icons.MAP_OUTLINED,
        selected_icon=ft.Icons.MAP,
        label="Workspace",
        route="/workspace",
    ),
    NavigationItem(
        icon=ft.Icons.FILE_DOWNLOAD_OUTLINED,
        selected_icon=ft.Icons.FILE_DOWNLOAD,
        label="Export",
        route="/export",
    ),
]


class DownloadDropdown(ft.Container):
    """Dropdown panel showing active downloads."""
    
    def __init__(self, download_manager: DownloadManager, on_view: Callable[[str], None] = None, on_cancel: Callable[[str], None] = None):
        super().__init__()
        self.download_manager = download_manager
        self._on_view = on_view
        self._on_cancel = on_cancel
        self._build()
    
    def _build(self):
        """Build the dropdown content."""
        downloads = self.download_manager.get_all()
        
        if not downloads:
            content = ft.Container(
                content=ft.Text("No active downloads", color=Colors.ON_SURFACE_VARIANT, size=13),
                padding=Spacing.MD,
            )
        else:
            items = []
            for dl in downloads:
                status_icon = ft.Icons.DOWNLOADING if dl.status == "downloading" else \
                              ft.Icons.CHECK_CIRCLE if dl.status == "complete" else \
                              ft.Icons.ERROR if dl.status == "error" else ft.Icons.CANCEL
                status_color = Colors.INFO if dl.status == "downloading" else \
                               Colors.SUCCESS if dl.status == "complete" else \
                               Colors.ERROR
                
                item = ft.Container(
                    content=ft.Column([
                        ft.Row([
                            ft.Icon(status_icon, color=status_color, size=16),
                            ft.Text(dl.name, color=Colors.ON_SURFACE, size=13, expand=True),
                            # View button
                            ft.IconButton(
                                icon=ft.Icons.OPEN_IN_NEW,
                                icon_size=16,
                                icon_color=Colors.PRIMARY,
                                tooltip="View in Workspace",
                                on_click=lambda e, id=dl.id: self._on_view(id) if self._on_view else None,
                            ) if dl.status in ("complete", "downloading") else ft.Container(),
                            # Cancel button
                            ft.IconButton(
                                icon=ft.Icons.CLOSE,
                                icon_size=16,
                                icon_color=Colors.ERROR,
                                tooltip="Cancel",
                                on_click=lambda e, id=dl.id: self._on_cancel(id) if self._on_cancel else None,
                            ) if dl.status == "downloading" else ft.Container(),
                        ], spacing=4),
                        # Progress bar for active downloads
                        ft.ProgressBar(
                            value=dl.progress,
                            bgcolor=Colors.SURFACE_VARIANT,
                            color=Colors.PRIMARY,
                            height=4,
                        ) if dl.status == "downloading" else ft.Container(),
                    ], spacing=4),
                    padding=ft.padding.symmetric(horizontal=Spacing.SM, vertical=Spacing.XS),
                )
                items.append(item)
            
            content = ft.Column(items, spacing=0, scroll=ft.ScrollMode.AUTO)
        
        self.content = ft.Container(
            content=ft.Column([
                ft.Container(
                    content=ft.Text("Downloads", weight=ft.FontWeight.W_600, color=Colors.ON_SURFACE, size=14),
                    padding=ft.padding.only(left=Spacing.MD, right=Spacing.MD, top=Spacing.SM, bottom=Spacing.XS),
                ),
                ft.Divider(height=1, color=Colors.DIVIDER),
                content,
            ], spacing=0),
            width=280,
            bgcolor=Colors.SURFACE,
            border_radius=8,
            border=ft.border.all(1, Colors.BORDER),
            shadow=ft.BoxShadow(
                spread_radius=0,
                blur_radius=8,
                color=Colors.CARD_SHADOW,
                offset=ft.Offset(0, 4),
            ),
        )
    
    def refresh(self):
        """Refresh the dropdown content."""
        self._build()


class AppShell(ft.Container):
    """Main application shell with top navigation and content area."""
    
    def __init__(
        self,
        page: ft.Page,
        on_route_change: Callable[[str], None] = None,
    ):
        super().__init__()
        self._page = page
        self._on_route_change_callback = on_route_change
        self._selected_index = 0
        self._download_dropdown_visible = False
        
        # Download manager
        self.download_manager = DownloadManager(on_change=self._on_downloads_changed)
        
        # Content placeholder
        self._content_area = ft.Container(
            expand=True,
            padding=Spacing.PAGE_HORIZONTAL,
        )
        
        # Build the shell
        self._build()
    
    def _build(self):
        """Build the shell layout."""
        # Create navigation tabs
        self._nav_tabs = []
        for i, item in enumerate(NAV_ITEMS):
            is_selected = i == self._selected_index
            tab = ft.Container(
                content=ft.Row([
                    ft.Icon(
                        item.selected_icon if is_selected else item.icon,
                        color=Colors.PRIMARY if is_selected else Colors.ON_SURFACE_VARIANT,
                        size=20,
                    ),
                    ft.Text(
                        item.label,
                        color=Colors.PRIMARY if is_selected else Colors.ON_SURFACE,
                        weight=ft.FontWeight.W_600 if is_selected else ft.FontWeight.NORMAL,
                        size=14,
                    ),
                ], spacing=6),
                padding=ft.padding.symmetric(horizontal=Spacing.MD, vertical=Spacing.SM),
                border_radius=6,
                bgcolor=Colors.PRIMARY_CONTAINER if is_selected else None,
                on_click=lambda e, idx=i: self._on_tab_click(idx),
            )
            self._nav_tabs.append(tab)
        
        # Download badge
        self._download_badge = ft.Container(
            content=ft.Text(
                str(self.download_manager.active_count),
                color=Colors.ON_PRIMARY,
                size=10,
                weight=ft.FontWeight.BOLD,
            ),
            width=16,
            height=16,
            border_radius=8,
            bgcolor=Colors.ERROR,
            alignment=ft.Alignment(0, 0),
            visible=self.download_manager.active_count > 0,
        )
        
        # Download button with badge
        self._download_button = ft.Stack([
            ft.IconButton(
                icon=ft.Icons.DOWNLOAD,
                icon_color=Colors.ON_SURFACE_VARIANT,
                tooltip="Downloads",
                on_click=self._toggle_download_dropdown,
            ),
            ft.Container(
                content=self._download_badge,
                right=4,
                top=4,
            ),
        ])
        
        # Download dropdown
        self._download_dropdown = DownloadDropdown(
            self.download_manager,
            on_view=self._on_download_view,
            on_cancel=self._on_download_cancel,
        )
        self._download_dropdown.visible = False
        
        # Settings button
        settings_button = ft.IconButton(
            icon=ft.Icons.SETTINGS_OUTLINED,
            icon_color=Colors.ON_SURFACE_VARIANT,
            tooltip="Settings",
            on_click=lambda _: self._navigate_to("/settings"),
        )
        
        # App bar with navigation tabs
        self._app_bar = ft.Container(
            content=ft.Row(
                controls=[
                    # Logo and title
                    ft.Row(
                        controls=[
                            ft.Icon(ft.Icons.SATELLITE_ALT, color=Colors.PRIMARY, size=28),
                            ft.Text(
                                "TEMPO Analyzer",
                                size=20,
                                weight=ft.FontWeight.W_600,
                                color=Colors.ON_SURFACE,
                            ),
                        ],
                        spacing=Spacing.SM,
                    ),
                    ft.Container(width=Spacing.XL),
                    # Navigation tabs
                    ft.Row(
                        controls=self._nav_tabs,
                        spacing=Spacing.XS,
                    ),
                    # Spacer
                    ft.Container(expand=True),
                    # Settings button
                    settings_button,
                ],
            ),
            padding=ft.padding.symmetric(horizontal=Spacing.LG, vertical=Spacing.SM),
            bgcolor=Colors.SURFACE,
            border=ft.border.only(bottom=ft.BorderSide(1, Colors.BORDER)),
        )
        
        # Main layout
        self.content = ft.Column(
            controls=[
                self._app_bar,
                self._content_area,
            ],
            spacing=0,
            expand=True,
        )
        
        self.bgcolor = Colors.BACKGROUND
        self.expand = True
    
    def _on_tab_click(self, index: int):
        """Handle tab click."""
        self._selected_index = index
        route = NAV_ITEMS[index].route
        self._update_tab_styles(defer_update=True)
        self._navigate_to(route, skip_tab_update=True)
    
    def _update_tab_styles(self, defer_update: bool = False):
        """Update tab visual states.

        Args:
            defer_update: If True, skip page.update() to allow batching.
        """
        for i, tab in enumerate(self._nav_tabs):
            is_selected = i == self._selected_index
            item = NAV_ITEMS[i]

            # Update icon
            tab.content.controls[0].name = item.selected_icon if is_selected else item.icon
            tab.content.controls[0].color = Colors.PRIMARY if is_selected else Colors.ON_SURFACE_VARIANT

            # Update text
            tab.content.controls[1].color = Colors.PRIMARY if is_selected else Colors.ON_SURFACE
            tab.content.controls[1].weight = ft.FontWeight.W_600 if is_selected else ft.FontWeight.NORMAL

            # Update background
            tab.bgcolor = Colors.PRIMARY_CONTAINER if is_selected else None

        if not defer_update and self._page:
            self._page.update()
    
    def _toggle_download_dropdown(self, e):
        """Toggle download dropdown visibility."""
        self._download_dropdown_visible = not self._download_dropdown_visible
        self._download_dropdown.visible = self._download_dropdown_visible
        if self._download_dropdown_visible:
            self._download_dropdown.refresh()
        self._page.update()
    
    def _on_downloads_changed(self):
        """Handle download state changes."""
        count = self.download_manager.active_count
        self._download_badge.content.value = str(count)
        self._download_badge.visible = count > 0
        if self._download_dropdown_visible:
            self._download_dropdown.refresh()
        if self._page:
            self._page.update()
    
    def _on_download_view(self, download_id: str):
        """Handle view download click."""
        self._download_dropdown_visible = False
        self._download_dropdown.visible = False
        # Navigate to workspace with this dataset
        self._navigate_to(f"/workspace/{download_id}")
    
    def _on_download_cancel(self, download_id: str):
        """Handle cancel download click."""
        self.download_manager.cancel(download_id)
    
    def _navigate_to(self, route: str, skip_tab_update: bool = False):
        """Navigate to a route.

        Args:
            route: The route to navigate to.
            skip_tab_update: If True, skip tab style update (already done by caller).
        """
        # Update tab selection if it's a main nav route (unless already done)
        if not skip_tab_update:
            for i, item in enumerate(NAV_ITEMS):
                if route.startswith(item.route):
                    self._selected_index = i
                    self._update_tab_styles(defer_update=True)
                    break

        # Close download dropdown
        self._download_dropdown_visible = False
        self._download_dropdown.visible = False

        # Notify callback (this sets content and triggers update)
        if self._on_route_change_callback:
            self._on_route_change_callback(route)
            # Callback's set_content handles the page.update()
        elif self._page:
            # Only update directly if no callback
            self._page.update()
    
    def set_content(self, content: ft.Control, defer_update: bool = False):
        """Set the main content area.

        Args:
            content: The content control to display.
            defer_update: If True, skip page.update() (caller handles it).
        """
        self._content_area.content = content
        if not defer_update and self._page:
            self._page.update()
    
    def navigate_to(self, route: str):
        """Public method to navigate to a route."""
        self._navigate_to(route)
    
    @property
    def selected_route(self) -> str:
        """Get the currently selected route."""
        return NAV_ITEMS[self._selected_index].route


========================================
FILE: src/tempo_app/ui/theme.py
========================================
"""Theme configuration for TEMPO Analyzer.

Material Design 3 color scheme optimized for data visualization.
"""

import flet as ft


# =============================================================================
# Color Palette (Dark Theme First)
# =============================================================================

class Colors:
    """Application color constants (Light Mode)."""
    
    # Primary palette
    PRIMARY = "#5E35B1"           # Deep Purple 600
    PRIMARY_CONTAINER = "#EDE7F6"
    ON_PRIMARY = "#FFFFFF"
    
    # Surface colors (Light Mode)
    BACKGROUND = "#FAFAFA"        # Light gray
    SURFACE = "#FFFFFF"           # White card backgrounds
    SURFACE_VARIANT = "#F5F5F5"   # Slightly darker surfaces
    ON_SURFACE = "#212121"        # Primary text (dark)
    ON_SURFACE_VARIANT = "#757575"  # Secondary text (gray)
    
    # Status colors
    SUCCESS = "#43A047"           # Green
    SUCCESS_CONTAINER = "#E8F5E9"
    WARNING = "#FB8C00"           # Orange
    WARNING_CONTAINER = "#FFF3E0"
    ERROR = "#E53935"             # Red
    ERROR_CONTAINER = "#FFEBEE"
    INFO = "#1E88E5"              # Blue
    
    # "On" colors for containers
    ON_PRIMARY_CONTAINER = "#311B92"  # Deep Purple 900
    ON_SUCCESS_CONTAINER = "#1B5E20"  # Green 900
    ON_WARNING_CONTAINER = "#E65100"  # Orange 900
    ON_ERROR_CONTAINER = "#B71C1C"    # Red 900
    
    # Map-specific colors
    NO2_LOW = "#1A237E"           # Deep blue
    NO2_HIGH = "#FFD600"          # Yellow
    HCHO_LOW = "#311B92"          # Deep purple
    HCHO_HIGH = "#FF6D00"         # Deep orange
    FNR_VOC = "#3F51B5"           # Blue (VOC-limited)
    FNR_TRANSITION = "#9E9E9E"    # Gray (transition)
    FNR_NOX = "#F44336"           # Red (NOx-limited)
    
    # UI elements
    BORDER = "#E0E0E0"
    DIVIDER = "#EEEEEE"
    CARD_SHADOW = "#00000020"


class LightColors:
    """Light theme colors (for future use)."""
    
    PRIMARY = "#5E35B1"           # Deep Purple 600
    PRIMARY_CONTAINER = "#EDE7F6"
    ON_PRIMARY = "#FFFFFF"
    
    BACKGROUND = "#FAFAFA"
    SURFACE = "#FFFFFF"
    SURFACE_VARIANT = "#F5F5F5"
    ON_SURFACE = "#212121"
    ON_SURFACE_VARIANT = "#757575"
    
    SUCCESS = "#43A047"
    WARNING = "#FB8C00"
    ERROR = "#E53935"
    INFO = "#1E88E5"
    
    BORDER = "#E0E0E0"
    DIVIDER = "#EEEEEE"


# =============================================================================
# Typography
# =============================================================================

class Typography:
    """Font configurations."""
    
    # Font families
    FAMILY_PRIMARY = "Inter, Roboto, sans-serif"
    FAMILY_MONO = "JetBrains Mono, Consolas, monospace"
    
    # Font sizes
    DISPLAY_LARGE = 57
    DISPLAY_MEDIUM = 45
    DISPLAY_SMALL = 36
    HEADLINE_LARGE = 32
    HEADLINE_MEDIUM = 28
    HEADLINE_SMALL = 24
    TITLE_LARGE = 22
    TITLE_MEDIUM = 16
    TITLE_SMALL = 14
    BODY_LARGE = 16
    BODY_MEDIUM = 14
    BODY_SMALL = 12
    LABEL_LARGE = 14
    LABEL_MEDIUM = 12
    LABEL_SMALL = 11


# =============================================================================
# Spacing & Sizing
# =============================================================================

class Spacing:
    """Spacing constants."""
    
    XS = 4
    SM = 8
    MD = 16
    LG = 24
    XL = 32
    XXL = 48
    
    # Page padding
    PAGE_HORIZONTAL = 24
    PAGE_VERTICAL = 16
    
    # Card padding
    CARD_PADDING = 16
    
    # Navigation rail width
    NAV_RAIL_WIDTH = 80
    NAV_RAIL_EXPANDED = 200


class Sizing:
    """Component size constants."""
    
    # Window
    WINDOW_MIN_WIDTH = 1200
    WINDOW_MIN_HEIGHT = 700
    WINDOW_DEFAULT_WIDTH = 1600
    WINDOW_DEFAULT_HEIGHT = 900
    
    # Cards
    CARD_BORDER_RADIUS = 12
    
    # Buttons
    BUTTON_HEIGHT = 40
    BUTTON_BORDER_RADIUS = 8
    
    # Inputs
    INPUT_HEIGHT = 48
    INPUT_BORDER_RADIUS = 8
    
    # Icons
    ICON_SM = 16
    ICON_MD = 24
    ICON_LG = 32


# =============================================================================
# Theme Builder
# =============================================================================

def create_dark_theme() -> ft.Theme:
    """Create the dark theme for the application."""
    return ft.Theme(
        color_scheme_seed=Colors.PRIMARY,
        visual_density=ft.VisualDensity.COMFORTABLE,
    )


def create_light_theme(font_scale: float = 1.0) -> ft.Theme:
    """Create the light theme for the application."""
    t = ft.Theme(
        color_scheme_seed=LightColors.PRIMARY,
        visual_density=ft.VisualDensity.COMFORTABLE,
    )
    
    # Scale typography
    t.text_theme = ft.TextTheme(
        display_large=ft.TextStyle(size=Typography.DISPLAY_LARGE * font_scale),
        display_medium=ft.TextStyle(size=Typography.DISPLAY_MEDIUM * font_scale),
        display_small=ft.TextStyle(size=Typography.DISPLAY_SMALL * font_scale),
        headline_large=ft.TextStyle(size=Typography.HEADLINE_LARGE * font_scale),
        headline_medium=ft.TextStyle(size=Typography.HEADLINE_MEDIUM * font_scale),
        headline_small=ft.TextStyle(size=Typography.HEADLINE_SMALL * font_scale),
        title_large=ft.TextStyle(size=Typography.TITLE_LARGE * font_scale),
        title_medium=ft.TextStyle(size=Typography.TITLE_MEDIUM * font_scale),
        title_small=ft.TextStyle(size=Typography.TITLE_SMALL * font_scale),
        body_large=ft.TextStyle(size=Typography.BODY_LARGE * font_scale),
        body_medium=ft.TextStyle(size=Typography.BODY_MEDIUM * font_scale),
        body_small=ft.TextStyle(size=Typography.BODY_SMALL * font_scale),
        label_large=ft.TextStyle(size=Typography.LABEL_LARGE * font_scale),
        label_medium=ft.TextStyle(size=Typography.LABEL_MEDIUM * font_scale),
        label_small=ft.TextStyle(size=Typography.LABEL_SMALL * font_scale),
    )
    return t


# =============================================================================
# Reusable Styles
# =============================================================================

def card_style(
    padding: int = Spacing.CARD_PADDING,
    bgcolor: str = Colors.SURFACE,
) -> dict:
    """Get common card styling."""
    return {
        "bgcolor": bgcolor,
        "border_radius": Sizing.CARD_BORDER_RADIUS,
        "padding": padding,
        "border": ft.border.all(1, Colors.BORDER),
    }


def section_header_style() -> dict:
    """Get section header text style."""
    return {
        "size": Typography.TITLE_MEDIUM,
        "weight": ft.FontWeight.W_600,
        "color": Colors.ON_SURFACE,
    }


def body_text_style() -> dict:
    """Get body text style."""
    return {
        "size": Typography.BODY_MEDIUM,
        "color": Colors.ON_SURFACE_VARIANT,
    }


def primary_button_style() -> ft.ButtonStyle:
    """Get primary button style."""
    return ft.ButtonStyle(
        color=Colors.ON_PRIMARY,
        bgcolor=Colors.PRIMARY,
        shape=ft.RoundedRectangleBorder(radius=Sizing.BUTTON_BORDER_RADIUS),
        padding=ft.padding.symmetric(horizontal=Spacing.LG, vertical=Spacing.SM),
    )


def secondary_button_style() -> ft.ButtonStyle:
    """Get secondary button style."""
    return ft.ButtonStyle(
        color=Colors.PRIMARY,
        bgcolor=Colors.SURFACE_VARIANT,
        shape=ft.RoundedRectangleBorder(radius=Sizing.BUTTON_BORDER_RADIUS),
        padding=ft.padding.symmetric(horizontal=Spacing.MD, vertical=Spacing.SM),
    )


========================================
FILE: src/tempo_app/ui/__init__.py
========================================
"""UI module - Flet user interface components."""


========================================
FILE: src/tempo_app/__init__.py
========================================
"""TEMPO Analyzer - Desktop Application for NASA TEMPO Satellite Data Analysis."""

__version__ = "1.0.0"
__author__ = "TEMPO Analyzer Team"


========================================
FILE: requirements.txt
========================================
# TEMPO Analyzer - Dependencies
# Install with: pip install -r requirements.txt

# UI Framework - Flutter for Python
flet>=0.21.0

# Data Processing
numpy>=1.24.0
pandas>=2.0.0
xarray>=2023.1.0
netcdf4>=1.6.0

# NASA RSIG API
pyrsig>=0.8.0

# Plotting & Mapping
matplotlib>=3.7.0
cartopy>=0.22.0
pyproj>=3.6.0
pillow>=10.0.0

# Excel Export & Import
xlsxwriter>=3.1.0
openpyxl>=3.1.0

# HTTP & Async File Handling
requests>=2.31.0
aiohttp>=3.9.0
aiofiles>=23.2.0
